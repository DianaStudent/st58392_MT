Key;Item Type;Publication Year;Author;Title;CitationCount;Publication Title;ISBN;ISSN;DOI;Url;Abstract Note;Date;Date Added;Date Modified;Access Date;Pages;Num Pages;Issue;Volume;Number Of Volumes;Journal Abbreviation;Short Title;Series;Series Number;Series Text;Series Title;Publisher;Place;Language;Rights;Type;Archive;Archive Location;Library Catalog;Call Number;Extra;Notes;File Attachments;Link Attachments;Manual Tags;Automatic Tags;Editor;Series Editor;Translator;Contributor;Attorney Agent;Book Author;Cast Member;Commenter;Composer;Cosponsor;Counsel;Interviewer;Producer;Recipient;Reviewed Author;Scriptwriter;Words By;Guest;Number;Edition;Running Time;Scale;Medium;Artwork Size;Filing Date;Application Number;Assignee;Issuing Authority;Country;Meeting Name;Conference Name;Court;References;Reporter;Legal Status;Priority Numbers;Programming Language;Version;System;Code;Code Number;Section;Session;Committee;History;Legislative Body
ERK7TJ53;journalArticle;2023;"Quanjun Zhang; Tongke Zhang; Jun Zhai; Chunrong Fang; Bo Yu; Weisong Sun; Zhenyu Chen";A Critical Review of Large Language Model on Software Engineering: An   Example from ChatGPT and Automated Program Repair;;arXiv.org;;;10.48550/arxiv.2310.08879;;Large Language Models (LLMs) have been gaining increasing attention and demonstrated promising performance across a variety of Software Engineering (SE) tasks, such as Automated Program Repair (APR), code summarization, and code completion. For example, ChatGPT, the latest black-box LLM, has been investigated by numerous recent research studies and has shown impressive performance in various tasks. However, there exists a potential risk of data leakage since these LLMs are usually close-sourced with unknown specific training details, e.g., pre-training datasets. In this paper, we seek to review the bug-fixing capabilities of ChatGPT on a clean APR benchmark with different research objectives. We first introduce {\benchmark}, a new benchmark with buggy and the corresponding fixed programs from competitive programming problems starting from 2023, after the training cutoff point of ChatGPT. The results on {\benchmark} show that ChatGPT is able to fix 109 out of 151 buggy programs using the basic prompt within 35 independent rounds, outperforming state-of-the-art LLMs CodeT5 and PLBART by 27.5\% and 62.4\% prediction accuracy. We also investigate the impact of three types of prompts, i.e., problem description, error feedback, and bug localization, leading to additional 34 fixed bugs. Besides, we provide additional discussion from the interactive nature of ChatGPT to illustrate the capacity of a dialog-based repair workflow with 9 additional fixed bugs. Inspired by the findings, we further pinpoint various challenges and opportunities for advanced SE study equipped with such LLMs (e.g.,~ChatGPT) in the near future. More importantly, our work calls for more research on the reevaluation of the achievements obtained by existing black-box LLMs across various SE tasks, not limited to ChatGPT on APR.;2023-10-13;30.04.2025 13:51;05.05.2025 11:53;;;;;;;;;;;;;;;;;;;;;;ARXIV_ID: 2310.08879 DOI: 10.48550/arxiv.2310.08879 MAG ID: 4387687220 S2ID: b33ee4c8c707db84fc0cf8176a9d8e3ae69e3378;;C:\Users\DK00747\Zotero\storage\62W2E2JQ\Quanjun Zhang et al. - 2023 - A Critical Review of Large Language Model on Software Engineering An   Example from ChatGPT and Aut.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 1.3. Fine-tuned / Instruction-tuned; 4. Software Engineering; 4.1. Development Task Automation; 4.1.2. Code Repair / Refactoring; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
YIFTJETL;journalArticle;2022;"Khaliq, Zubair; Khaliq, Zubair; Farooq, Sheikh Umar; Farooq, Sheikh Umar; Khan, Dawood Ashraf; Khan, Dawood Ashraf";A Deep Learning-based automated framework for functional User Interface testing;;Information & Software Technology;;;10.1016/j.infsof.2022.106969;;;2022;30.04.2025 13:51;05.05.2025 11:54;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: 4281716368 tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\XUHDTIG5\Khaliq et al. - 2022 - A Deep Learning-based automated framework for functional User Interface testing.pdf;;"2. ML; 2.2. Deep Learning; 2.2.1. CNN / RNN / LSTM; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.4. Test Repair; 3.2. Test Data Generation; 3.2.1. Test Oracle; 3.2.3. Test Datasets (HTML, Screenshots, UML); 3.3. Functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
3JFLGF5I;conferencePaper;2024;"Taromirad, Masoumeh; Runeson, Per";A literature survey of assertions in software testing;6;International Conference on Engineering of Computer-Based Systems;;;;https://link.springer.com/chapter/10.1007/978-3-031-49252-5_8;;2024;30.04.2025 12:07;05.05.2025 11:57;30.04.2025 12:07;75–96;;;;;;;;;;;Springer;;;;;;;Google Scholar;;;;C:\Users\DK00747\Zotero\storage\FSJ9YR33\Taromirad and Runeson - 2024 - A literature survey of assertions in software test.pdf;;"3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.6. Failure Detection; 3.2. Test Data Generation; 3.2.1. Test Oracle; 3.3. Functional Testing; 3.5. Robustness / Non-functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
YI8E4MGG;journalArticle;2023;"Belgacem, Hichem; Li, Xiaochen; Bianculli, Domenico; Briand, Lionel";A Machine Learning Approach for Automated Filling of Categorical Fields in Data Entry Forms;14;ACM Transactions on Software Engineering and Methodology;;1049-331X, 1557-7392;10.1145/3533021;https://dl.acm.org/doi/10.1145/3533021;"Users frequently interact with software systems through data entry forms. However, form filling is time-consuming and error-prone. Although several techniques have been proposed to auto-complete or pre-fill fields in the forms, they provide limited support to help users fill categorical fields, i.e., fields that require users to choose the right value among a large set of options.             In this article, we propose LAFF, a learning-based automated approach for filling categorical fields in data entry forms. LAFF first builds Bayesian Network models by learning field dependencies from a set of historical input instances, representing the values of the fields that have been filled in the past. To improve its learning ability, LAFF uses local modeling to effectively mine the local dependencies of fields in a cluster of input instances. During the form filling phase, LAFF uses such models to predict possible values of a target field, based on the values in the already-filled fields of the form and their dependencies; the predicted values (endorsed based on field dependencies and prediction confidence) are then provided to the end-user as a list of suggestions.             We evaluated LAFF by assessing its effectiveness and efficiency in form filling on two datasets, one of them proprietary from the banking domain. Experimental results show that LAFF is able to provide accurate suggestions with a Mean Reciprocal Rank value above 0.73. Furthermore, LAFF is efficient, requiring at most 317 ms per suggestion.";2023-04-30;04.05.2025 10:39;05.05.2025 11:57;04.05.2025 10:39;1-40;;2.0;32.0;;ACM Trans. Softw. Eng. Methodol.;;;;;;;;en;;;;;DOI.org (Crossref);;;;C:\Users\DK00747\Zotero\storage\SNVSBCKS\Belgacem et al. - 2023 - A Machine Learning Approach for Automated Filling of Categorical Fields in Data Entry Forms.pdf;;"2. ML; 2.1. Traditional Machine Learning; 4. Software Engineering; 4.1. Development Task Automation";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
SYVG5S6D;journalArticle;2024;"Ricca, Filippo; Marchetto, Alessandro; Marchetto, A.; Stocco, Andrea";A multi-year grey literature review on AI-assisted test automation;;arXiv.org;;;10.48550/arxiv.2408.06224;;Context: Test Automation (TA) techniques are crucial for quality assurance in software engineering but face limitations such as high test suite maintenance costs and the need for extensive programming skills. Artificial Intelligence (AI) offers new opportunities to address these issues through automation and improved practices. Objectives: Given the prevalent usage of AI in industry, sources of truth are held in grey literature as well as the minds of professionals, stakeholders, developers, and end-users. This study surveys grey literature to explore how AI is adopted in TA, focusing on the problems it solves, its solutions, and the available tools. Additionally, the study gathers expert insights to understand AI's current and future role in TA. Methods: We reviewed over 3,600 grey literature sources over five years, including blogs, white papers, and user manuals, and finally filtered 342 documents to develop taxonomies of TA problems and AI solutions. We also cataloged 100 AI-driven TA tools and interviewed five expert software testers to gain insights into AI's current and future role in TA. Results: The study found that manual test code development and maintenance are the main challenges in TA. In contrast, automated test generation and self-healing test scripts are the most common AI solutions. We identified 100 AI-based TA tools, with Applitools, Testim, Functionize, AccelQ, and Mabl being the most adopted in practice. Conclusion: This paper offers a detailed overview of AI's impact on TA through grey literature analysis and expert interviews. It presents new taxonomies of TA problems and AI solutions, provides a catalog of AI-driven tools, and relates solutions to problems and tools to solutions. Interview insights further revealed the state and future potential of AI in TA. Our findings support practitioners in selecting TA tools and guide future research directions.;2024;30.04.2025 13:51;05.05.2025 11:57;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\GJQKVLIB\Ricca et al. - 2024 - A multi-year grey literature review on AI-assisted test automation.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.3. Fine-tuned / Instruction-tuned; 2. ML; 2.2. Deep Learning; 2.2.1. CNN / RNN / LSTM; 2.5. Reinforcement Learning Strategies; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.2. Test Optimization; 3.1.4. Test Repair; 3.2. Test Data Generation; 3.2.1. Test Oracle";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
8E66R2Q9;journalArticle;2024;"Naimi, Lahbib; Bouziane, El Mahi; Manaouch, Mohamed; Jakim, Abdeslam";A new approach for automatic test case generation from use case diagram using LLMs and prompt engineering;;IEEE International Conference on Circuits and Systems for Communications;;;10.1109/iccsc62074.2024.10616548;;The automation of test case generation from UML diagrams is a growing field that aims to make the software development process smoother. This paper suggests a new framework that uses generative artificial intelligence (AI) to turn use case diagrams into test cases that can be executed. By getting information from the XML representation of use case diagrams, we can create detailed instructions that guide a generative AI model to make test cases for each use case scenario. This method not only makes test case creation easier but also ensures we cover everything well and accurately, which could make software products get to market faster. this approach shows how traditional software engineering methods and new AI techniques can work well together, giving us an idea of what automated software testing might look like in the future.;2024;30.04.2025 13:51;05.05.2025 11:57;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.2. Test Optimization; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.4. Visual / GUI-based Prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
3C8BHVI7;conferencePaper;2024;"Sun, J.; Wang, J.; Zhu, Y.; Li, X.; Xie, Y.; Chen, J.";A Noval Approach to Automated Test Script Generation using Large Language Models for Domain-Specific Language;0;;;;;;This paper presents a novel method for generating automated test scripts for Domain-Specific Languages (DSLs) in software testing, particularly for the automotive industry. It emphasizes the growing importance of software testing in ensuring product quality amid IT advancements. The paper reviews software testing's evolution, modern processes, and the role of Large Language Models (LLMs). It highlights DSLs' significance and uses the automotive sector to show how LLMs can automate test script generation. Tests indicate that in cases with a small sample size, the effectiveness of prompt engineering is superior to model fine-tuning. The proposed method thus relies on well-designed prompts to direct LLMs to produce accurate scripts. The generation system's overview is discussed, along with an evaluation of the scripts' quality using metrics like Levenshtein Distance. Results indicate that LLMs boost test automation, defect detection, and software reliability. Future work will optimize these tools for higher testing automation levels. © 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).;2024;30.04.2025 12:08;05.05.2025 11:57;;52-57;;;3864.0;;;;;;;;;;;;;Scopus;;Scopus;;;;"C:\Users\DK00747\Zotero\storage\4583Z7BY\Sun et al. - 2024 - A Noval Approach to Automated Test Script Generation using Large Language Models for Domain-Specific.pdf; C:\Users\DK00747\Zotero\storage\26QFJM32\display.html";;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.2. Test Optimization; 3.1.6. Failure Detection; 3.1.8. Test Quality Analysis; 5.1. Prompting Strategies; 5.2. Few-shot / Zero-shot";"domain-specific languages; large language models; Levenshtein Distance; software testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;CEUR Workshop Proceedings;;;;;;;;;;;;;;;
KFG69I9R;conferencePaper;2023;"Chang, Xiaoning; Liang, Zheheng; Zhang, Yifei; Cui, Lei; Long, Zhenyue; Wu, Guoquan; Gao, Yu; Chen, Wei; Wei, Jun; Huang, Tao";A reinforcement learning approach to generating test cases for web applications;8;2023 IEEE/ACM International Conference on Automation of Software Test (AST);;;;https://ieeexplore.ieee.org/abstract/document/10173983/;;2023;04.05.2025 10:40;05.05.2025 12:01;04.05.2025 10:40;13–23;;;;;;;;;;;IEEE;;;;;;;Google Scholar;;;;C:\Users\DK00747\Zotero\storage\4BMK6H9I\Chang et al. - 2023 - A reinforcement learning approach to generating test cases for web applications.pdf;;"2. ML; 2.5. Reinforcement Learning Strategies; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.3. Functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
C6PE82AW;journalArticle;2022;"Pham, Phuoc; Nguyen, Vu; Nguyen, Tien N.";A review of AI-augmented end-to-end test automation tools;;International Conference on Automated Software Engineering;;;10.1145/3551349.3563240;;Software testing is a process of evaluating and verifying whether a software product still works as expected, and it is repetitive, laborious, and time-consuming. To address this problem, automation tools have been developed to automate testing activities and enhance quality and delivery time. However, automation tools become less effective with continuous integration and continuous delivery (CI/CD) pipelines when the system under test is constantly changing. Recent advances in artificial intelligence and machine learning (AI/ML) present the potential for addressing important challenges in test automation. AI/ML can be applied to automate various testing activities such as detecting bugs and errors, maintaining existing test cases, or generating new test cases much faster than humans.;2022;30.04.2025 13:51;05.05.2025 12:02;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: 4313563555 tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\D3UGLAWW\Pham et al. - 2022 - A review of AI-augmented end-to-end test automation tools.pdf;;"3. Software Testing; 3.1. Test Automation; 3.1.9. Automated testing tools; 4. Software Engineering; 4.2. Development Process Automation; 4.2.1. CI/CD; 4.2.4. Low-code / No-code Development";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
UW5NXVD9;journalArticle;2024;"Wang, Qing; Wang, Junjie; Li, Mingyang; Wang, Yawen; Liu, Zhe";A roadmap for software testing in open-collaborative and AI-powered era;;ACM Transactions on Software Engineering and Methodology;;;10.1145/3709355;;Internet technology has given rise to an open-collaborative software development paradigm, necessitating the open-collaborative schema to software testing. It enables diverse and globally distributed contributions, but also presents significant challenges to efficient testing processes, coordination among personnel, and management of testing artifacts. At the same time, advancements in artificial intelligence (AI) have enhanced testing capabilities and enabling automation, while also introducing new testing needs and unique challenges for AI-based systems. In this context, this paper explores software testing in the open-collaborative and AI-powered era, focusing on the interrelated dimensions of process, personnel, and technology. Among them, process involves managing testing workflows and artifacts to improve efficiency, personnel emphasizes the role of individuals in ensuring testing quality through collaboration and contributions, while technology refers to AI methods that enhance testing capabilities and address challenges in AI-based systems. Furthermore, we delve into the challenges and opportunities arising from emerging technologies such as large language models (LLMs) and the AI model-centric development paradigm.;2024;30.04.2025 13:51;05.05.2025 12:02;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\FXESBA5B\Wang et al. - 2024 - A roadmap for software testing in open-collaborative and AI-powered era.pdf;;"3. Software Testing; 3.1. Test Automation; 3.1.2. Test Optimization; 3.1.8. Test Quality Analysis; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 4.1.6. Requirement Generation; 4.1.5. Requirement Classification";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
6EGAETET;journalArticle;2024;"Ju, Bangyan; Yang, Jin; Yu, Tingting; Abdullayev, Tamerlan; Wu, Yuanyuan; Wang, Dingbang; Zhao, Yu";A study of using multimodal llms for non-crash functional bug detection in android apps;;Asia-Pacific Software Engineering Conference;;;10.1109/apsec65559.2024.00017;;Numerous approaches employing various strategies have been developed to test the graphical user interfaces (GUIs) of mobile apps. However, traditional GUI testing techniques, such as random and model-based testing, primarily focus on generating test sequences that excel in achieving high code coverage but often fail to act as effective test oracles for noncrash functional (NCF) bug detection. To tackle these limitations, this study empirically investigates the capability of leveraging large language models (LLMs) to be test oracles to detect NCF bugs in Android apps. Our intuition is that the training corpora of LLMs, encompassing extensive mobile app usage and bug report descriptions, enable them with the domain knowledge relevant to NCF bug detection. We conducted a comprehensive empirical study to explore the effectiveness of LLMs as test oracles for detecting NCF bugs in Android apps on 71 welldocumented NCF bugs. The results demonstrated that LLMs achieve a 49;2024;30.04.2025 13:51;05.05.2025 12:02;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\WT2GMQUR\Ju et al. - 2024 - A study of using multimodal llms for non-crash functional bug detection in android apps.pdf;;"1. LLM; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.6. Failure Detection; 3.2. Test Data Generation; 3.2.1. Test Oracle; 3.2.3. Test Datasets (HTML, Screenshots, UML); 3.3. Functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
5GGMFJHD;journalArticle;2024;"Wang, Yue; Wang, Xu; Yu, Hongwei; Gao, Fei; Liu, Xueshi; Wang, Xiaoling";A study on C code defect detection with fine-tuned large language models;;Asia-Pacific Software Engineering Conference;;;10.1109/apsec65559.2024.00055;;Large Language Models(LLMs) have demonstrated excellent capabilities in many areas of software engineering(SE), including code completion, code generation, code understanding, code repair, etc., and the most prominent performer in this regard is ChatGPT. However, its cost of use makes the integration of ChatGPT into code defect detection techniques costly. In this paper, we focus on low-cost-of-use, fine-tunable, open-source large language models with less than 10B parameters, and study their capabilities of C code defect detection when fine-tuned with real-world data and improved with prompt engineering. We studied LLaMa3-8B, DeepSeek-Coder-7b and Qwen2-7B, as they are the typical models with prompt capabilities, whose performance in SE is close to ChatGPT, and they are open-source models. Experimental results show that our method can significantly improve the performance of LLMs within 10B parameters on code defect detection, and the output of the models can be applied to several downstream tasks, such as improving the report quality of static analysis tools.;2024;30.04.2025 13:51;05.05.2025 12:02;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 1.3. Fine-tuned / Instruction-tuned; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 4.1.2. Code Repair / Refactoring; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
EU53GML2;journalArticle;2024;"Qi, Fei; Hou, Ying; Lin, Ning; Bao, Shanshan; Xu, Nuo";A survey of testing techniques based on large language models;;Proceedings of the 2024 International Conference on Computer and Multimedia Technology;;;10.1145/3675249.3675298;;;2024;30.04.2025 13:51;05.05.2025 12:02;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\W7WNMDM5\Qi et al. - 2024 - A survey of testing techniques based on large language models.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 1.3. Fine-tuned / Instruction-tuned; 2. ML; 2.2. Deep Learning; 2.2.3. Transfer / Ensemble Learning; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.2. Test Data Generation; 3.2.3. Test Datasets (HTML, Screenshots, UML); 3.4. Integration Testing; 3.5. Robustness / Non-functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
3657JTLI;journalArticle;2024;"Chen, Liguo; Guo, Qi; Jia, Hongrui; Zeng, Zhengran; Wang, Xin; Xu, Yijiang; Wu, Jian; Wang, Yidong; Gao, Qing; Wang, Jindong; Ye, Wei; Zhang, Shikun";A survey on evaluating large language models in code generation tasks;;arXiv.org;;;10.48550/arxiv.2408.16498;;This paper provides a comprehensive review of the current methods and metrics used to evaluate the performance of Large Language Models (LLMs) in code generation tasks. With the rapid growth in demand for automated software development, LLMs have demonstrated significant potential in the field of code generation. The paper begins by reviewing the historical development of LLMs and their applications in code generation. Next, it details various methods and metrics for assessing the code generation capabilities of LLMs, including code correctness, efficiency, readability, and evaluation methods based on expert review and user experience. The paper also evaluates the widely used benchmark datasets, identifying their limitations and proposing directions for future improvements. Specifically, the paper analyzes the performance of code generation models across different tasks by combining multiple evaluation metrics, such as code compilation/interpretation success rates, unit test pass rates, and performance and efficiency metrics, to comprehensively assess the practical application of LLMs in code generation. Finally, the paper discusses the challenges faced in evaluating LLMs in code generation, particularly how to ensure the comprehensiveness and accuracy of evaluation methods and how to adapt to the evolving practices of software development. These analyses and discussions provide valuable insights for further optimizing and improving the application of LLMs in code generation tasks.;2024;30.04.2025 13:51;05.05.2025 12:02;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\6RUAU8ML\Chen et al. - 2024 - A survey on evaluating large language models in code generation tasks.pdf;;"1. LLM; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
YQDQACHS;journalArticle;2024;"Lops, Andrea; Narducci, F.; Ragone, Azzurra; Trizio, Michelantonio; Bartolini, Claudio";A system for automated unit test generation using large language models and assessment of generated test suites;;International Conference on Software Testing, Verification and Validation Workshops;;;10.1109/icstw64639.2025.10962454;;Unit tests are fundamental for ensuring software correctness but are costly and time-intensive to design and create. Recent advances in Large Language Models (LLMs) have shown potential for automating test generation, though existing evaluations often focus on simple scenarios and lack scalability for real-world applications. To address these limitations, we present AgoneTest, an automated system for generating and assessing complex, class-level test suites for Java projects. Leveraging the Methods2Test dataset, we developed Classes2Test, a new dataset enabling the evaluation of LLM-generated tests against human-written tests. Our key contributions include a scalable automated software system, a new dataset, and a detailed methodology for evaluating test quality.;2024;30.04.2025 13:51;05.05.2025 12:02;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\ET7X4FFR\Lops et al. - 2024 - A system for automated unit test generation using large language models and assessment of generated.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.4. Test Repair; 3.1.8. Test Quality Analysis; 3.5. Robustness / Non-functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
A9YL2LJW;journalArticle;2023;"Alagarsamy, Saranya; Tantithamthavorn, Chakkrit; Aleti, Aldeida";A3Test: Assertion-augmented automated test case generation;;Information and Software Technology;;;10.48550/arxiv.2302.10352;;Test case generation is an important activity, yet a time-consuming and laborious task. Recently, AthenaTest – a deep learning approach for generating unit test cases – is proposed. However, AthenaTest can generate less than one-fifth of the test cases correctly, due to a lack of assertion knowledge and test signature verification. In this paper, we propose A3Test, a DL-based test case generation approach that is augmented by assertion knowledge with a mechanism to verify naming consistency and test signatures. A3Test leverages the domain adaptation principles where the goal is to adapt the existing knowledge from an assertion generation task to the test case generation task. We also introduce a verification approach to verify naming consistency and test signatures. Through an evaluation of 5,278 focal methods from the Defects4j dataset, we find that our A3Test (1) achieves 147;2023;30.04.2025 13:51;05.05.2025 12:03;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: 4321524724 tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\2QPKMMYH\Alagarsamy et al. - 2023 - A3Test Assertion-augmented automated test case generation.pdf;;"1. LLM; 1.3. Fine-tuned / Instruction-tuned; 2. ML; 2.2. Deep Learning; 2.2.1. CNN / RNN / LSTM; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.4. Test Repair; 3.1.8. Test Quality Analysis";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
3L76RY26;conferencePaper;2021;"Ueda, Kiyoshi; Tsukada, Hikaru";Accuracy Improvement by Training Data Selection in Automatic Test Cases Generation Method;;2021 9th International Conference on Information and Education Technology (ICIET);978-1-6654-1933-8;;10.1109/ICIET51873.2021.9419636;https://ieeexplore.ieee.org/document/9419636/;;2021-03-27;02.05.2025 11:53;05.05.2025 12:03;02.05.2025 11:53;438-442;;;;;;;;;;;IEEE;Okayama, Japan;;https://doi.org/10.15223/policy-029;;;;DOI.org (Crossref);;;;;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.2. Test Optimization; 3.1.8. Test Quality Analysis; 3.3. Functional Testing; 3.2. Test Data Generation; 3.2.3. Test Datasets (HTML, Screenshots, UML)";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;2021 9th International Conference on Information and Education Technology (ICIET);;;;;;;;;;;;;;;
VXLFWXVA;journalArticle;2023;"Schäfer, Max; Nadi, Sarah; Eghbali, Aryaz; Tip, Frank";Adaptive test generation using a large language model;91;arXiv preprint arXiv:2302.06527;;;;;;2023;04.05.2025 10:40;05.05.2025 12:03;;1–21;;;;;;;;;;;;;;;;;;Google Scholar;;Publisher: Feb;;;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.5. Self-improving Prompts";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
EDTA96GK;journalArticle;2025;"Mani, Nariman; Attaranasl, Salma";Adaptive test healing using LLM/GPT and reinforcement learning;;International Conference on Software Testing, Verification and Validation Workshops;;;10.1109/icstw64639.2025.10962516;;Flaky tests disrupt software development pipelines by producing inconsistent results, undermining reliability and efficiency. This paper introduces a hybrid framework for adaptive test healing, combining Large Language Models (LLMs) like GPT with Reinforcement Learning (RL) to address test flakiness dynamically. LLMs analyze test logs to classify failures and extract contextual insights, while the RL agent learns optimal strategies for test retries, parameter tuning, and environment resets. Experimental results demonstrate the framework's effectiveness in reducing flakiness and improving CI/CD pipeline stability, outperforming traditional approaches. This work paves the way for scalable, intelligent test automation in dynamic development environments.;2025;30.04.2025 13:51;05.05.2025 12:03;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\L8YU3YXU\Mani and Attaranasl - 2025 - Adaptive test healing using LLMGPT and reinforcement learning.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 1.3. Fine-tuned / Instruction-tuned; 2. ML; 2.5. Reinforcement Learning Strategies; 3. Software Testing; 3.1. Test Automation; 3.1.2. Test Optimization; 3.1.4. Test Repair; 3.1.7. Self-healing Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
RNDU2PE9;conferencePaper;2025;"Yoon, Juyeon; Feldt, Robert; Yoo, Shin";Adaptive Testing for LLM-Based Applications: A Diversity-Based Approach;0;2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW);;;;https://ieeexplore.ieee.org/abstract/document/10962467/;;2025;30.04.2025 12:01;05.05.2025 12:03;30.04.2025 12:01;375–382;;;;;;Adaptive Testing for LLM-Based Applications;;;;;IEEE;;;;;;;Google Scholar;;;;C:\Users\DK00747\Zotero\storage\RTAEKDLC\Yoon et al. - 2025 - Adaptive Testing for LLM-Based Applications A Div.pdf;;"3. Software Testing; 3.1. Test Automation; 3.1.4. Test Repair; 3.1.3. Test Classification; 3.1.5. Test Prioritization; 3.1.6. Failure Detection; 3.1.8. Test Quality Analysis; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
J4QER43X;journalArticle;2025;"Madhumita, Ragavula; Chandana, Daravath";Advancing Software Testing: Integrating AI, Machine Learning, and Emerging Technologies;0;environments;;;;https://www.academia.edu/download/121021281/IJRESM_V8_I1_14.pdf;;2025;30.04.2025 11:59;05.05.2025 12:33;30.04.2025 11:59;9;;;7.0;;;Advancing Software Testing;;;;;;;;;;;;Google Scholar;;;;C:\Users\DK00747\Zotero\storage\HWRSJ6U5\Madhumita and Chandana - Advancing Software Testing Integrating AI, Machin.pdf;;"2. ML; 2.2. Deep Learning; 2.2.2. Autoencoders / GANs; 3. Software Testing; 3.1. Test Automation; 3.1.3. Test Classification; 3.1.5. Test Prioritization; 5. Prompt Engineering; 5.4. Visual / GUI-based Prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
BDYSEZBB;conferencePaper;2024;"Lops, Andrea; Narducci, Fedelucio; Ragone, Azzurra; Trizio, Michelantonio";AgoneTest: Automated creation and assessment of Unit tests leveraging Large Language Models;2;Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering;;;;https://dl.acm.org/doi/abs/10.1145/3691620.3695318;;2024;30.04.2025 11:54;05.05.2025 12:04;30.04.2025 11:54;2440–2441;;;;;;AgoneTest;;;;;;;;;;;;Google Scholar;;;;C:\Users\DK00747\Zotero\storage\R9RIII8E\Lops et al. - 2024 - AgoneTest Automated creation and assessment of Un.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.4. Test Repair; 3.1.8. Test Quality Analysis; 3.3. Functional Testing; 3.1.9. Automated testing tools; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
QDCEWCE6;journalArticle;2025;"Alenezi, Mamdouh; Akour, Mohammed";AI-Driven Innovations in Software Engineering: A Review of Current Practices and Future Directions;10;Applied Sciences;;;;https://www.mdpi.com/2076-3417/15/3/1344;;2025;30.04.2025 12:06;05.05.2025 12:04;30.04.2025 12:06;1344;;3.0;15.0;;;AI-Driven Innovations in Software Engineering;;;;;;;;;;;;Google Scholar;;Publisher: MDPI;;"C:\Users\DK00747\Zotero\storage\CUKI467H\1344.html; C:\Users\DK00747\Zotero\storage\U6FEUMCI\Alenezi and Akour - 2025 - AI-Driven Innovations in Software Engineering A Review of Current Practices and Future Directions.pdf";;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 1.3. Fine-tuned / Instruction-tuned; 2. ML; 2.1. Traditional Machine Learning; 2.2. Deep Learning; 2.3. Supervised / Unsupervised Learning Strategies; 2.4. Active Learning Strategies; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 4.2. Development Process Automation; 4.2.1. CI/CD; 4.2.4. Low-code / No-code Development";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
F5QG3KRL;journalArticle;2024;"Leotta, Maurizio; Yousaf, H. Z.; Ricca, Filippo; Garc?a, Boni";AI-generated test scripts for web E2E testing with ChatGPT and copilot: a preliminary study;;International Conference on Evaluation & Assessment in Software Engineering;;;10.1145/3661167.3661192;;Automated testing is vital for ensuring the reliability of web applications. This paper presents a preliminary study on leveraging artificial intelligence (AI) models, specifically ChatGPT and Github Copilot, to generate test scripts for web end-to-end testing. Through experimentation, we evaluated the feasibility and effectiveness of AI language models in generating test scripts based on natural language descriptions of user interactions with web applications. Our preliminary results show that AI-based generation generally provides an advantage over fully manual test scripts development. Starting from test cases clearly defined in Gherkin, a reduction in development time is always observable. In some cases, this reduction is statistically significant (e.g., Manual vs. a particular use of ChatGPT). These results are valid provided that the tester has some skills in manual test script development and is therefore able to modify the code produced by the AI-generation tools. This study contributes to the exploration of AI-driven solutions in web test scripts generation and lays the foundation for future research in this domain.;2024;30.04.2025 13:51;05.05.2025 12:04;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\CZHMXMNC\Leotta et al. - 2024 - AI-generated test scripts for web E2E testing with ChatGPT and copilot a preliminary study.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.4. Test Repair; 3.1.8. Test Quality Analysis; 3.3. Functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
UPPBRC6H;journalArticle;2024;"Garlapati, Anusha; Sai, Satya; Parmesh, Dr Muni; Savitha, Jaisri S";AI-powered multi-agent framework for automated unit test case generation: Enhancing software quality through llm’s;;2024 5th IEEE Global Conference for Advancement in Technology (GCAT);;;10.1109/gcat62922.2024.10923987;;"Recent years have witnessed an enormous rise in the design, repair and the enhancement of software automation tests. The reliability of program’s unit testing has major impact on its overall performance. The anticipated influence of Artificial Intelligence advancements on test automation methodologies are significant. Many studies on automated testing implicitly assume that the test results are deterministic, means that similar tests faults remain same. The precision of software is largely ensured by unit testing. But writing unit tests manually is a time-consuming process, which leads us to drive into ""Automation Analysis"". Recent years comprised the application of Large Language Models (LLM’s) in numerous fields related to software development, especially the automated creation of unit testing.However, these frameworks require more instructions, or few shot learnings on sample tests that already exist. This research provides a comprehensive empirical assessment of the efficiency of LLM’s for automating unit testing production, with no need for further manual analysis. The method we employ is put into practice for test cases, an adaptable Agents and LLM-based testing framework that evaluates test cases generated, by reviewing and re-writing them in different phases. Evaluation of this test cases was done by using mistral-large LLM Model. The analysis results that developed acquired an overall coverage of 100";2024;30.04.2025 13:51;05.05.2025 12:04;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.4. Test Repair; 3.1.8. Test Quality Analysis; 5.1. Prompting Strategies; 5.5. Self-improving Prompts";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
SH5S3743;journalArticle;2023;"Bayr?, Vahit; Demirel, Ece";AI-powered software testing: The impact of large language models on testing methodologies;;2023 4th International Informatics and Software Engineering Conference (IISEC);;;10.1109/iisec59749.2023.10391027;;Software testing is a crucial aspect of the software development lifecycle, ensuring the delivery of high-quality, reliable, and secure software systems. With the advancements in Artificial Intelligence (AI) and Natural Language Processing (NLP), Large Language Models (LLMs) have emerged as powerful tools capable of understanding and processing natural language texts easly. This article investigates the application of AI-based software testing, with a specific focus on the impact of LLMs in traditional testing methodologies. Through a comprehensive review of relevant literature and SeturDigital’s 25 year testing experience, this article explores the potential benefits, challenges, and prospects of integrating LLMs into software testing.;2023;30.04.2025 13:51;05.05.2025 12:04;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.3. Functional Testing; 3.4. Integration Testing; 3.5. Robustness / Non-functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
9RH4GMRH;journalArticle;2023;"Nguyen, C.; Bui, Huy; Nguyen, Vu-Loc; Nguyen, Tien";An approach to generating API test scripts using GPT;;Symposium on Information and Communication Technology;;;10.1145/3628797.3628947;;As more software systems publish and use web services or APIs today, automated API testing is an important activity to help effectively ensure the quality of software services before they are released for their usage. Generating test scripts and data is a crucial step to perform API test automation successfully. In this paper, we propose an approach leveraging GPT, a large language model, and API’s Swagger specification to automatically generate test scripts and test data for API testing. Our approach also applies GPT’s self-refining with the feedback by executing tests on Katalon. We evaluate our proposed approach using a data set of seven APIs consisting of 157 endpoints and 179 operations. The result shows that while our approach generates fewer test scripts and data inputs, it can cover more successful status codes of 2xx than a state-of-the-art tool. This result suggests that leveraging the ability of GPT as a large language model to interpret API’s Swagger specification has the potential for improving the efficacy of generating test scripts and data for API testing.;2023;30.04.2025 13:51;05.05.2025 12:04;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\UFCJDSM4\Nguyen et al. - 2023 - An approach to generating API test scripts using GPT.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.4. Integration Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
IBMHWI4S;journalArticle;2023;"Guilherme, Vitor; Vincenzi, Auri Marcelo Rizzo";An initial investigation of ChatGPT unit test generation capability;;Brazilian Symposium on Systematic and Automated Software Testing;;;10.1145/3624032.3624035;;Context: Software testing ensures software quality, but developers often disregard it. The use of automated testing generation is pursued to reduce the consequences of overlooked test cases in a software project. Problem: In the context of Java programs, several tools can completely automate generating unit test sets. Additionally, studies are conducted to offer evidence regarding the quality of the generated test sets. However, it is worth noting that these tools rely on machine learning and other AI algorithms rather than incorporating the latest advancements in Large Language Models (LLMs). Solution: This work aims to evaluate the quality of Java unit tests generated by an OpenAI LLM algorithm, using metrics like code coverage and mutation test score. Method: For this study, 33 programs used by other researchers in the field of automated test generation were selected. This approach was employed to establish a baseline for comparison purposes. For each program, 33 unit test sets were generated automatically, without human interference, by changing Open AI API parameters. After executing each test set, metrics such as code line coverage, mutation score, and success rate of test execution were collected to evaluate the efficiency and effectiveness of each set. Summary of Results: Our findings revealed that the OpenAI LLM test set demonstrated similar performance across all evaluated aspects compared to traditional automated Java test generation tools used in the previous research. These results are particularly remarkable considering the simplicity of the experiment and the fact that the generated test code did not undergo human analysis.;2023;30.04.2025 13:51;05.05.2025 12:04;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: 4387711873 tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\ILWLFZ74\Guilherme and Vincenzi - 2023 - An initial investigation of ChatGPT unit test generation capability.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.4. Integration Testing; 3.3. Functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
Y57XCQDA;journalArticle;2025;"Lönnfält, Albin; Tu, Viktor; Gay, Gregory; Singh, Animesh; Tahvili, Sahar";An intelligent test management system for optimizing decision making during software testing;;Journal of Systems and Software;;;10.1016/j.jss.2024.112202;;;2025;30.04.2025 13:51;05.05.2025 12:04;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\5B26FAR6\Lönnfält et al. - 2025 - An intelligent test management system for optimizing decision making during software testing.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.3. Fine-tuned / Instruction-tuned; 2. ML; 2.1. Traditional Machine Learning; 2.1.1. SVM / Decision Trees / Random Forest; 2.3. Supervised / Unsupervised Learning Strategies; 3. Software Testing; 3.1. Test Automation; 3.1.3. Test Classification; 3.1.5. Test Prioritization";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
7D3MIUVY;journalArticle;2023;"Santos, Robson; Santos, ?talo; Magalh?es, C.; Santos, Ronnie E. S.";Are we testing or being tested? Exploring the practical applications of large language models in software testing;;International Conference on Information Control Systems & Technologies;;;10.1109/icst60714.2024.00039;;A Large Language Model (LLM) represents a cutting-edge artificial intelligence model that generates content, including grammatical sentences, human-like paragraphs, and syntactically code snippets. LLMs can play a pivotal role in soft-ware development, including software testing. LLMs go beyond traditional roles such as requirement analysis and documentation and can support test case generation, making them valuable tools that significantly enhance testing practices within the field. Hence, we explore the practical application of LLMs in software testing within an industrial setting, focusing on their current use by professional testers. In this context, rather than relying on existing data, we conducted a cross-sectional survey and collected data within real working contexts-specifically, engaging with practitioners in industrial settings. We applied quantitative and qualitative techniques to analyze and synthesize our collected data. Our findings demonstrate that LLMs effectively enhance testing documents and significantly assist testing professionals in programming tasks like debugging and test case automation. LLMs can support individuals engaged in manual testing who need to code. However, it is crucial to emphasize that, at this early stage, software testing professionals should use LLMs with caution while well-defined methods and guidelines are being built for the secure adoption of these tools.;2023;30.04.2025 13:51;05.05.2025 12:05;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\NA2BXDKC\Santos et al. - 2023 - Are we testing or being tested Exploring the practical applications of large language models in sof.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 4. Software Engineering; 4.2. Development Process Automation; 4.2.2. Debugging Support; 4.2.4. Low-code / No-code Development";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
CS4ZUT7G;journalArticle;2022;"Alonso, Juan C.; Martin-Lopez, Alberto; Segura, Sergio; Garcia, Jose Maria; Ruiz-Cortes, Antonio";ARTE: Automated generation of realistic test inputs for web APIs;43;IEEE Transactions on Software Engineering;;;;https://ieeexplore.ieee.org/abstract/document/9712422/;;2022;04.05.2025 10:39;05.05.2025 12:05;04.05.2025 10:39;348–363;;1.0;49.0;;;ARTE;;;;;;;;;;;;Google Scholar;;Publisher: IEEE;;C:\Users\DK00747\Zotero\storage\5E3MWNGZ\Alonso et al. - 2022 - ARTE Automated generation of realistic test inputs for web APIs.pdf;;"2. ML; 2.1. Traditional Machine Learning; 3. Software Testing; 3.1. Test Automation; 3.2. Test Data Generation; 3.2.3. Test Datasets (HTML, Screenshots, UML)";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
PXPCYBJN;journalArticle;2025;"Ahmed, Iftekhar; Aleti, A.; Cai, Haipeng; Chatzigeorgiou, A.; He, Pinjia; Hu, Xing; Pezz?, Mauro; Poshyvanyk, Denys; Xia, Xin";Artificial intelligence for software engineering: The journey so far and the road ahead;;ACM Transactions on Software Engineering and Methodology;;;10.1145/3719006;;Artificial intelligence and recent advances in deep learning architectures, including transformer networks and large language models, change the way people think and act to solve problems. Software engineering, as an increasingly complex process to design, develop, test, deploy, and maintain large-scale software systems for solving real-world challenges, is profoundly affected by many revolutionary artificial intelligence tools in general, and machine learning in particular. In this roadmap for artificial intelligence in software engineering, we highlight the recent deep impact of artificial intelligence on software engineering by discussing successful stories of applications of artificial intelligence to classic and new software development challenges. We identify the new challenges that the software engineering community has to address in the coming years to successfully apply artificial intelligence in software engineering, and we share our research roadmap towards the effective use of artificial intelligence in the software engineering profession, while still protecting fundamental human values. We spotlight three main areas that challenge the research in software engineering: the use of generative artificial intelligence and large language models for engineering large software systems, the need of large and unbiased datasets and benchmarks for training and evaluating deep learning and large language models for software engineering, and the need of a new code of digital ethics to apply artificial intelligence in software engineering.;2025;30.04.2025 13:51;05.05.2025 12:05;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\TGCRY566\Ahmed et al. - 2025 - Artificial intelligence for software engineering The journey so far and the road ahead.pdf;;"2. ML; 2.2. Deep Learning; 2.2.2. Autoencoders / GANs; 4. Software Engineering; 4.1. Development Task Automation; 4.2. Development Process Automation";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
ITSMVMW3;journalArticle;2025;"Khade, Ms Prajakta Sudhir; Sambhe, Rajeshkumar U.";Artificial Intelligence in Software Development: A Review of Code Generation, Testing, Maintenance and Security;0;;;;;https://ijcsrr.org/wp-content/uploads/2025/04/08-0804-2025.pdf;;2025;30.04.2025 12:05;05.05.2025 12:34;30.04.2025 12:05;;;;;;;Artificial Intelligence in Software Development;;;;;;;;;;;;Google Scholar;;;;C:\Users\DK00747\Zotero\storage\HXNG9Z7D\Khade and Sambhe - Artificial Intelligence in Software Development A.pdf;;"1. LLM; 1.1. Transformer-based Models; 4. Software Engineering; 4.2. Development Process Automation; 4.2.1. CI/CD; 4.2.4. Low-code / No-code Development";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
6ISI3I8V;conferencePaper;2024;"Alshahwan, Nadia; Harman, Mark; Harper, Inna; Marginean, Alexandru; Sengupta, Shubho; Wang, Eddy";Assured Offline LLM-Based Software Engineering;17;Proceedings of the ACM/IEEE 2nd International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering;;;;https://dl.acm.org/doi/abs/10.1145/3643661.3643953;;2024;30.04.2025 12:02;05.05.2025 12:05;30.04.2025 12:02;7–12;;;;;;;;;;;;;;;;;;Google Scholar;;;;C:\Users\DK00747\Zotero\storage\B6YFFF2J\Alshahwan et al. - 2024 - Assured Offline LLM-Based Software Engineering.pdf;;"1. LLM; 1.1. Transformer-based Models; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 4.2. Development Process Automation; 4.2.2. Debugging Support; 4.2.4. Low-code / No-code Development; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
LRQ5QYUJ;journalArticle;2021;"Bergdahl, Joakim; Gordillo, Camilo; Tollmar, Konrad; Gisslén, Linus";Augmenting Automated Game Testing with Deep Reinforcement Learning;102;;;;10.48550/ARXIV.2103.15819;https://arxiv.org/abs/2103.15819;General game testing relies on the use of human play testers, play test scripting, and prior knowledge of areas of interest to produce relevant test data. Using deep reinforcement learning (DRL), we introduce a self-learning mechanism to the game testing framework. With DRL, the framework is capable of exploring and/or exploiting the game mechanics based on a user-defined, reinforcing reward signal. As a result, test coverage is increased and unintended game play mechanics, exploits and bugs are discovered in a multitude of game types. In this paper, we show that DRL can be used to increase test coverage, find exploits, test map difficulty, and to detect common problems that arise in the testing of first-person shooter (FPS) games.;2021;04.05.2025 10:30;05.05.2025 12:06;04.05.2025 10:30;;;;;;;;;;;;;;;arXiv.org perpetual, non-exclusive license;;;;DOI.org (Datacite);;Publisher: arXiv Version Number: 1;;C:\Users\DK00747\Zotero\storage\Q34XRSZ8\Bergdahl et al. - 2021 - Augmenting Automated Game Testing with Deep Reinforcement Learning.pdf;;"2. ML; 2.2. Deep Learning; 2.5. Reinforcement Learning Strategies; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.6. Failure Detection; 3.5. Robustness / Non-functional Testing";"FOS: Computer and information sciences; Machine Learning (cs.LG); Artificial Intelligence (cs.AI)";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
J2I92CWR;conferencePaper;2021;"Sedaghatbaf, Ali; Moghadam, Mahshid Helali; Saadatmand, Mehrdad";Automated Performance Testing Based on Active Deep Learning;;2021 IEEE/ACM International Conference on Automation of Software Test (AST);978-1-6654-3567-3;;10.1109/AST52587.2021.00010;https://ieeexplore.ieee.org/document/9463020/;;2021-05;02.05.2025 11:53;05.05.2025 12:06;02.05.2025 11:53;11-19;;;;;;;;;;;IEEE;Madrid, Spain;;https://doi.org/10.15223/policy-029;;;;DOI.org (Crossref);;;;C:\Users\DK00747\Zotero\storage\BEDGQ9GR\Sedaghatbaf et al. - 2021 - Automated Performance Testing Based on Active Deep Learning.pdf;;"2. ML; 2.2. Deep Learning; 2.2.1. CNN / RNN / LSTM; 2.4. Active Learning Strategies; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.5. Robustness / Non-functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;2021 IEEE/ACM International Conference on Automation of Software Test (AST);;;;;;;;;;;;;;;
Y7YIG5JV;conferencePaper;2024;"Ahammad, Abdellatif; El Bajta, Manal; Radgui, Maryam";Automated Software Testing Using Machine Learning: A Systematic Mapping Study;0;2024 10th International Conference on Optimization and Applications (ICOA);;;;https://ieeexplore.ieee.org/abstract/document/10754031/;;2024;30.04.2025 11:58;05.05.2025 12:06;30.04.2025 11:58;1–6;;;;;;Automated Software Testing Using Machine Learning;;;;;IEEE;;;;;;;Google Scholar;;;;;;"2. ML; 2.2. Deep Learning; 2.2.1. CNN / RNN / LSTM; 2.5. Reinforcement Learning Strategies; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
DVBYGJSA;journalArticle;2024;"Kang, Long; Ai, Jun; Lu, Minyan";Automated structural test case generation for human-computer interaction software based on large language model;;International Conferences on Dependable Systems and Their Applications;;;10.1109/dsa63982.2024.00027;;As software systems expand in complexity, managing the vast and varied collection of test cases becomes increasingly difficult with traditional manual testing methods. This paper presents a new approach for automating the generation of structured test cases, named Test Element Extraction and Restructuring (TEER), which leverages the advanced natural language processing capabilities of large language models (LLMs). Specifically targeting human-computer interaction (HCI) software, TEER employs prompt tuning techniques to extract critical elements from natural language test cases and systematically reassemble them into structured formats. The study evaluates the effectiveness of TEER by applying it to common test cases from desktop HCI applications. The experimental results demonstrate that this method successfully produces structured test cases that meet predefined requirements.;2024;30.04.2025 13:51;05.05.2025 12:06;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\TW3QY6EP\Kang et al. - 2024 - Automated structural test case generation for human-computer interaction software based on large lan.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.3. Fine-tuned / Instruction-tuned; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.3. Test Classification; 3.1.5. Test Prioritization; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.3. Instructional / Role prompting; 5.4. Visual / GUI-based Prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
YZUJDQ76;journalArticle;2024;"Ozer, Edipcan; Akcayol, M. A.";Automated test case output generation using Seq2Seq models;;Proceedings of the 2024 13th International Conference on Software and Information Engineering;;;10.1145/3708635.3708644;;;2024;30.04.2025 13:51;05.05.2025 12:06;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\A56PSMCF\Ozer and Akcayol - 2024 - Automated test case output generation using Seq2Seq models.pdf;;"1. LLM; 1.1. Transformer-based Models; 2. ML; 2.2. Deep Learning; 2.2.1. CNN / RNN / LSTM; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 4. Software Engineering";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
V9C8EVVU;journalArticle;2024;"Hadzhikoleva, S.; Rachovski, Todor; Ivanov, Ivan; Hadzhikolev, E.; Dimitrov, G.";Automated test creation using large language models: a practical application;;Applied Sciences;;;10.3390/app14199125;;The article presents work on developing a software application for test creation using artificial intelligence and large language models. Its main goal is to optimize the educators’ work by automating the process of test generation and evaluation, with the tests being stored for subsequent analysis and use. The application can generate test questions based on specified criteria such as difficulty level, Bloom’s taxonomy level, question type, style and format, feedback inclusion, and more, thereby providing opportunities to enhance the adaptability and efficiency of the learning process. It is developed on the Google Firebase platform, utilizing the ChatGPT API, and also incorporates cloud computing to ensure scalability and data reliability.;2024;30.04.2025 13:51;05.05.2025 12:06;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\LZ7TK6T7\Hadzhikoleva et al. - 2024 - Automated test creation using large language models a practical application.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.4. Visual / GUI-based Prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
6B8WPB5F;journalArticle;2021;"Avdeenko, Tatiana; Serdyukov, Konstantin";Automated Test Data Generation Based on a Genetic Algorithm with Maximum Code Coverage and Population Diversity;;Applied Sciences;;2076-3417;10.3390/app11104673;https://www.mdpi.com/2076-3417/11/10/4673;In the present paper, we investigate an approach to intelligent support of the software white-box testing process based on an evolutionary paradigm. As a part of this approach, we solve the urgent problem of automated generation of the optimal set of test data that provides maximum statement coverage of the code when it is used in the testing process. We propose the formulation of a fitness function containing two terms, and, accordingly, two versions for implementing genetic algorithms (GA). The first term of the fitness function is responsible for the complexity of the code statements executed on the path generated by the current individual test case (current set of statements). The second term formulates the maximum possible difference between the current set of statements and the set of statements covered by the remaining test cases in the population. Using only the first term does not make it possible to obtain 100 percent statement coverage by generated test cases in one population, and therefore implies repeated launch of the GA with changed weights of the code statements which requires recompiling the code under the test. By using both terms of the proposed fitness function, we obtain maximum statement coverage and population diversity in one launch of the GA. Optimal relation between the two terms of fitness function was obtained for two very different programs under testing.;2021-05-20;02.05.2025 11:53;05.05.2025 12:06;02.05.2025 11:53;4673;;10.0;11.0;;Applied Sciences;;;;;;;;en;https://creativecommons.org/licenses/by/4.0/;;;;DOI.org (Crossref);;;;C:\Users\DK00747\Zotero\storage\BIEYD9DP\Avdeenko and Serdyukov - 2021 - Automated Test Data Generation Based on a Genetic Algorithm with Maximum Code Coverage and Populatio.pdf;;"2. ML; 2.1. Traditional Machine Learning; 2.5. Reinforcement Learning Strategies; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.2. Test Data Generation";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
PEBZI5A6;journalArticle;2025;"Rosenbach, Tim; Heidrich, David; Weinert, Alexander";Automated testing of the GUI of a real-life engineering software using large language models;;International Conference on Software Testing, Verification and Validation Workshops;;;10.1109/icstw64639.2025.10962502;;One important step in software development is testing the finished product with actual users. These tests aim, among other goals, at determining unintuitive behavior of the software as it is presented to the end-user. Moreover, they aim to determine inconsistencies in the user-facing interface. They provide valuable feedback for the development of the software, but are time-intensive to conduct.In this work, we present GERALLT, a system that uses Large Language Models (LLMs) to perform exploratory tests of the Graphical User Interface (GUI) of a real-life engineering software. GERALLT automatically generates a list of potential unintuitive and inconsistent parts of the interface. We present the architecture of GERALLT and evaluate it on a real-world use case of the engineering software, which has been extensively tested by developers and users. Our results show that GERALLT is able to determine issues with the interface that support the software development team in future development of the software.;2025;30.04.2025 13:51;05.05.2025 12:07;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.3. Functional Testing; 3.5. Robustness / Non-functional Testing; 4. Software Engineering; 4.1. Development Task Automation; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.4. Visual / GUI-based Prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
LVVPQ29Q;journalArticle;2024;"Alshahwan, N.; Chheda, Jubin; Finogenova, Anastasia; Gokkaya, Beliz; Harman, Mark; Harper, Inna; Marginean, Alexandru; Sengupta, Shubho; Wang, Eddy";Automated unit test improvement using large language models at meta;;SIGSOFT FSE Companion;;;10.48550/arxiv.2402.09171;;This paper describes Meta's TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests. TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination. We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75;2024;30.04.2025 13:51;05.05.2025 12:07;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\K8XZS5SB\Alshahwan et al. - 2024 - Automated unit test improvement using large language models at meta.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.4. Test Repair; 3.1.8. Test Quality Analysis; 3.3. Functional Testing; 3.5. Robustness / Non-functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
LQLZANIZ;journalArticle;2024;"Ö?al, Akdeniz Kutay; Keskinöz, Mehmet";Automatic unit test code generation using large language models;;Signal Processing and Communications Applications Conference;;;10.1109/siu61531.2024.10600772;;This study aimed to automate the production of unit tests, a critical component of the software development process. By using pre-trained Large Language Models, manual effort and training costs were reduced, and test production capacity was increased. Instead of directly feeding the test functions obtained from the Java projects to be tested into the model, the project was analyzed to extract additional information. The data obtained from this analysis were used to create an effective prompt template. Furthermore, the sources of the problematic tests produced were identified, and this information was fed back into the model, enabling it to autonomously correct the errors. The results of the study showed that the model was able to generate tests covering;2024;30.04.2025 13:51;05.05.2025 12:07;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.4. Test Repair; 3.1.8. Test Quality Analysis; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.5. Self-improving Prompts";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
U3NZMXYL;conferencePaper;2021;"Zheng, Yan; Liu, Yi; Xie, Xiaofei; Liu, Yepang; Ma, Lei; Hao, Jianye; Liu, Yang";Automatic Web Testing Using Curiosity-Driven Reinforcement Learning;;2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE);978-1-6654-0296-5;;10.1109/ICSE43902.2021.00048;https://ieeexplore.ieee.org/document/9402046/;;2021-05;02.05.2025 11:53;05.05.2025 12:07;02.05.2025 11:53;423-435;;;;;;;;;;;IEEE;Madrid, ES;;;;;;DOI.org (Crossref);;;;C:\Users\DK00747\Zotero\storage\B3KJP3V9\Zheng et al. - 2021 - Automatic Web Testing Using Curiosity-Driven Reinforcement Learning.pdf;;"1. LLM; 1.3. Fine-tuned / Instruction-tuned; 2. ML; 2.5. Reinforcement Learning Strategies; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.6. Failure Detection; 3.3. Functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE);;;;;;;;;;;;;;;
VHA6F7R5;journalArticle;2024;"Wen, Cheng; Cai, Yuandao; Zhang, Bin; Su, Jie; Xu, Zhiwu; Liu, Dugang; Qin, Shengchao; Ming, Zhong; Cong, Tian";Automatically inspecting thousands of static bug warnings with large language model: How far are we?;;ACM Transactions on Knowledge Discovery from Data;;;10.1145/3653718;;Static analysis tools for capturing bugs and vulnerabilities in software programs are widely employed in practice, as they have the unique advantages of high coverage and independence from the execution environment. However, existing tools for analyzing large codebases often produce a great deal of false warnings over genuine bug reports. As a result, developers are required to manually inspect and confirm each warning, a challenging, time-consuming, and automation-essential task. This article advocates a fast, general, and easily extensible approach called Llm4sa that automatically inspects a sheer volume of static warnings by harnessing (some of) the powers of Large Language Models (LLMs). Our key insight is that LLMs have advanced program understanding capabilities, enabling them to effectively act as human experts in conducting manual inspections on bug warnings with their relevant code snippets. In this spirit, we propose a static analysis to effectively extract the relevant code snippets via program dependence traversal guided by the bug warning reports themselves. Then, by formulating customized questions that are enriched with domain knowledge and representative cases to query LLMs, Llm4sa can remove a great deal of false warnings and facilitate bug discovery significantly. Our experiments demonstrate that Llm4sa is practical in automatically inspecting thousands of static warnings from Juliet benchmark programs and 11 real-world C/C++ projects, showcasing a high precision (81.13;2024;30.04.2025 13:51;05.05.2025 12:07;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\NQCR9JCJ\Wen et al. - 2024 - Automatically inspecting thousands of static bug warnings with large language model How far are we.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.6. Failure Detection; 3.1.8. Test Quality Analysis; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
Y4HG9GYC;journalArticle;2021;"Esnaashari, Mehdi; Damia, Amir Hossein";Automation of software test data generation using genetic algorithm and reinforcement learning;;Expert Systems with Applications;;09574174;10.1016/j.eswa.2021.115446;https://linkinghub.elsevier.com/retrieve/pii/S0957417421008605;;2021-11;02.05.2025 11:53;05.05.2025 12:07;02.05.2025 11:53;115446;;;183.0;;Expert Systems with Applications;;;;;;;;en;;;;;DOI.org (Crossref);;;;C:\Users\DK00747\Zotero\storage\8VXNSTSH\Esnaashari and Damia - 2021 - Automation of software test data generation using genetic algorithm and reinforcement learning.pdf;;"2. ML; 2.1. Traditional Machine Learning; 2.5. Reinforcement Learning Strategies; 3. Software Testing; 3.1. Test Automation; 3.2. Test Data Generation; 3.1.8. Test Quality Analysis";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
VB5P8WEU;journalArticle;2025;"Shin, Subin; Oh, Jeesun; Lee, Sangwon";Can llms see what I see? A study on five prompt engineering techniques for evaluating UX on a shopping site;;CHI Extended Abstracts;;;10.1145/3706599.3720079;;;2025;30.04.2025 13:51;05.05.2025 12:07;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\XR4GCKWQ\Shin et al. - 2025 - Can llms see what I see A study on five prompt engineering techniques for evaluating UX on a shoppi.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.2. Test Data Generation; 3.2.3. Test Datasets (HTML, Screenshots, UML); 3.3. Functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.3. Instructional / Role prompting; 5.4. Visual / GUI-based Prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
55GLYV8J;journalArticle;2023;"Xie, Zhuokui; Chen, Yinghao; Zhi, Chen; Deng, Shuiguang; Yin, Jianwei";ChatUniTest: a ChatGPT-based automated unit test generation tool;105;arXiv e-prints;;;;;;2023;04.05.2025 10:41;05.05.2025 12:07;;arXiv–2305;;;;;;ChatUniTest;;;;;;;;;;;;Google Scholar;;;;C:\Users\DK00747\Zotero\storage\8GZ5ELQQ\Xie et al. - 2023 - ChatUniTest a ChatGPT-based automated unit test generation tool.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.3. Functional Testing; 3.1.9. Automated testing tools; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.5. Self-improving Prompts";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
WRRG9NGY;journalArticle;2023;"Chen, Yinghao; Hu, Zehao; Zhi, Chen; Han, Junxiao; Deng, Shuiguang; Yin, Jianwei";ChatUniTest: a framework for LLM-based test generation;;SIGSOFT FSE Companion;;;10.1145/3663529.3663801;;Unit testing is an essential yet frequently arduous task. Various automated unit test generation tools have been introduced to mitigate this challenge. Notably, methods based on large language models (LLMs) have garnered considerable attention and exhibited promising results in recent years. Nevertheless, LLM-based tools encounter limitations in generating accurate unit tests. This paper presents ChatUniTest, an LLM-based automated unit test generation framework. ChatUniTest incorporates an adaptive focal context mechanism to encompass valuable context in prompts and adheres to a generation-validation-repair mechanism to rectify errors in generated unit tests. Subsequently, we have developed ChatUniTest Core, a common library that implements core workflow, complemented by the ChatUniTest Toolchain, a suite of seamlessly integrated tools enhancing the capabilities of ChatUniTest. Our effectiveness evaluation reveals that ChatUniTest outperforms TestSpark and EvoSuite in half of the evaluated projects, achieving the highest overall line coverage. Furthermore, insights from our user study affirm that ChatUniTest delivers substantial value to various stakeholders in the software testing domain. ChatUniTest is available at https://github.com/ZJU-ACES-ISE/ChatUniTest, and the demo video is available at https://www.youtube.com/watch?v=GmfxQUqm2ZQ.;2023;30.04.2025 13:51;05.05.2025 12:07;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\78XNY6IB\Chen et al. - 2023 - ChatUniTest a framework for LLM-based test generation.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.4. Integration Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
YWJ3M2MV;journalArticle;2024;"Mu, Fangwen; Shi, Lin; Wang, Song; Yu, Zhuohao; Zhang, Binquan; Wang, ChenXue; Liu, Shichao; Wang, Qing";ClarifyGPT: a framework for enhancing LLM-based code generation via requirements clarification;;Proc. ACM Softw. Eng.;;;10.1145/3660810;;Large Language Models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in automatically generating code from provided natural language requirements. However, in real-world practice, it is inevitable that the requirements written by users might be ambiguous or insufficient. Current LLMs will directly generate programs according to those unclear requirements, regardless of interactive clarification, which will likely deviate from the original user intents. To bridge that gap, we introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. Specifically, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we invite ten participants to use ClarifyGPT for code generation on two benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96;2024;30.04.2025 13:51;05.05.2025 12:08;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\SQS52GKE\Mu et al. - 2024 - ClarifyGPT a framework for enhancing LLM-based code generation via requirements clarification.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 4.1.5. Requirement Classification; 4.1.5. Requirement Classification; 4.1.6. Requirement Generation; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
WKVDJZKJ;journalArticle;2023;"Lemieux, Carole; Inala, Jeevana Priya; Lahiri, Shuvendu K.; Sen, Siddhartha";CodaMosa: Escaping coverage plateaus in test generation with pre-trained large language models;;International Conference on Software Engineering;;;10.1109/icse48619.2023.00085;;Search-based software testing (SBST) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST's performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, SBST can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models (LLMs) of code, such as OpenAI's Codex, can be used to help SBST's exploration. Our proposed algorithm, CodaMosa, conducts SBST until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help SBST redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, CodaMosa achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to SBST and LLM-only baselines.;2023;30.04.2025 13:51;05.05.2025 12:08;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: 4384304865 tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\JZCVXVF8\Lemieux et al. - 2023 - CodaMosa Escaping coverage plateaus in test generation with pre-trained large language models.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.2. Test Data Generation; 3.2.1. Test Oracle";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
BPFIIC7F;journalArticle;2024;"Ryan, Gabriel; Jain, Siddhartha; Shang, Mingyue; Wang, Shiqi; Ma, Xiaofei; Ramanathan, M.; Ray, Baishakhi";Code-aware prompting: a study of coverage guided test generation in regression setting using LLM;;Proc. ACM Softw. Eng.;;;10.48550/arxiv.2402.00097;;Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent work using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated testsuites still suffer from low coverage. In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt’s approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a specific prompt aligned with the execution paths of the method under test, and exposing relevant type and dependency focal context to the model. Our approach enables pretrained LLMs to generate more complete test cases without any additional training. We implement SymPrompt using the TreeSitter parsing framework and evaluate on a benchmark challenging methods from open source Python projects. SymPrompt enhances correct test generations by a factor of 5 and bolsters relative coverage by 26;2024;30.04.2025 13:51;05.05.2025 12:08;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\4GDZNNSV\Ryan et al. - 2024 - Code-aware prompting a study of coverage guided test generation in regression setting using LLM.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.5. Robustness / Non-functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
5D7NKGXM;journalArticle;2021;"Wang, Yue; Wang, Weishi; Joty, Shafiq; Hoi, Steven C. H.";CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation;;;;;10.48550/arXiv.2109.00859;http://arxiv.org/abs/2109.00859;Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https: //github.com/salesforce/CodeT5 .;2021-09-02;02.05.2025 11:56;13.05.2025 15:07;02.05.2025 11:56;;;;;;;CodeT5;;;;;;;;;;;;arXiv.org;;arXiv:2109.00859 [cs];;"C:\Users\DK00747\Zotero\storage\UPC3TQM3\Wang et al. - 2021 - CodeT5 Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Gener.pdf; C:\Users\DK00747\Zotero\storage\XYVQ7QS7\2109.html";;"1. LLM; 1.1. Transformer-based Models; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 4.1.4. Source Code Analysis";"Computer Science - Computation and Language; Computer Science - Programming Languages";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
AI5DCY76;conferencePaper;2025;"Furman, Deborah Ann; Farchi, Eitan; Gildein, Michael Edward; Hicks, Andrew CM; Rawlins, Ryan Thomas";Combinatorial Test Design Model Creation using Large Language Models;0;2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW);;;;https://ieeexplore.ieee.org/abstract/document/10962528/;;2025;30.04.2025 12:01;05.05.2025 12:08;30.04.2025 12:01;314–323;;;;;;;;;;;IEEE;;;;;;;Google Scholar;;;;;;"1. LLM; 3. Software Testing; 3.1. Test Automation; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
7SB6KGDZ;journalArticle;2023;"Martins, José; Branco, Frederico; Mamede, Henrique";Combining low-code development with ChatGPT to novel no-code approaches: A focus-group study;;Intelligent Systems with Applications;;26673053;10.1016/j.iswa.2023.200289;https://linkinghub.elsevier.com/retrieve/pii/S266730532300114X;;2023-11;14.12.2024 7:42;05.05.2025 12:08;14.12.2024 7:42;200289;;;20.0;;Intelligent Systems with Applications;Combining low-code development with ChatGPT to novel no-code approaches;;;;;;;en;;;;;DOI.org (Crossref);;;;C:\Users\DK00747\Zotero\storage\ACTVIB8Q\Martins et al. - 2023 - Combining low-code development with ChatGPT to nov.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 4.2. Development Process Automation; 4.2.2. Debugging Support; 4.2.4. Low-code / No-code Development";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
TNCF3C39;conferencePaper;2024;"Wanigasekara, Sashini; Asanka, Dinesh; Rajapakse, Chathura; Wickramaarachchi, Dilani; Wijesinghe, Abhiru";Comparing the adaptability of a genetic algorithm and an LLM-based framework for automated software test data generation: In the context of web applications;;2024 IEEE 3rd International Conference on Data, Decision and Systems (ICDDS);;;10.1109/icdds62937.2024.10910287;;In the fast-paced world of software development, ensuring software quality is paramount. Software Quality Assurance (SQA) plays a vital role, primarily through testing, which can be carried out manually or automatically. Yet, creating comprehensive test data (TD) for web applications can be a formidable task. Manual test data generation (TDG) is time-consuming and error prone. Automation of TDG has become increasingly important in the realm of software quality assurance as it enables efficient and effective testing of software systems. The need for an appropriate framework for automated TDG is critical to achieve comprehensive and reliable test coverage. Automated TDG offers significant advantages, including time and resource savings, improved test coverage, and seamless integration into the software development process. The core aim of this research is to bridge the gap between manual and existing automated methods, resulting in time and cost savings, heightened testing efficiency, and elevated software quality. Research objectives encompass comparing the adaptability of an AGA based automated TDG model and a LLM based automated TDG model to a web application. The results from the LLM model for triangle classification program was found to be potentially acceptable and accurate than the AGA model's results. This research discusses the challenges encountered when implementing and using the AGA-based framework in the web application context and how an LLM model could overcome the challenges. The study highlights the benefits of using the LLM approach, demonstrating its relevance and accuracy in generating test data compared to the Genetic Algorithm-based model. The practical implications for software quality assurance practices are discussed, emphasizing the enhanced efficiency and effectiveness of the LLM model in improving software quality.;2024;30.04.2025 13:51;09.05.2025 9:46;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\84S8SWZH\authors.html;;"1. LLM; 1.3. Fine-tuned / Instruction-tuned; 2. ML; 2.1. Traditional Machine Learning; 2.1.2. Feature-based Models; 2.2. Deep Learning; 2.2.3. Transfer / Ensemble Learning; 3. Software Testing; 3.1. Test Automation; 3.2. Test Data Generation; 3.2.3. Test Datasets (HTML, Screenshots, UML); 3.5. Robustness / Non-functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
LFH79NV3;journalArticle;2024;"Karpurapu, Shanthi; Myneni, Sravanthy; Nettur, Unnati; Gajja, Likhit Sagar; Burke, Dave; Stiehm, Tom; Payne, Jeffery";Comprehensive evaluation and insights into the use of large language models in the automation of behavior-driven development acceptance test formulation;;IEEE access : practical innovations, open solutions;;;10.1109/access.2024.3391815;;Behavior-driven development (BDD) is an Agile testing methodology fostering collaboration among developers, QA analysts, and stakeholders. In this manuscript, we propose a novel approach to enhance BDD practices using large language models (LLMs) to automate acceptance test generation. Our study uses zero and few-shot prompts to evaluate LLMs such as GPT-3.5, GPT-4, Llama-2-13B, and PaLM-2. The paper presents a detailed methodology that includes the dataset, prompt techniques, LLMs, and the evaluation process. The results demonstrate that GPT-3.5 and GPT-4 generate error-free BDD acceptance tests with better performance. The few-shot prompt technique highlights its ability to provide higher accuracy by incorporating examples for in-context learning. Furthermore, the study examines syntax errors, validation accuracy, and comparative analysis of LLMs, revealing their effectiveness in enhancing BDD practices. However, our study acknowledges that there are limitations to the proposed approach. We emphasize that this approach can support collaborative BDD processes and create opportunities for future research into automated BDD acceptance test generation using LLMs.;2024;30.04.2025 13:51;05.05.2025 12:08;;;;;;;IEEE Access;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\GHKYJLTS\Karpurapu et al. - 2024 - Comprehensive evaluation and insights into the use of large language models in the automation of beh.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.3. Functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
W92T5JCM;conferencePaper;2021;"Schulz, Henning; Okanoviã, Duðan; Van Hoorn, André; T?ma, Petr";Context-tailored Workload Model Generation for Continuous Representative Load Testing;;Proceedings of the ACM/SPEC International Conference on Performance Engineering;978-1-4503-8194-9;;10.1145/3427921.3450240;https://dl.acm.org/doi/10.1145/3427921.3450240;;2021-04-09;02.05.2025 11:53;05.05.2025 12:08;02.05.2025 11:53;21-32;;;;;;;;;;;ACM;Virtual Event France;en;;;;;DOI.org (Crossref);;;;C:\Users\DK00747\Zotero\storage\EF7ZJ35W\Schulz et al. - 2021 - Context-tailored Workload Model Generation for Continuous Representative Load Testing.pdf;;"2. ML; 2.1. Traditional Machine Learning; 2.1.1. SVM / Decision Trees / Random Forest; 2.2. Deep Learning; 2.2.1. CNN / RNN / LSTM; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.5. Robustness / Non-functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ICPE '21: ACM/SPEC International Conference on Performance Engineering;;;;;;;;;;;;;;;
8XKBIFQJ;journalArticle;2025;"Nettur, Suresh Babu; Karpurapu, Shanthi; Nettur, Unnati; Gajja, Likhit Sagar";Cypress copilot: Development of an AI assistant for boosting productivity and transforming web application testing;;IEEE access : practical innovations, open solutions;;;10.1109/access.2024.3521407;;In today’s fast-paced software development environment, Agile methodologies demand rapid delivery and continuous improvement, making automated testing essential for maintaining quality and accelerating feedback loops. Our study addresses the challenges of developing and maintaining automation code for web-based application testing. In this paper, we propose a novel approach that leverages large language models (LLMs) and a novel prompt technique, few-shot chain, to automate code generation for web application testing. We chose the Behavior-Driven Development (BDD) methodology owing to its advantages and selected the Cypress tool for automating web application testing, as it is one of the most popular and rapidly growing frameworks in this domain. We comprehensively evaluated various OpenAI models, including GPT-4-Turbo, GPT-4o, and GitHub Copilot, using zero-shot and few-shot chain prompt techniques. Furthermore, we extensively validated with a vast set of test cases to identify the optimal approach. Our results indicate that the Cypress automation code generated by GPT-4o using a few-shot chained prompt approach excels in generating complete code for each test case, with fewer empty methods and improved syntactical accuracy and maintainability. Based on these findings, we developed a novel open-source Visual Studio Code (IDE) extension, “Cypress Copilot” utilizing GPT-4o and a few-shot chain prompt technique, which has shown promising results. Finally, we validate the Cypress Copilot tool by generating automation code for end-to-end web tests, demonstrating its effectiveness in testing various web applications and its ability to streamline development processes. More importantly, we are releasing this tool to the open-source community, as it has the potential to be a promising partner in enhancing productivity in web application automation testing.;2025;30.04.2025 13:51;05.05.2025 12:09;;;;;;;IEEE Access;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\DZ9XF9VW\Nettur et al. - 2025 - Cypress copilot Development of an AI assistant for boosting productivity and transforming web appli.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.3. Functional Testing; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.4. Visual / GUI-based Prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
7HAQCV55;conferencePaper;2021;"Mirabella, A. Giuliano; Martin-Lopez, Alberto; Segura, Sergio; Valencia-Cabrera, Luis; Ruiz-Cortés, Antonio";Deep learning-based prediction of test input validity for restful apis;36;2021 IEEE/ACM third international workshop on deep learning for testing and testing for deep learning (deepTest);;;;https://ieeexplore.ieee.org/abstract/document/9476896/;;2021;05.05.2025 11:16;05.05.2025 12:09;05.05.2025 11:16;9–16;;;;;;;;;;;IEEE;;;;;;;Google Scholar;;;;C:\Users\DK00747\Zotero\storage\VSJ5NZNQ\Mirabella et al. - 2021 - Deep learning-based prediction of test input validity for restful apis.pdf;;"2. ML; 2.2. Deep Learning; 2.2.1. CNN / RNN / LSTM; 2.5. Reinforcement Learning Strategies; 3. Software Testing; 3.1. Test Automation; 3.1.5. Test Prioritization; 3.1.6. Failure Detection; 3.1.8. Test Quality Analysis; 3.2. Test Data Generation; 3.2.3. Test Datasets (HTML, Screenshots, UML); 3.5. Robustness / Non-functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
U5IRPQR8;journalArticle;2024;"Corradini, Davide; Montolli, Zeno; Pasqua, Michele; Ceccato, Mariano";DeepREST: Automated test case generation for REST apis exploiting deep reinforcement learning;;International Conference on Automated Software Engineering;;;10.48550/arxiv.2408.08594;;Automatically crafting test scenarios for REST APIs helps deliver more reliable and trustworthy web-oriented systems. However, current black-box testing approaches rely heavily on the information available in the API’s formal documentation, i.e., the Open API Specification (OAS for short). While useful, the OAS mostly covers syntactic aspects of the API (e.g., producer-consumer relations between operations, input value properties, and additional constraints in natural language), and it lacks a deeper understanding of the API business logic. Missing semantics include implicit ordering (logic dependency) between operations and implicit input-value constraints. These limitations hinder the ability of black-box testing tools to generate truly effective test cases automatically.This paper introduces DeepREST, a novel black-box approach for automatically testing REST APIs. It leverages deep reinforcement learning to uncover implicit API constraints, that is, constraints hidden from API documentation. Curiosity-driven learning guides an agent in the exploration of the API and learns an effective order to test its operations. This helps identify which operations to test first to take the API in a testable state and avoid failing API interactions later. At the same time, experience gained on successful API interactions is leveraged to drive accurate input data generation (i.e., what parameters to use and how to pick their values). Additionally, DeepREST alternates exploration with exploitation by mutating successful API interactions to improve test coverage and collect further experience.Our empirical validation suggests that the proposed approach is very effective in achieving high test coverage and fault detection and superior to a state-of-the-art baseline.;2024;30.04.2025 13:51;05.05.2025 12:09;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\UXL9A5HY\Corradini et al. - 2024 - DeepREST Automated test case generation for REST apis exploiting deep reinforcement learning.pdf;;"2. ML; 2.2. Deep Learning; 2.2.2. Autoencoders / GANs; 2.5. Reinforcement Learning Strategies; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.5. Robustness / Non-functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
ZYWEFFII;journalArticle;2023;"Shin, Jiho; Hashtroudi, Sepehr; Hemmati, H.; Wang, Song";Domain adaptation for code model-based unit test case generation;;International Symposium on Software Testing and Analysis;;;10.1145/3650212.3680354;;Recently, deep learning-based test case generation approaches have been proposed to automate the generation of unit test cases. In this study, we leverage Transformer-based code models to generate unit tests with the help of Domain Adaptation (DA) at a project level. Specifically, we use CodeT5, a relatively small language model trained on source code data, and fine-tune it on the test generation task. Then, we apply domain adaptation to each target project data to learn project-specific knowledge (project-level DA). We use the Methods2test dataset to fine-tune CodeT5 for the test generation task and the Defects4j dataset for project-level domain adaptation and evaluation. We compare our approach with (a) CodeT5 fine-tuned on the test generation without DA, (b) the A3Test tool, and (c) GPT-4 on five projects from the Defects4j dataset. The results show that tests generated using DA can increase the line coverage by 18.62;2023;30.04.2025 13:51;05.05.2025 12:09;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\6JU24F3Y\Shin et al. - 2023 - Domain adaptation for code model-based unit test case generation.pdf;;"1. LLM; 1.3. Fine-tuned / Instruction-tuned; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.3. Functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
RA539EEK;journalArticle;2021;"Yasin, Husam N.; Hamid, Siti Hafizah Ab; Raja Yusof, Raja Jamilah";DroidbotX: Test Case Generation Tool for Android Applications Using Q-Learning;;Symmetry;;2073-8994;10.3390/sym13020310;https://www.mdpi.com/2073-8994/13/2/310;Android applications provide benefits to mobile phone users by offering operative functionalities and interactive user interfaces. However, application crashes give users an unsatisfactory experience, and negatively impact the application’s overall rating. Android application crashes can be avoided through intensive and extensive testing. In the related literature, the graphical user interface (GUI) test generation tools focus on generating tests and exploring application functions using different approaches. Such tools must choose not only which user interface element to interact with, but also which type of action to be performed, in order to increase the percentage of code coverage and to detect faults with a limited time budget. However, a common limitation in the tools is the low code coverage because of their inability to find the right combination of actions that can drive the application into new and important states. A Q-Learning-based test coverage approach developed in DroidbotX was proposed to generate GUI test cases for Android applications to maximize instruction coverage, method coverage, and activity coverage. The overall performance of the proposed solution was compared to five state-of-the-art test generation tools on 30 Android applications. The DroidbotX test coverage approach achieved 51.5% accuracy for instruction coverage, 57% for method coverage, and 86.5% for activity coverage. It triggered 18 crashes within the time limit and shortest event sequence length compared to the other tools. The results demonstrated that the adaptation of Q-Learning with upper confidence bound (UCB) exploration outperforms other existing state-of-the-art solutions.;2021-02-12;02.05.2025 11:53;05.05.2025 12:09;02.05.2025 11:53;310;;2.0;13.0;;Symmetry;DroidbotX;;;;;;;en;https://creativecommons.org/licenses/by/4.0/;;;;DOI.org (Crossref);;;;C:\Users\DK00747\Zotero\storage\N8LJFEL7\Yasin et al. - 2021 - DroidbotX Test Case Generation Tool for Android Applications Using Q-Learning.pdf;;"1. LLM; 1.3. Fine-tuned / Instruction-tuned; 2. ML; 2.5. Reinforcement Learning Strategies; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.3. Functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
WUV4AAR3;journalArticle;2023;"Dakhel, Arghavan Moradi; Nikanjam, Amin; Majdinasab, Vahid; Khomh, Foutse; Desmarais, Michel C.";Effective test generation using pre-trained large language models and mutation testing;;Information and Software Technology;;;10.48550/arxiv.2308.16557;;One of the critical phases in software development is software testing. Testing helps with identifying potential bugs and reducing maintenance costs. The goal of automated test generation tools is to ease the development of tests by suggesting efficient bug-revealing tests. Recently, researchers have leveraged Large Language Models (LLMs) of code to generate unit tests. While the code coverage of generated tests was usually assessed, the literature has acknowledged that the coverage is weakly correlated with the efficiency of tests in bug detection. To improve over this limitation, in this paper, we introduce MuTAP for improving the effectiveness of test cases generated by LLMs in terms of revealing bugs by leveraging mutation testing. Our goal is achieved by augmenting prompts with surviving mutants, as those mutants highlight the limitations of test cases in detecting bugs. MuTAP is capable of generating effective test cases in the absence of natural language descriptions of the Program Under Test (PUTs). We employ different LLMs within MuTAP and evaluate their performance on different benchmarks. Our results show that our proposed method is able to detect up to 28;2023;30.04.2025 13:51;05.05.2025 12:09;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: 4386384572 tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\SSX8XCAH\Dakhel et al. - 2023 - Effective test generation using pre-trained large language models and mutation testing.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.6. Failure Detection; 3.5. Robustness / Non-functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
WVZMZAL2;journalArticle;2024;"Feng, Sidong; Lu, Haochuan; Jiang, Jianqin; Xiong, Ting; Huang, Likun; Liang, Yinglin; Li, Xiaoqin; Deng, Yuetang; Aleti, A.";Enabling cost-effective UI automation testing with retrieval-based llms: a case study in WeChat;;International Conference on Automated Software Engineering;;;10.48550/arxiv.2409.07829;;;2024;30.04.2025 13:51;05.05.2025 12:09;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\9WFA2HMR\Feng et al. - 2024 - Enabling cost-effective UI automation testing with retrieval-based llms a case study in WeChat.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 1.3. Fine-tuned / Instruction-tuned; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.6. Failure Detection; 3.3. Functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.4. Visual / GUI-based Prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
QRH6YFRS;journalArticle;2024;"Su, Yanqi; Liao, Dianshu; Xing, Zhenchang; Huang, Qing; Xie, Mulong; Lu, Qinghua; Xu, Xiwei";Enhancing exploratory testing by large language model and knowledge graph;;International Conference on Software Engineering;;;10.1145/3597503.3639157;;Exploratory testing leverages the tester's knowledge and creativity to design test cases for effectively uncovering system-level bugs from the end user's perspective. Researchers have worked on test scenario generation to support exploratory testing based on a system knowledge graph, enriched with scenario and oracle knowledge from bug reports. Nevertheless, the adoption of this approach is hindered by difficulties in handling bug reports of inconsistent quality and varied expression styles, along with the infeasibility of the generated test scenarios. To overcome these limitations, we utilize the superior natural language understanding (NLU) capabilities of Large Language Models (LLMs) to construct a System KG of User Tasks and Failures (SysKG-UTF). Leveraging the system and bug knowledge from the KG, along with the logical reasoning capabilities of LLMs, we generate test scenarios with high feasibility and coherence. Particularly, we design chain-of-thought (CoT) reasoning to extract human-like knowledge and logical reasoning from LLMs, simulating a developer's process of validating test scenario feasibility. Our evaluation shows that our approach significantly enhances the KG construction, particularly for bug reports with low quality. Furthermore, our approach generates test scenarios with high feasibility and coherence. The user study further proves the effectiveness of our generated test scenarios in supporting exploratory testing. Specifically, 8 participants find 36 bugs from 8 seed bugs in two hours using our test scenarios, a significant improvement over the 21 bugs found by the state-of-the-art baseline.;2024;30.04.2025 13:51;05.05.2025 12:09;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\LZV9DEJ6\Su et al. - 2024 - Enhancing exploratory testing by large language model and knowledge graph.pdf;;"1. LLM; 1.3. Fine-tuned / Instruction-tuned; 2. ML; 2.1. Traditional Machine Learning; 2.1.2. Feature-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.2. Test Data Generation; 3.2.2. Test Documentation; 3.3. Functional Testing; 3.5. Robustness / Non-functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.2. Few-shot / Zero-shot; 5.4. Visual / GUI-based Prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
IZZY53CR;journalArticle;2022;"Brisset, Sacha; Rouvoy, Romain; Seinturier, Lionel; Pawlak, Renaud";Erratum: Leveraging Flexible Tree Matching to repair broken locators in web automation scripts;12;Information and Software Technology;;9505849.0;10.1016/j.infsof.2021.106754;https://linkinghub.elsevier.com/retrieve/pii/S0950584921002020;;2022-04;04.05.2025 10:30;05.05.2025 12:09;04.05.2025 10:30;106754;;;144.0;;Information and Software Technology;Erratum;;;;;;;en;;;;;DOI.org (Crossref);;;;C:\Users\DK00747\Zotero\storage\ZLUUWQHD\Brisset et al. - 2022 - Erratum Leveraging Flexible Tree Matching to repair broken locators in web automation scripts.pdf;;"3. Software Testing; 3.1. Test Automation; 3.1.4. Test Repair; 3.1.7. Self-healing Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
HFX2SJDH;journalArticle;2024;"Yuan, Zhiqiang; Liu, Mingwei; Ding, Shiji; Wang, Kaixin; Chen, Yixuan; Peng, Xin; Lou, Yiling";Evaluating and improving ChatGPT for unit test generation;;Proc. ACM Softw. Eng.;;;10.1145/3660783;;"Unit testing plays an essential role in detecting bugs in functionally-discrete program units (e.g., methods). Manually writing high-quality unit tests is time-consuming and laborious. Although the traditional techniques are able to generate tests with reasonable coverage, they are shown to exhibit low readability and still cannot be directly adopted by developers in practice. Recent work has shown the large potential of large language models (LLMs) in unit test generation. By being pre-trained on a massive developer-written code corpus, the models are capable of generating more human-like and meaningful test code. In this work, we perform the first empirical study to evaluate the capability of ChatGPT (i.e., one of the most representative LLMs with outstanding performance in code generation and comprehension) in unit test generation. In particular, we conduct both a quantitative analysis and a user study to systematically investigate the quality of its generated tests in terms of correctness, sufficiency, readability, and usability. We find that the tests generated by ChatGPT still suffer from correctness issues, including diverse compilation errors and execution failures (mostly caused by incorrect assertions); but the passing tests generated by ChatGPT almost resemble manually-written tests by achieving comparable coverage, readability, and even sometimes developers' preference. Our findings indicate that generating unit tests with ChatGPT could be very promising if the correctness of its generated tests could be further improved. Inspired by our findings above, we further propose ChatTester, a novel ChatGPT-based unit test generation approach, which leverages ChatGPT itself to improve the quality of its generated tests. ChatTester incorporates an initial test generator and an iterative test refiner. Our evaluation demonstrates the effectiveness of ChatTester by generating 34.3";2024;30.04.2025 13:51;05.05.2025 12:10;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\FYGBSNZS\Yuan et al. - 2024 - Evaluating and improving ChatGPT for unit test generation.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.4. Test Repair; 3.1.8. Test Quality Analysis";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
QUYFKD4S;journalArticle;2024;"Li, Yihao; Liu, Pan; Wang, Haiyang; Chu, Jie; Wong, W. E.";Evaluating large language models for software testing;;;;;10.1016/j.csi.2024.103942;;;2024;30.04.2025 13:51;05.05.2025 12:10;;;;;;;Comput. Stand. Interfaces;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\JJ8MFKM5\Li et al. - 2024 - Evaluating large language models for software testing.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.6. Failure Detection; 3.1.8. Test Quality Analysis; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.3. Instructional / Role prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
79N7UN4V;journalArticle;2024;"Al-khafaji, N.; Majeed, Basit Khalaf";Evaluating large language models using arabic prompts to generate python codes;;2024 4th International Conference on Emerging Smart Technologies and Applications (eSmarTA);;;10.1109/esmarta62850.2024.10638877;;Currently, the popularity of large language models (LLMs) for instance, ChatGPT from OpenAI and Gemini from Google is increasing greatly in our lives, due to their unparalleled performance in various applications. These models play a vital role in both academic and industrial fields. With this popularity, evaluating these models has become extremely important, especially when using the Arabic language. Despite the increasing popularity and performance of AI, there have been no empirical studies evaluating the use of LLMs for Arabic prompts in the field of code generation. However, the code generation in LLM can be strongly influenced by the choice of prompt. Evaluating the LLMs by Arabic prompts helps us better understand the strengths and weaknesses of these models. Therefore, we highlighted the evaluation of the most popular LLM programs (Chatgpt-3.5, ChatGPT-4 and Gemini) when generating Python codes based on Arabic prompts. In this study we employed CodeBLUE score and Flake8 as a metric to evaluate the LLMs capabilities for code generation via Arabic prompts. Our results indicate significant differences in performance across different LLMs and prompts levels. This study lays the foundation for further research into LLM capabilities based on Arabic prompts and suggests practical implications for using LLM in automated code generation and test-driven development tasks.;2024;30.04.2025 13:51;05.05.2025 12:10;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 4.2. Development Process Automation; 4.2.3. Test-Driven Development; 4.2.4. Low-code / No-code Development; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
LAPHWCNF;journalArticle;2025;"Godage, Tharindu; Nimishan, Sivaraj; Vasanthapriyan, S.; Vigneshwaran, Palanisamy; Joseph, Charles; Thuseethan, S.";Evaluating the effectiveness of large language models in automated unit test generation;;2025 5th International Conference on Advanced Research in Computing (ICARC);;;10.1109/icarc64760.2025.10962997;;The increasing use of Artificial Intelligence (AI) in software development underscores the need to select suitable Large Language Models (LLMs) for automating software unit test generation. No prior work has been conducted to evaluate the performance of LLM in this domain. To address this gap, this study evaluates the effectiveness of four prominent LLMs—GPT-4, Claude 3.5, Command-R-08-2024 and Llama 3.1—in generating unit test cases. This study particularly aims to evaluate the performance of these models in real-world testing scenarios. Hence, 106 test cases from 23 test suites based on interviews with software experts and QA engineers are used to ensure relevance and comprehensiveness. These test cases are analyzed using JavaScript Engines Specification Tester (JEST) for code coverage and Stryker for mutation testing while adopting both quantitative and qualitative analysis. The findings reveal that Claude 3.5 consistently outperforms the other models against test success rate, statement coverage, and mutation score with the achieved accuracy of 93.33;2025;30.04.2025 13:51;05.05.2025 12:10;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.5. Robustness / Non-functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
CIWVJU23;journalArticle;2024;"Konuk, Metin; Ba?lum, Cem; Yayan, U?ur";Evaluation of large language models for unit test generation;;2024 Innovations in Intelligent Systems and Applications Conference (ASYU);;;10.1109/asyu62119.2024.10756954;;In recent years, Artificial Intelligence (AI) has significantly transformed various industries, especially software development, through automation and enhanced decision-making processes. Traditional software testing, often manual and error-prone, cannot keep up with rapid development cycles and complex systems, leading to extended development times, higher costs, and undetected bugs. This study develops an AI-based platform using OpenAI models to generate and execute unit tests across multiple programming languages. By leveraging Large Language Models (LLMs) like GPT, we automate unit test creation, demonstrating proficiency in understanding and generating natural language to interpret code. Our web-based system architecture ensures efficient test generation and execution, significantly reducing manual effort and mitigating human error, thus revolutionizing software testing. Furthermore, we introduce unique evaluation metrics such as “Is Executable” and “Assertion Count” to assess the performance and effectiveness of the generated unit tests, providing a comprehensive measure of the models' capabilities.;2024;30.04.2025 13:51;05.05.2025 12:10;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.3. Functional Testing; 3.4. Integration Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
UNNH7VZJ;journalArticle;2025;"Wang, Yuchen; Guo, Shangxin; Tan, Chee Wei";From code generation to software testing: AI copilot with context-based RAG;;IEEE Software;;;10.1109/ms.2025.3549628;;The rapid pace of large-scale software development places increasing demands on traditional testing methodologies, often leading to bottlenecks in efficiency, accuracy, and coverage. We propose a novel perspective on software testing by positing bug detection and coding with fewer bugs as two interconnected problems that share a common goal, which is reducing bugs with limited resources. We extend our previous work on AI-assisted programming, which supports code auto-completion and chatbot-powered Q&A, to the realm of software testing. We introduce Copilot for Testing, an automated testing system that synchronizes bug detection with codebase updates, leveraging context-based Retrieval Augmented Generation (RAG) to enhance the capabilities of large language models (LLMs). Our evaluation demonstrates a 31.2;2025;30.04.2025 13:51;05.05.2025 12:10;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\D3CF75YJ\Wang et al. - 2025 - From code generation to software testing AI copilot with context-based RAG.pdf;;"2. ML; 2.1. Traditional Machine Learning; 2.1.2. Feature-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.6. Failure Detection; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 4.1.4. Source Code Analysis; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.4. Visual / GUI-based Prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
QLU9XF54;conferencePaper;2024;"Hoffmann, Jacob; Frister, Demian";Generating software tests for mobile applications using fine-tuned large language models;6;Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024);;;;https://dl.acm.org/doi/abs/10.1145/3644032.3644454;;2024;30.04.2025 11:55;05.05.2025 12:10;30.04.2025 11:55;76–77;;;;;;;;;;;;;;;;;;Google Scholar;;;;C:\Users\DK00747\Zotero\storage\EM7KFRX2\Hoffmann and Frister - 2024 - Generating software tests for mobile applications .pdf;;"1. LLM; 1.1. Transformer-based Models; 1.3. Fine-tuned / Instruction-tuned; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.3. Functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
RWEBGG3H;journalArticle;2024;"Tamagnan, Frédéric; Vernotte, Alexandre; Bouquet, F.; Legeard, B.";Generation of regression tests from logs with clustering guided by usage patterns;;Software testing, verification & reliability;;;10.1002/stvr.1900;;Clustering is increasingly being used to select the appropriate test suites. In this paper, we apply this approach to regression testing. Regression testing is the practice of verifying the robustness and reliability of software by retesting after changes have been made. Creating and maintaining functional regression tests is a laborious and costly activity. To be effective, these tests must represent the actual user journeys of the application. In addition, an optimal number of test cases is critical for the rapid execution of the regression test suite to stay within the time and computational resource budget as it is re?run at each major iteration of the software development. Therefore, the selection and maintenance of functional regression tests based on the analysis of application logs has gained popularity in recent years. This paper presents a novel approach to improve regression testing by automating the creation of test suites using user traces fed into clustering pipelines. Our methodology introduces a new metric based on pattern mining to quantify the statistical coverage of prevalent user paths. This metric helps to determine the optimal number of clusters within a clustering pipeline, thus addressing the challenge of suboptimal test suite sizes. Additionally, we introduce two criteria, to systematically evaluate and rank clustering pipelines. Experimentation involving 33 variations of clustering pipelines across four datasets demonstrates the potential effectiveness of our automated approach compared with manually crafted test suites. (All the experiments and data on Scanner, Spree and Booked Scheduler are available at https://github.com/frederictamagnan/STVR2024.) Then, we analyse the semantics of the clusters based on their principal composing patterns.;2024;30.04.2025 13:51;05.05.2025 12:10;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\3EC8JJJ8\Tamagnan et al. - 2024 - Generation of regression tests from logs with clustering guided by usage patterns.pdf;;"2. ML; 2.1. Traditional Machine Learning; 2.1.1. SVM / Decision Trees / Random Forest; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.2. Test Optimization; 3.1.3. Test Classification; 3.1.5. Test Prioritization; 3.1.8. Test Quality Analysis; 3.5. Robustness / Non-functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
UG96KKEE;journalArticle;2023;"Zimmermann, Daniel E.; Koziolek, Anne";GUI-based software testing: An automated approach using GPT-4 and selenium WebDriver;;2023 38th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW);;;10.1109/asew60602.2023.00028;;This paper presents a novel method for GUI testing in web applications that largely automates the process by integrating the advanced language model GPT-4 with Selenium, a popular web application testing framework. Unlike traditional deep learning approaches, which require extensive training data, GPT-4 is pre-trained on a large corpus, giving it significant generalisation and inference capabilities. These capabilities allow testing without the need for recorded data from human testers, significantly reducing the time and effort required for the testing process. We also compare the efficiency of our integrated GPT-4 approach with monkey testing, a widely used technique for automated GUI testing where user input is randomly generated. To evaluate our approach, we implemented a web calculator with an integrated code coverage system. The results show that our integrated GPT-4 approach provides significantly better branch coverage compared to monkey testing. These results highlight the significant potential of integrating specific AI models such as GPT-4 and automated testing tools to improve the accuracy and efficiency of GUI testing in web applications.;2023;30.04.2025 13:51;05.05.2025 12:10;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: 4388214674 tex.pmcid: null;;;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.3. Functional Testing; 4. Software Engineering";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
GIDD2BNY;journalArticle;2025;"Rehan, Shaheer; Al-Bander, Baidaa; Ahmad, Amro Al-Said";Harnessing large language models for automated software testing: a leap towards scalable test case generation;;Electronics;;;10.3390/electronics14071463;;Software testing is critical for ensuring software reliability, with test case generation often being resource-intensive and time-consuming. This study leverages the Llama-2 large language model (LLM) to automate unit test generation for Java focal methods, demonstrating the potential of AI-driven approaches to optimize software testing workflows. Our work leverages focal methods to prioritize critical components of the code to produce more context-sensitive and scalable test cases. The dataset, comprising 25,000 curated records, underwent tokenization and QLoRA quantization to facilitate training. The model was fine-tuned, achieving a training loss of 0.046. These results show the promise of AI-driven test case generation and underscore the feasibility of using fine-tuned LLMs for test case generation, highlighting opportunities for improvement through larger datasets, advanced hyperparameter optimization, and enhanced computational resources. We conducted a human-in-the-loop validation on a subset of unit tests generated by our fined-tuned LLM. This confirms that these tests effectively leverage focal methods, demonstrating the model’s capability to generate more contextually accurate unit tests. The work suggests the need to develop novel validation objective metrics specifically tailored for the automation of test cases generated by utilizing large language models. This work establishes a foundation for scalable and efficient software testing solutions driven by artificial intelligence. The data and code are publicly available on GitHub;2025;30.04.2025 13:51;05.05.2025 12:10;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\2JJJ6QJC\Rehan et al. - 2025 - Harnessing large language models for automated software testing a leap towards scalable test case g.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.3. Fine-tuned / Instruction-tuned; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.3. Functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
9LLJF28B;journalArticle;2024;"Wang, Zejun; Liu, Kaibo; Li, Ge; Jin, Zhi";HITS: High-coverage LLM-based unit test generation via method slicing;;International Conference on Automated Software Engineering;;;10.48550/arxiv.2408.11324;;"Large language models (LLMs) have behaved well in generating unit tests for Java projects. However, the performance for covering the complex focal methods within the projects is poor. Complex methods comprise many conditions and loops, requiring the test cases to be various enough to cover all lines and branches. However, existing test generation methods with LLMs provide the whole method-to-test to the LLM without assistance on input analysis. The LLM has difficulty inferring the test inputs to cover all conditions, resulting in missing lines and branches. To tackle the problem, we propose decomposing the focal methods into slices and asking the LLM to generate test cases slice by slice. Our method simplifies the analysis scope, making it easier for the LLM to cover more lines and branches in each slice. We build a dataset comprising complex focal methods collected from the projects used by existing state-of-the-art approaches. Our experiment results show that our method significantly outperforms current test case generation methods with LLMs and the typical SBST method Evosuite regarding both line and branch coverage scores.CCS CONCEPTS• Software and its engineering ? Software testing and debugging; • Computing methodologies ? Natural language processing.";2024;30.04.2025 13:51;05.05.2025 12:10;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\LZNVM7XP\Wang et al. - 2024 - HITS High-coverage LLM-based unit test generation via method slicing.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.4. Integration Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
I6DL9G4C;journalArticle;2024;Kähkönen, Samu;Improving software development workflows using generative AI;0;;;;;https://aaltodoc.aalto.fi/items/26cf8b65-5f4e-4a24-938f-2597bd51ebc7;;2024;30.04.2025 11:58;05.05.2025 12:10;30.04.2025 11:58;;;;;;;;;;;;;;;;;;;Google Scholar;;;;C:\Users\DK00747\Zotero\storage\FEFHAMNY\Kähkönen - 2024 - Improving software development workflows using gen.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 4.1.2. Code Repair / Refactoring; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.5. Self-improving Prompts";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
I3DXZVXX;journalArticle;2023;Gay, Gregory;Improving the readability of generated tests using GPT-4 and ChatGPT code interpreter;;International Symposium on Search Based Software Engineering;;;10.1007/978-3-031-48796-5_11;;;2023;30.04.2025 13:51;05.05.2025 12:11;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.8. Test Quality Analysis; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
ZQ828H6L;journalArticle;2024;"Santos, Shexmo; Fernandes, Raiane Eunice S.; dos Santos, Marcos Cesar Barbosa; Soares, Michel S.; Rocha, F?bio Gomes; Marczak, Sabrina";Increasing test coverage by automating BDD tests in proofs of concepts (pocs) using LLM;;Brazilian Symposium on Software Quality;;;10.1145/3701625.3701637;;;2024;30.04.2025 13:51;05.05.2025 12:11;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\N5K4622Y\Santos et al. - 2024 - Increasing test coverage by automating BDD tests in proofs of concepts (pocs) using LLM.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.2. Test Optimization; 3.1.8. Test Quality Analysis; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
LDD4WQSP;journalArticle;2024;"Yoon, Juyeon; Feldt, R.; Yoo, Shin";Intent-driven mobile GUI testing with autonomous large language model agents;;International Conference on Information Control Systems & Technologies;;;10.1109/icst60714.2024.00020;;GUI testing checks if a software system behaves as expected when users interact with its graphical interface, e.g., testing specific functionality or validating relevant use case scenarios. Currently, deciding what to test at this high level is a manual task since automated GUI testing tools target lower level adequacy metrics such as structural code coverage or activity coverage. We propose DroidAgent, an autonomous GUI testing agent for Android, for semantic, intent-driven automation of GUI testing. It is based on Large Language Models and support mechanisms such as long- and short-term memory. Given an Android app, DroidAgent sets relevant task goals and subsequently tries to achieve them by interacting with the app. Our empirical evaluation of DroidAgent using 15 apps from the Themis benchmark shows that it can set up and perform realistic tasks, with a higher level of autonomy. For example, when testing a messaging app, DroidAgent created a second account and added a first account as a friend, testing a realistic use case, without human intervention. On average, DroidAgent achieved 61;2024;30.04.2025 13:51;05.05.2025 12:11;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.3. Test Classification; 3.1.5. Test Prioritization; 3.1.8. Test Quality Analysis; 3.3. Functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.4. Visual / GUI-based Prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
CJG9YKEG;journalArticle;2023;"Jabbar, Emad; Hemmati, Hadi; Feldt, R.";Investigating execution trace embedding for test case prioritization;;International Conference on Software Quality, Reliability and Security;;;10.1109/qrs60937.2023.00036;;Most automated software testing tasks, such as test case generation, selection, and prioritization, can benefit from an abstract representation of test cases. Although test case representation usually is not explicitly discussed in software literature, but traditionally test cases are mostly represented based on what they cover in source code (e.g., which statements or branches), which is in fact an abstract representation. In this paper, we hypothesize that execution traces of test cases, as representations of their behaviour, can be leveraged to better encode test cases compared to code-based coverage information, for automated testing tasks. To validate this hypothesis, we propose an embedding approach, Test2Vec, based on an state-of-the-art neural program embedding (CodeBert), where the encoder maps test execution traces, i.e. sequences of method calls with their inputs and return values, to fixed-length, numerical vectors. We evaluate this representation in automated test case prioritization (TP) task. Our TP method is a classifier trained on the passing and failing vectors of historical test cases, in regression testing. We compare our embedding with multiple baselines and related work including CodeBert itself. The empirical study is based on 250 real faults and 703,353 seeded faults (mutants) over 250 revisions of 10 open-source Java projects from Defects4J, with a total of over 1,407,206 execution traces. Results show that our approach improves all alternatives, significantly, with respect to studied metrics. We also show that both inputs and outputs of a method are important elements of the execution-based embedding.;2023;30.04.2025 13:51;05.05.2025 12:11;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\BFLL9TJ2\Jabbar et al. - 2023 - Investigating execution trace embedding for test case prioritization.pdf;;"2. ML; 2.2. Deep Learning; 2.2.1. CNN / RNN / LSTM; 3. Software Testing; 3.1. Test Automation; 3.1.5. Test Prioritization";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
PUVEHHQ5;journalArticle;2024;"Le, Tri; Tran, Thien; Cao, Duy; Le, Vy; Nguyen, Tien; Nguyen, Vu";KAT: Dependency-aware automated API testing with large language models;;International Conference on Information Control Systems & Technologies;;;10.1109/icst60714.2024.00017;;API testing has increasing demands for software companies. Prior API testing tools were aware of certain types of dependencies that needed to be concise between operations and parameters. However, their approaches, which are mostly done manually or using heuristic-based algorithms, have limitations due to the complexity of these dependencies. In this paper, we present KAT (Katalon API Testing), a novel AI -driven approach that leverages the large language model GPT in conjunction with advanced prompting techniques to autonomously generate test cases to validate RESTful APIs. Our comprehensive strategy encompasses various processes to construct an operation depen-dency graph from an OpenAPI specification and to generate test scripts, constraint validation scripts, test cases, and test data. Our evaluation of KAT using 12 real-world RESTful services shows that it can improve test coverage, detect more undocumented status codes, and reduce false positives in these services in comparison with a state-of-the-art automated test generation tool. These results indicate the effectiveness of using the large language model for generating test scripts and data for API testing.;2024;30.04.2025 13:51;05.05.2025 12:11;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\QYWUTNZB\Le et al. - 2024 - KAT Dependency-aware automated API testing with large language models.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.2. Test Data Generation; 3.2.3. Test Datasets (HTML, Screenshots, UML); 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
7AKAHNH5;journalArticle;2023;"Hou, Xinyi; Zhao, Yanjie; Liu, Yue; Zhou, Yang; Wang, Kailong; Li, Li; Luo, Xiapu; Lo, David; Grundy, John; Wang, Haoyu";Large language models for software engineering: a systematic literature review;;ACM Transactions on Software Engineering and Methodology;;;10.48550/arxiv.2308.10620;;Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We collect and analyze 229 research papers from 2017 to 2023 to answer four key research questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, preprocessing, and application highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and flagging promising areas for future study.;2023;30.04.2025 13:51;05.05.2025 12:11;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: 4386081573 tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\XJPGRN48\Hou et al. - 2023 - Large language models for software engineering a systematic literature review.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.2. Test Data Generation; 3.2.2. Test Documentation; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 4.1.4. Source Code Analysis; 4.2. Development Process Automation; 4.2.2. Debugging Support; 4.2.4. Low-code / No-code Development; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
C9BYRTGN;conferencePaper;2024;"Görmez, Muhammet Kür?at; Y?lmaz, Murat; Clarke, Paul M.";Large Language Models for Software Engineering: A Systematic Mapping Study;3;European Conference on Software Process Improvement;;;;https://link.springer.com/chapter/10.1007/978-3-031-71139-8_5;;2024;30.04.2025 12:00;05.05.2025 12:11;30.04.2025 12:00;64–79;;;;;;Large Language Models for Software Engineering;;;;;Springer;;;;;;;Google Scholar;;;;;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 1.3. Fine-tuned / Instruction-tuned; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
A683E4YG;journalArticle;2025;"Far, Seyed Mohammad Taghavi; Feyzi, Farid";Large language models for software vulnerability detection: a guide for researchers on models, methods, techniques, datasets, and metrics;;;;;10.1007/s10207-025-00992-7;;;2025;30.04.2025 13:51;05.05.2025 12:11;;;;;;;Int. J. Inf. Sec.;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
8F6N2VSV;journalArticle;2025;Bektas, Ali;Large Language Models in Software Engineering: A Critical Review of Evaluation Strategies;0;;;;;https://www.inf.fu-berlin.de/inst/ag-se/theses/Bektas25-LLM-evaluation-strategies.pdf;;2025;30.04.2025 11:57;05.05.2025 12:35;30.04.2025 11:57;;;;;;;Large Language Models in Software Engineering;;;;;;;;;;;;Google Scholar;;;;C:\Users\DK00747\Zotero\storage\3KS2CIX6\Bektas - Large Language Models in Software Engineering A C.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 1.3. Fine-tuned / Instruction-tuned; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
MUVUHTYW;journalArticle;2024;"Crandall, Johannah L.; Crandall, Aaron S.";Large language model-supported software testing with the CS matrix taxonomy;0;"Journal of Computing Sciences in Colleges (JCSC; Formerly: Journal of Computing in Small Colleges)";;;10.5555/3715602.3715612;;;2024;30.04.2025 13:51;05.05.2025 12:11;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
KEJXQR6U;conferencePaper;2023;"Nie, Pengyu; Banerjee, Rahul; Li, Junyi Jessy; Mooney, Raymond J.; Gligoric, Milos";Learning Deep Semantics for Test Completion;;2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE);978-1-6654-5701-9;;10.1109/ICSE48619.2023.00178;https://ieeexplore.ieee.org/document/10172620/;;2023-05;02.05.2025 11:54;05.05.2025 12:11;02.05.2025 11:54;2111-2123;;;;;;;;;;;IEEE;Melbourne, Australia;;https://doi.org/10.15223/policy-029;;;;DOI.org (Crossref);;;;C:\Users\DK00747\Zotero\storage\DR5JIXYJ\Nie et al. - 2023 - Learning Deep Semantics for Test Completion.pdf;;"2. ML; 2.2. Deep Learning; 3. Software Testing; 3.1. Test Automation; 3.1.8. Test Quality Analysis; 3.2. Test Data Generation; 3.2.1. Test Oracle";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE);;;;;;;;;;;;;;;
ERHD8LK6;journalArticle;2023;"Kim, Myeongsoo; Stennett, Tyler; Shah, Dhruv; Sinha, Saurabh; Orso, A.";Leveraging large language models to improve REST API testing;;2024 IEEE/ACM 46th International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER);;;10.1145/3639476.3639769;;The widespread adoption of REST APIs, coupled with their growing complexity and size, has led to the need for automated REST API testing tools. Current tools focus on the structured data in REST API specifications but often neglect valuable insights available in unstructured natural-language descriptions in the specifications, which leads to suboptimal test coverage. Recently, to address this gap, researchers have developed techniques that extract rules from these human-readable descriptions and query knowledge bases to derive meaningful input values. However, these techniques are limited in the types of rules they can extract and prone to produce inaccurate results. This paper presents RESTGPT, an innovative approach that leverages the power and intrinsic context-awareness of Large Language Models (LLMs) to improve REST API testing. RESTGPT takes as input an API specification, extracts machine-interpretable rules, and generates example parameter values from natural-language descriptions in the specification. It then augments the original specification with these rules and values. Our evaluations indicate that RESTGPT outperforms existing techniques in both rule extraction and value generation. Given these promising results, we outline future research directions for advancing REST API testing through LLMs.;2023;30.04.2025 13:51;05.05.2025 12:12;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\CQJ3QAYW\Kim et al. - 2023 - Leveraging large language models to improve REST API testing.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.2. Test Data Generation; 3.2.3. Test Datasets (HTML, Screenshots, UML)";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
75H88SDE;journalArticle;2024;"Yin, Hang; Mohammed, Hamza; Boyapati, Sai";Leveraging pre-trained large language models (llms) for on-premises comprehensive automated test case generation: An empirical study;;2024 9th International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS);;;10.1109/iciibms62405.2024.10792720;;The rapidly evolving field of Artificial Intelligence (AI)-assisted software testing has predominantly focused on automated test code generation, with limited research exploring the realm of automated test case generation from user stories requirements. This paper presents a comprehensive empirical study on harnessing pre-trained Large Language Models (LLMs) for generating concrete test cases from natural language requirements given in user stories. We investigate the efficacy of various prompting and alignment techniques, including prompt chaining, few-shot instructions, and agency-based approaches, to facilitate secure on-premises deployment. By integrating our learnings with an on-premises model setup, wherein we deploy a RoPE scaled 4-bit quantized LLaMA 3 70B Instruct model, optionally augmented with LoRA adapters trained on QA datasets, we demonstrate that this approach yields more accurate and consistent test cases despite video random-access memory (VRAM) constraints, thereby maintaining the security benefits of an on-premises deployment.;2024;30.04.2025 13:51;05.05.2025 12:12;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\5P52GHGZ\Yin et al. - 2024 - Leveraging pre-trained large language models (llms) for on-premises comprehensive automated test cas.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.3. Fine-tuned / Instruction-tuned; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
XYXV6PIG;journalArticle;2023;"Yu, Shengcheng; Fang, Chunrong; Ling, Yucheng; Wu, Chentian; Chen, Zhenyu";LLM for test script generation and migration: Challenges, capabilities, and opportunities;;International Conference on Software Quality, Reliability and Security;;;10.1109/qrs60937.2023.00029;;This paper investigates the application of large language models (LLM) in the domain of mobile application test script generation. Test script generation is a vital component of software testing, enabling efficient and reliable automation of repetitive test tasks. However, existing generation approaches often encounter limitations, such as difficulties in accurately capturing and reproducing test scripts across diverse devices, platforms, and applications. These challenges arise due to differences in screen sizes, input modalities, platform behaviors, API inconsistencies, and application architectures. Overcoming these limitations is crucial for achieving robust and comprehensive test automation.By leveraging the capabilities of LLMs, we aim to address these challenges and explore its potential as a versatile tool for test automation. We investigate how well LLMs can adapt to diverse devices and systems while accurately capturing and generating test scripts. Additionally, we evaluate its cross-platform generation capabilities by assessing its ability to handle operating system variations and platform-specific behaviors. Furthermore, we explore the application of LLMs in cross-app migration, where it generates test scripts across different applications and software environments based on existing scripts.Throughout the investigation, we analyze its adaptability to various user interfaces, app architectures, and interaction patterns, ensuring accurate script generation and compatibility. The findings of this research contribute to the understanding of LLMs’ capabilities in test automation. Ultimately, this research aims to enhance software testing practices, empowering app developers to achieve higher levels of software quality and development efficiency.;2023;30.04.2025 13:51;05.05.2025 12:12;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\E3TB4AAQ\Yu et al. - 2023 - LLM for test script generation and migration Challenges, capabilities, and opportunities.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.4. Test Repair; 3.2. Test Data Generation; 3.2.3. Test Datasets (HTML, Screenshots, UML); 3.3. Functional Testing; 3.5. Robustness / Non-functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.3. Instructional / Role prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
FCC66653;journalArticle;2025;"Rincon, André Mesquita; Vincenzi, A.; Faria, Jo?o Pascoal";LLM prompt engineering for automated white-box integration test generation in REST apis;;International Conference on Software Testing, Verification and Validation Workshops;;;10.1109/icstw64639.2025.10962507;;This study explores prompt engineering for automated white-box integration testing of RESTful APIs using Large Language Models (LLMs). Four versions of prompts were designed and tested across three OpenAI models (GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o) to assess their impact on code coverage, token consumption, execution time, and financial cost. The results indicate that different prompt versions, especially with more advanced models, achieved up to 90;2025;30.04.2025 13:51;05.05.2025 12:12;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.4. Integration Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
YNXWCR9X;journalArticle;2024;"Xue, Zhiyi; Li, Liangguo; Tian, Senyue; Chen, Xiaohong; Li, Pingping; Chen, Liangyu; Jiang, Tingting; Zhang, Min";LLM4Fin: Fully automating LLM-powered test case generation for FinTech software acceptance testing;;International Symposium on Software Testing and Analysis;;;10.1145/3650212.3680388;;;2024;30.04.2025 13:51;05.05.2025 12:12;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\CTPZK432\Xue et al. - 2024 - LLM4Fin Fully automating LLM-powered test case generation for FinTech software acceptance testing.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 1.3. Fine-tuned / Instruction-tuned; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.3. Functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
T5BWHIJI;journalArticle;2024;"Paduraru, C.; Staicu, A.; Stefanescu, Alin";LLM-based methods for the creation of unit tests in game development;;International Conference on Knowledge-Based Intelligent Information & Engineering Systems;;;10.1016/j.procs.2024.09.473;;;2024;30.04.2025 13:51;05.05.2025 12:12;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\ABZWYUJW\Paduraru et al. - 2024 - LLM-based methods for the creation of unit tests in game development.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.3. Fine-tuned / Instruction-tuned; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.3. Functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
7ZC6CDFN;journalArticle;2024;"He, Junda; Treude, Christoph; Lo, David";LLM-based multi-agent systems for software engineering: Literature review, vision and the road ahead;;ACM Transactions on Software Engineering and Methodology;;;10.1145/3712003;;Integrating Large Language Models (LLMs) into autonomous agents marks a significant shift in the research landscape by offering cognitive abilities that are competitive with human planning and reasoning. This paper explores the transformative potential of integrating Large Language Models into Multi-Agent (LMA) systems for addressing complex challenges in software engineering (SE). By leveraging the collaborative and specialized abilities of multiple agents, LMA systems enable autonomous problem-solving, improve robustness, and provide scalable solutions for managing the complexity of real-world software projects. In this paper, we conduct a systematic review of recent primary studies to map the current landscape of LMA applications across various stages of the software development lifecycle (SDLC). To illustrate current capabilities and limitations, we perform two case studies to demonstrate the effectiveness of state-of-the-art LMA frameworks. Additionally, we identify critical research gaps and propose a comprehensive research agenda focused on enhancing individual agent capabilities and optimizing agent synergy. Our work outlines a forward-looking vision for developing fully autonomous, scalable, and trustworthy LMA systems, laying the foundation for the evolution of Software Engineering 2.0.;2024;30.04.2025 13:51;05.05.2025 12:12;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\L7FBYD83\He et al. - 2024 - LLM-based multi-agent systems for software engineering Literature review, vision and the road ahead.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 4.1.5. Requirement Classification; 4.1.6. Requirement Generation; 4.2. Development Process Automation; 4.2.2. Debugging Support; 4.2.4. Low-code / No-code Development; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
MTQNP6W6;conferencePaper;2024;"Ouedraogo, Wendkuuni C.; Kabore, Kader; Tian, Haoye; Song, Yewei; Koyuncu, Anil; Klein, Jacques; Lo, David; Bissyande, Tegawende F.";Llms and prompting for unit test generation: A large-scale evaluation;4;Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering;;;;https://dl.acm.org/doi/abs/10.1145/3691620.3695330;;2024;30.04.2025 11:53;05.05.2025 12:12;30.04.2025 11:53;2464–2465;;;;;;Llms and prompting for unit test generation;;;;;;;;;;;;Google Scholar;;;;C:\Users\DK00747\Zotero\storage\K93SCEG2\Ouedraogo et al. - 2024 - Llms and prompting for unit test generation A lar.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
R6KYK9PQ;journalArticle;2024;"Boukhlif, Mohamed; Kharmoum, Nassim; Kharmoum, N.; Hanine, Mohamed";LLMs for intelligent software testing: a comparative study;;Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security;;;10.1145/3659677.3659749;;;2024;30.04.2025 13:51;05.05.2025 12:12;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\UQWFXUR2\Boukhlif et al. - 2024 - LLMs for intelligent software testing a comparative study.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 1.3. Fine-tuned / Instruction-tuned; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.3. Functional Testing; 3.5. Robustness / Non-functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
KVCS2S6A;journalArticle;2024;"Yang, Tinghui; Jiang, Zhiyuan; Wang, Yongjun";LLMSQLi: a black-box web sqli detection tool based on large language model;;2024 5th International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE);;;10.1109/icbase63199.2024.10762654;;Black box detection tools of SQL injection vulnerabilities simulate real-world web attack scenarios, making it essential for evaluating SQLi risks in actual web applications. However, current black-box approaches depend on predefined rules for SQLi vulnerability detection, which limits both their efficiency and accuracy. In this paper, we propose a black-box SQLi detection tool based on large language multi-agent, LLMSQLi, which uses the context understanding and reasoning capabilities of large language models and the cooperative division of labor mode of multi-agent to generate payloads customized for test targets and efficiently detect SQL injection vulnerabilities in Web programs. Drawing inspiration from real-world teams of security experts, LLMSQLi simulates the step-by-step process of human experts in testing tasks through the LLM Muti-Agent collaboration model. We ran experiments on SQLiMicroBenchmark to compare the performance of LLMSQLi and two of the most advanced black-box SQLi testing tools. Experiments show that LLMSQLi successfully detected all 15 targets and outperformed other tools.;2024;30.04.2025 13:51;05.05.2025 12:12;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.5. Robustness / Non-functional Testing; 3.1.9. Automated testing tools";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
US2MZV95;journalArticle;2023;"Liu, Zhe; Chen, Chunyang; Wang, Junjie; Chen, Mengzhuo; Wu, Boyu; Che, Xing; Wang, Dandan; Wang, Qing";Make LLM a testing expert: Bringing human-like interaction to mobile GUI testing via functionality-aware decisions;;International Conference on Software Engineering;;;10.1145/3597503.3639180;;Automated Graphical User Interface (GUI) testing plays a crucial role in ensuring app quality, especially as mobile applications have become an integral part of our daily lives. Despite the growing popularity of learning-based techniques in automated GUI testing due to their ability to generate human-like interactions, they still suffer from several limitations, such as low testing coverage, inadequate generalization capabilities, and heavy reliance on training data. Inspired by the success of Large Language Models (LLMs) like ChatGPT in natural language understanding and question answering, we formulate the mobile GUI testing problem as a Q&A task. We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process. Within this framework, we have also introduced a functionality-aware memory prompting mechanism that equips the LLM with the ability to retain testing knowledge of the whole process and conduct long-term, functionality-based reasoning to guide exploration. We evaluate it on 93 apps from Google Play and demonstrate that it outperforms the best baseline by 32;2023;30.04.2025 13:51;05.05.2025 12:12;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\GRDUAGIU\Liu et al. - 2023 - Make LLM a testing expert Bringing human-like interaction to mobile GUI testing via functionality-a.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.6. Failure Detection; 3.1.8. Test Quality Analysis; 3.3. Functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.4. Visual / GUI-based Prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
2KBECXF5;journalArticle;2025;"Azimi, Mohammad Yusaf; Yilmaz, Cemal";Model-based test execution from high-level natural language instructions using GPT-4;;Software quality journal;;;10.1007/s11219-025-09712-9;;;2025;30.04.2025 13:51;05.05.2025 12:12;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.4. Visual / GUI-based Prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
899FRPF3;journalArticle;2025;"Straubinger, Philipp; Kreis, M.; Lukasczyk, Stephan; Fraser, Gordon";Mutation testing via iterative large language model-driven scientific debugging;;International Conference on Software Testing, Verification and Validation Workshops;;;10.1109/icstw64639.2025.10962485;;Large Language Models (LLMs) can generate plausible test code. Intuitively they generate this by imitating tests seen in their training data, rather than reasoning about execution semantics. However, such reasoning is important when applying mutation testing, where individual tests need to demonstrate differences in program behavior between a program and specific artificial defects (mutants). In this paper, we evaluate whether Scientific Debugging, which has been shown to help LLMs when debugging, can also help them to generate tests for mutants. In the resulting approach, LLMs form hypotheses about how to kill specific mutants, and then iteratively generate and refine tests until they succeed, all with detailed explanations for each step. We compare this method to three baselines: (1) directly asking the LLM to generate tests, (2) repeatedly querying the LLM when tests fail, and (3) search-based test generation with Pynguin. Our experiments evaluate these methods based on several factors, including mutation score, code coverage, success rate, and the ability to identify equivalent mutants. The results demonstrate that LLMs, although requiring higher computational cost, consistently outperform Pynguin in generating tests with better fault detection and coverage. Importantly, we observe that the iterative refinement of test cases is important for achieving high-quality test suites.;2025;30.04.2025 13:51;05.05.2025 12:13;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\GBZ3B48H\Straubinger et al. - 2025 - Mutation testing via iterative large language model-driven scientific debugging.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.6. Failure Detection; 3.5. Robustness / Non-functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.5. Self-improving Prompts";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
XIWVQM6F;journalArticle;2024;"Karmarkar, Hrishikesh; Agrawal, Supriya; Chauhan, Avriti; Shete, Pranav";Navigating confidentiality in test automation: a case study in LLM driven test data generation;;IEEE International Conference on Software Analysis, Evolution, and Reengineering;;;10.1109/saner60148.2024.00041;;In out sourced industrial projects for testing of web applications, often neither the application to be tested, nor its source code are provided to the testing team, due to confidentiality reasons, making systematic testing of these applications very challenging. However, textual descriptions of such systems are often available. So, one can consider leveraging a Large Language Model (LLM) to parse these descriptions and synthesize test generators (programs that produce test data). In our experience, LLM synthesized test generators suffer from two problems:- (1) unsound: the generators might produce invalid data and (2) incomplete: the generators typically fail to generate all expected valid inputs. To mitigate these problems, we introduce TestRefineGen a method for autonomously generating test data from textual descriptions. TestRe-fineGen begins by invoking an LLM to parse a given corpus of documents and produce multiple test gener-ators. It then uses a novel ranking approach to identify generators that can produce invalid test data, and then automatically repairs them using a counterexample-guided refinement process. Lastly, TestRefineGen per-forms a generalization procedure that offsets synthesis or refinements that leads to incompleteness, to obtain generators that produce more comprehensive valid in-puts. We evaluated the effectiveness of TestRefineGen on a manually curated set of 256 textual descriptions of test data. TestRefineGen synthesized generators that produce valid test data for 66.01;2024;30.04.2025 13:51;05.05.2025 12:13;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.4. Test Repair; 3.3. Functional Testing; 3.5. Robustness / Non-functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
IWBAR8S7;journalArticle;2024;"Alshahwan, N.; Harman, Mark; Marginean, Alexandru; Tal, Rotem; Wang, Eddy";Observation-based unit test generation at Meta;;SIGSOFT FSE Companion;;;10.48550/arxiv.2402.06111;;TestGen automatically generates unit tests, carved from serialized observations of complex objects, observed during app execution. We describe the development and deployment of TestGen at Meta. In particular, we focus on the scalability challenges overcome during development in order to deploy observation-based test carving at scale in industry. So far, TestGen has landed 518 tests into production, which have been executed 9,617,349 times in continuous integration, finding 5,702 faults. Meta is currently in the process of more widespread deployment. Our evaluation reveals that, when carving its observations from 4,361 reliable end-to-end tests, TestGen was able to generate tests for at least 86% of the classes covered by end-to-end tests. Testing on 16 Kotlin Instagram app-launch-blocking tasks demonstrated that the TestGen tests would have trapped 13 of these before they became launch blocking.;2024;30.04.2025 13:51;05.05.2025 12:13;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\A5SANI9S\Alshahwan et al. - 2024 - Observation-based unit test generation at Meta.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.6. Failure Detection; 3.1.8. Test Quality Analysis; 3.3. Functional Testing; 3.5. Robustness / Non-functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
XJQGHFKC;conferencePaper;2024;"Yang, Lin; Yang, Chen; Gao, Shutao; Wang, Weijing; Wang, Bo; Zhu, Qihao; Chu, Xiao; Zhou, Jianyi; Liang, Guangtai; Wang, Qianxiang";On the evaluation of large language models in unit test generation;23;Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering;;;;https://dl.acm.org/doi/abs/10.1145/3691620.3695529?casa_token=RN6C5CwP27kAAAAA:ttVBtvNRdcXcbiHfVfv1HZ5cnIK10tDw3Lc6MZGqc00O7X7djuTpdEsf0PWo_5XQcNQXR8F8EkQsFg;;2024;30.04.2025 11:56;05.05.2025 12:13;30.04.2025 11:56;1607–1619;;;;;;;;;;;;;;;;;;Google Scholar;;;;C:\Users\DK00747\Zotero\storage\XPFENDY3\Yang et al. - 2024 - On the evaluation of large language models in unit.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 1.3. Fine-tuned / Instruction-tuned; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.4. Integration Testing; 3.5. Robustness / Non-functional Testing; 3.3. Functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.2. Few-shot / Zero-shot";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
SW99SN4M;journalArticle;2024;"Xiao, Danni; Guo, Yimeng; Li, Yanhui; Chen, Lin";Optimizing search-based unit test generation with large language models: An empirical study;;Asia-Pacific Symposium on Internetware;;;10.1145/3671016.3674813;;;2024;30.04.2025 13:51;05.05.2025 12:13;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\D8AGVLFX\Xiao et al. - 2024 - Optimizing search-based unit test generation with large language models An empirical study.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.5. Robustness / Non-functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
8HZE3RFY;journalArticle;2024;"Yu, Xiao; Liu, Lei; Hu, Xing; Keung, J.; Xia, Xin; Lo, David";Practitioners’ expectations on automated test generation;;Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis;;;10.1145/3650212.3680386;;;2024;30.04.2025 13:51;05.05.2025 12:13;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\EIFCYPPY\Yu et al. - 2024 - Practitioners’ expectations on automated test generation.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.6. Failure Detection";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
GFJQEGZV;conferencePaper;2021;"Hershkovich, Eran; Stern, Roni; Abreu, Rui; Elmishali, Amir";Prioritized Test Generation Guided by Software Fault Prediction;;2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW);978-1-6654-4456-9;;10.1109/ICSTW52544.2021.00045;https://ieeexplore.ieee.org/document/9440144/;;2021-04;02.05.2025 11:53;05.05.2025 12:13;02.05.2025 11:53;218-225;;;;;;;;;;;IEEE;Porto de Galinhas, Brazil;;https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html;;;;DOI.org (Crossref);;;;;;"2. ML; 2.3. Supervised / Unsupervised Learning Strategies; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.5. Test Prioritization; 3.1.8. Test Quality Analysis";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW);;;;;;;;;;;;;;;
4P6U5CWV;conferencePaper;2024;"Edirisinghe, Hasali; Wickramaarachchi, Dilani";Quality Assurance for LLM-Generated Test Cases: A Systematic Literature Review;1;2024 8th SLAAI International Conference on Artificial Intelligence (SLAAI-ICAI);;;;https://ieeexplore.ieee.org/abstract/document/10844968/;;2024;30.04.2025 11:58;05.05.2025 12:13;30.04.2025 11:58;1–6;;;;;;Quality Assurance for LLM-Generated Test Cases;;;;;IEEE;;;;;;;Google Scholar;;;;;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
VRIE4AYY;journalArticle;2024;"Nashaat, Mona; Miller, James";Refining software defect prediction through attentive neural models for code understanding;;Journal of Systems and Software;;;10.1016/j.jss.2024.112266;;;2024;30.04.2025 13:51;05.05.2025 12:13;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\JX5MF66T\Nashaat and Miller - 2024 - Refining software defect prediction through attentive neural models for code understanding.pdf;;"2. ML; 2.3. Supervised / Unsupervised Learning Strategies; 4. Software Engineering; 4.1. Development Task Automation; 4.1.4. Source Code Analysis";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
REL28XLJ;bookSection;2022;"Cheddi, Fatima; Tahiri, Ahmed; Serrhini, Mohammed";Regenerating a Graphical User Interface Using Deep Learning;2;Advances on Smart and Soft Computing;978-981-16-5558-6 978-981-16-5559-3;;;https://link.springer.com/10.1007/978-981-16-5559-3_28;;2022;04.05.2025 10:31;05.05.2025 12:13;04.05.2025 10:31;341-352;;;1399.0;;;;;;;;Springer Singapore;Singapore;en;;;;;DOI.org (Crossref);;Series Title: Advances in Intelligent Systems and Computing DOI: 10.1007/978-981-16-5559-3_28;;;;"2. ML; 2.2. Deep Learning; 2.2.1. CNN / RNN / LSTM; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 5. Prompt Engineering; 5.4. Visual / GUI-based Prompting";;"Saeed, Faisal; Al-Hadhrami, Tawfik; Mohammed, Errais; Al-Sarem, Mohammed";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
94C7ZWJS;journalArticle;2025;"Wang, Fanyu; Arora, Chetan; Tantithamthavorn, Chakkrit; Huang, Kaicheng; Aleti, Aldeida";Requirements-Driven Automated Software Testing: A Systematic Review;0;;;;;https://www.preprints.org/frontend/manuscript/3ad71ccffb8f4bc5ef999185fdf2db9c/download_pub;;2025;30.04.2025 12:04;05.05.2025 12:13;30.04.2025 12:04;;;;;;;Requirements-Driven Automated Software Testing;;;;;;;;;;;;Google Scholar;;;;C:\Users\DK00747\Zotero\storage\U9YX8REM\Wang et al. - 2025 - Requirements-Driven Automated Software Testing A .pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 4. Software Engineering; 4.1. Development Task Automation; 4.1.5. Requirement Classification; 4.1.6. Requirement Generation";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
INWL4XU7;journalArticle;2023;"Dakhama, Aidan; Even-Mendoza, Karine; Langdon, W. B.; Menéndez, Héctor D.; Petke, Justyna";SearchGEM5: Towards reliable gem5 with search based software testing and large language models;;International Symposium on Search Based Software Engineering;;;10.1007/978-3-031-48796-5_14;;;2023;30.04.2025 13:51;05.05.2025 12:13;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.5. Robustness / Non-functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
FCC3A67T;journalArticle;2023;"Dong, Yihong; Jiang, Xue; Jin, Zhi; Li, Ge";Self-collaboration code generation via ChatGPT;;ACM Transactions on Software Engineering and Methodology;;;10.1145/3672459;;"Although Large Language Models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, 1) Multiple LLM agents act as distinct ‘experts’, each responsible for a specific subtask within a complex task; 2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other’s work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development’s analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9";2023;30.04.2025 13:51;05.05.2025 12:13;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\P6U6MPIB\Dong et al. - 2023 - Self-collaboration code generation via ChatGPT.pdf;;"1. LLM; 1.1. Transformer-based Models; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.3. Instructional / Role prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
KAE53AVZ;journalArticle;2024;"Kim, Yunjin; Kouatly, Rand";Self-healing automation testing with Selenium and ChatGPT API integration to improve the efficiency of the maintenance;;2024 2nd International Conference on Foundation and Large Language Models (FLLM);;;10.1109/fllm63129.2024.10852490;;The rapid delivery in software development life cycle demands more adaptable automation testing frameworks. The current automation test frameworks struggle with maintaining the scripts due to frequent changes in web elements. To resolve this issue, this paper introduces a self-healing automation testing framework that updates broken locators without human intervention. ChatGPT API is integrated within the newly developed automation framework to reduce time and effort for maintenance. The results show the potential of the self-healing automation testing framework with ChatGPT API integration compared to traditional automation tests.;2024;30.04.2025 13:51;05.05.2025 12:13;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 2. ML; 2.2. Deep Learning; 2.2.2. Autoencoders / GANs; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.4. Test Repair; 3.1.7. Self-healing Testing; 3.3. Functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
JQYB26WQ;journalArticle;2024;"Alian, Parsa; Nashid, Noor; Shahbandeh, M.; Mesbah, Ali";Semantic constraint inference for web form test generation;;International Symposium on Software Testing and Analysis;;;10.1145/3650212.3680332;;Automated test generation for web forms has been a longstanding challenge, exacerbated by the intrinsic human-centric design of forms and their complex, device-agnostic structures. We introduce an innovative approach, called FormNexus, for automated web form test generation, which emphasizes deriving semantic insights from individual form elements and relations among them, utilizing textual content, DOM tree structures, and visual proximity. The insights gathered are transformed into a new conceptual graph, the Form Entity Relation Graph (FERG), which offers machine-friendly semantic information extraction. Leveraging LLMs, FormNexus adopts a feedback-driven mechanism for generating and refining input constraints based on real-time form submission responses. The culmination of this approach is a robust set of test cases, each produced by methodically invalidating constraints, ensuring comprehensive testing scenarios for web forms. This work bridges the existing gap in automated web form testing by intertwining the capabilities of LLMs with advanced semantic inference methods. Our evaluation demonstrates that FormNexus combined with GPT-4 achieves 89;2024;30.04.2025 13:51;05.05.2025 12:14;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\5PVY6V8Z\Alian et al. - 2024 - Semantic constraint inference for web form test generation.pdf;;"2. ML; 2.1. Traditional Machine Learning; 2.1.2. Feature-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.3. Functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.5. Self-improving Prompts";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
QG6VW93C;journalArticle;2023;"Qi, Xiaofang; Qian, Xiang; Li, Yanhui";Semantic test repair for web applications;;ESEC/SIGSOFT FSE;;;10.1145/3611643.3616324;;Automation testing is widely used in the functional testing of web applications. However, during the evolution of web applications, such web test scripts tend to break. It is essential to repair such broken test scripts to make regression testing run successfully. As manual repairing is time-consuming and expensive, researchers focus on automatic repairing techniques. Empirical study shows that the web element locator is the leading cause of web test breakages. Most existing repair techniques utilize Document Object Model attributes or visual appearances of elements to find their location but neglect their semantic information. This paper proposes a novel semantic repair technique called Semantic Test Repair (Semter) for web test repair. Our approach captures relevant semantic information from test executions on the application’s basic version and locates target elements by calculating semantic similarity between elements to repair tests. Our approach can also repair test workflow due to web page additions or deletions by a local exploration in the updated version. We evaluated the efficacy of our technique on six real-world web applications compared with three baselines. Experimental results show that Semter achieves an 84;2023;30.04.2025 13:51;05.05.2025 12:14;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\XP8T92QI\Qi et al. - 2023 - Semantic test repair for web applications.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.4. Test Repair; 3.1.8. Test Quality Analysis; 5. Prompt Engineering; 5.4. Visual / GUI-based Prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
2N3C4WSD;journalArticle;2024;"Wang, Junjie; Huang, Yuchao; Chen, Chunyang; Liu, Zhe; Wang, Song; Wang, Qing";Software testing with large language models: Survey, landscape, and vision;;IEEE Transactions on Software Engineering;;;10.1109/tse.2024.3368208;;Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.;2024;30.04.2025 13:51;05.05.2025 12:14;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\6M3YR9P5\Wang et al. - 2024 - Software testing with large language models Survey, landscape, and vision.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 1.3. Fine-tuned / Instruction-tuned; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.3. Functional Testing; 3.4. Integration Testing; 3.5. Robustness / Non-functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.2. Few-shot / Zero-shot";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
ZZ6VWVYV;conferencePaper;2021;"Mastropaolo, Antonio; Scalabrino, Simone; Cooper, Nathan; Nader Palacio, David; Poshyvanyk, Denys; Oliveto, Rocco; Bavota, Gabriele";Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks;;2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE);978-1-6654-0296-5;;10.1109/ICSE43902.2021.00041;https://ieeexplore.ieee.org/document/9401982/;;2021-05;02.05.2025 11:54;05.05.2025 12:14;02.05.2025 11:54;336-347;;;;;;;;;;;IEEE;Madrid, ES;;https://doi.org/10.15223/policy-029;;;;DOI.org (Crossref);;;;C:\Users\DK00747\Zotero\storage\2628A9F4\Mastropaolo et al. - 2021 - Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.3. Fine-tuned / Instruction-tuned; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 4.1.2. Code Repair / Refactoring; 4.1.4. Source Code Analysis";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE);;;;;;;;;;;;;;;
24G9EN5Y;conferencePaper;2021;"Tsimpourlas, Foivos; Rajan, Ajitha; Allamanis, Miltiadis";Supervised learning over test executions as a test oracle;;Proceedings of the 36th Annual ACM Symposium on Applied Computing;978-1-4503-8104-8;;10.1145/3412841.3442027;https://dl.acm.org/doi/10.1145/3412841.3442027;;2021-03-22;02.05.2025 11:53;05.05.2025 12:14;02.05.2025 11:53;1521-1531;;;;;;;;;;;ACM;Virtual Event Republic of Korea;en;;;;;DOI.org (Crossref);;;;C:\Users\DK00747\Zotero\storage\ADFY6TE5\Tsimpourlas et al. - 2021 - Supervised learning over test executions as a test oracle.pdf;;"1. LLM; 1.3. Fine-tuned / Instruction-tuned; 2. ML; 2.1. Traditional Machine Learning; 2.1.1. SVM / Decision Trees / Random Forest; 2.3. Supervised / Unsupervised Learning Strategies; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.3. Test Classification; 3.1.8. Test Quality Analysis; 3.5. Robustness / Non-functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;SAC '21: The 36th ACM/SIGAPP Symposium on Applied Computing;;;;;;;;;;;;;;;
67HCNRB4;journalArticle;2024;"Xu, Jiahe; Xu, Jingwei; Chen, Taolue; Ma, Xiaoxing";Symbolic execution with test cases generated by large language models;;International Conference on Software Quality, Reliability and Security;;;10.1109/qrs62785.2024.00031;;Symbolic execution is a powerful program analysis technique. External environment construction and internal path explosion are two long-standing problems which may affect the effectiveness and performance of symbolic execution on complex programs. The intrinsic challenge is to achieve a sufficient understanding of the program context to construct a set of execution environments which can guide the selection of symbolic states. In this paper, we propose a novel program-context-guided symbolic execution framework LangSym based on program’s instruction/user manual. Leveraging the capabilities of natural language understanding and code generation in large language models (LLMs), LangSym can automatically extract the knowledge related to the functionality of the program, and generate adequate test cases and the corresponding environments as the prior knowledge for symbolic execution. We instantiate LangSym in KLEE, a widely adopted symbolic execution engine, to build a pipeline that could automatically leverage LLMs to boost the symbolic execution. We evaluate LangSym on almost all GNU Coreutils programs and considerable large-scale programs, showing that LangSym outperforms the existing strategies in KLEE with at least a 10;2024;30.04.2025 13:51;05.05.2025 12:14;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 1.1. Transformer-based Models; 2. ML; 2.1. Traditional Machine Learning; 2.1.2. Feature-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.2. Test Data Generation; 3.2.2. Test Documentation; 3.2.3. Test Datasets (HTML, Screenshots, UML)";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
QIWRPZMJ;journalArticle;2025;"Bardakci, Tolgahan; Demeyer, Serge; Beyazit, Mutlu";"Test amplification for REST apis using ""out-of-the-box"" large language models";0;IEEE Software;;;10.48550/arxiv.2503.10306;;"REST APIs (Representational State Transfer Application Programming Interfaces) are an indispensable building block in today's cloud-native applications, so testing them is critically important. However, writing automated tests for such REST APIs is challenging because one needs strong and readable tests that exercise the boundary values of the protocol embedded in the REST API. In this paper, we report our experience with using""out of the box""large language models (ChatGPT and GitHub's Copilot) to amplify REST API test suites. We compare the resulting tests based on coverage and understandability, and we derive a series of guidelines and lessons learned concerning the prompts that result in the strongest test suite.";2025;30.04.2025 13:51;05.05.2025 12:14;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\9ID5JPGI\Bardakci et al. - 2025 - Test amplification for REST apis using out-of-the-box large language models.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.4. Visual / GUI-based Prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
25BTEYRN;journalArticle;2024;"Nabeel, Mohamad; Nimara, Doumitrou Daniil; Zanouda, Tahar";Test code generation for telecom software systems using two-stage generative model;;2024 IEEE International Conference on Communications Workshops (ICC Workshops);;;10.1109/iccworkshops59551.2024.10615269;;In recent years, the evolution of Telecom towards achieving intelligent, autonomous, and open networks has led to an increasingly complex Telecom Software system, supporting various heterogeneous deployment scenarios, with multi-standard and multi-vendor support. As a result, it becomes a challenge for large-scale Telecom software companies to develop and test software for all deployment scenarios. To address these challenges, we propose a framework for Automated Test Generation for large-scale Telecom Software systems. We begin by generating Test Case Input data for test scenarios observed using a time-series Generative model trained on historical Telecom Network data during field trials. Additionally, the time-series Generative model helps in preserving the privacy of Telecom data. The generated time-series software performance data are then utilized with test descriptions written in natural language to generate Test Script using the Generative Large Language Model. Our comprehensive experiments on public datasets and Telecom datasets obtained from operational Telecom Networks demonstrate that the framework can effectively generate comprehensive test case data input and useful test code.;2024;30.04.2025 13:51;05.05.2025 12:14;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\A7IJ28DX\Nabeel et al. - 2024 - Test code generation for telecom software systems using two-stage generative model.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.5. Test Prioritization; 3.1.8. Test Quality Analysis; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
GG9YA9WT;journalArticle;2024;"Mehmood, Abid; Ilyas, Qazi Mudassir; Ahmad, Muneer; Shi, Zhongliang";Test Suite Optimization Using Machine Learning Techniques: A Comprehensive Study;3;IEEE Access;;;;https://ieeexplore.ieee.org/abstract/document/10741285/;;2024;30.04.2025 12:00;05.05.2025 12:14;30.04.2025 12:00;;;;;;;Test Suite Optimization Using Machine Learning Techniques;;;;;;;;;;;;Google Scholar;;Publisher: IEEE;;C:\Users\DK00747\Zotero\storage\GAIS5EHF\Mehmood et al. - 2024 - Test Suite Optimization Using Machine Learning Techniques A Comprehensive Study.pdf;;"2. ML; 2.2. Deep Learning; 2.2.1. CNN / RNN / LSTM; 2.2.2. Autoencoders / GANs; 2.5. Reinforcement Learning Strategies; 3. Software Testing; 3.1. Test Automation; 3.1.2. Test Optimization; 3.1.5. Test Prioritization;";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
CZTDNV2F;journalArticle;2024;"Li, Youwei; Li, Yangyang; Yang, Yangzhao";Test-agent: a multimodal app automation testing framework based on the large language model;;2024 IEEE 4th International Conference on Digital Twins and Parallel Intelligence (DTPI);;;10.1109/dtpi61353.2024.10778901;;This paper introduces a multimodal agent-based app automation testing framework, named Test-Agent, built on the Large Language Model (LLM), designed to address the growing challenges in mobile application automation testing. As mobile applications become more prevalent and emerging systems like Harmony OS Next and mini-programs emerge, traditional automated testing methods, which depend on manually crafting test cases and scripts, are no longer sufficient for cross-platform compatibility and complex interaction logic. The Test-Agent framework employs artificial intelligence technologies to analyze application interface screenshots and user natural language instructions. Combined with deep learning models, it automatically generates and executes test actions on mobile devices. This innovative approach eliminates the need for pre-written test scripts or backend system access, relying solely on screenshots and UI structure information. It achieves cross-platform and cross-application universality, significantly reducing the workload of test case writing, enhancing test execution efficiency, and strengthening cross-platform adaptability. Test-Agent offers an innovative and efficient solution for automated testing of mobile applications.;2024;30.04.2025 13:51;05.05.2025 12:14;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;;;"1. LLM; 1.1. Transformer-based Models; 1.3. Fine-tuned / Instruction-tuned; 2. ML; 2.2. Deep Learning; 2.2.1. CNN / RNN / LSTM; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.2. Test Data Generation; 3.2.3. Test Datasets (HTML, Screenshots, UML); 3.3. Functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.3. Instructional / Role prompting; 5.4. Visual / GUI-based Prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
KMTSZ5YA;journalArticle;2024;"Mathews, N.; Nagappan, Mei";Test-driven development and LLM-based code generation;;International Conference on Automated Software Engineering;;;10.1145/3691620.3695527;;"Recent Large Language Models (LLMs) have demonstrated significant capabilities in generating code snippets directly from problem statements. This increasingly automated process mirrors traditional human-led software development, where code is often written in response to a requirement. Historically, Test-Driven Development (TDD) has proven its merit, requiring developers to write tests before the functional code, ensuring alignment with the initial problem statements. Applying TDD principles to LLM-based code generation offers one distinct benefit: it enables developers to verify the correctness of generated code against predefined tests. This paper investigates if and how TDD can be incorporated into AI-assisted code-generation processes. We experimentally evaluate our hypothesis that providing LLMs like GPT-4 and Llama 3 with tests in addition to the problem statements enhances code generation outcomes. We experimented with established function-level code generation benchmarks such as MBPP and HumanEval. Our results consistently demonstrate that including test cases leads to higher success in solving programming challenges. We assert that TDD is a promising paradigm for helping ensure that the code generated by LLMs effectively captures the requirements.CCS CONCEPTS• Software and its engineering ? Software development techniques; • Computing methodologies ? Artificial intelligence.";2024;30.04.2025 13:51;05.05.2025 12:14;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\ULHTETM6\Mathews and Nagappan - 2024 - Test-driven development and LLM-based code generation.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 4.1. Development Task Automation; 4.1.1. Code Generation; 4.1.2. Code Repair / Refactoring; 4.2. Development Process Automation; 4.2.3. Test-Driven Development; 4.2.4. Low-code / No-code Development; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
JLBDIRZB;journalArticle;2024;"Sapozhnikov, Arkadii; Olsthoorn, Mitchell; Panichella, Annibale; Kovalenko, Vladimir; Derakhshanfar, P.";TestSpark: IntelliJ idea's ultimate test generation companion;;2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion);;;10.1145/3639478.3640024;;Writing software tests is laborious and time-consuming. To address this, prior studies introduced various automated test-generation techniques. A well-explored research direction in this field is unit test generation, wherein artificial intelligence (AI) techniques create tests for a method/class under test. While many of these techniques have primarily found applications in a research context, existing tools (e.g., EvoSuite, Randoop, and AthenaTest) are not user-friendly and are tailored to a single technique. This paper introduces Test-Spark, a plugin for IntelliJ IDEA that enables users to generate unit tests with only a few clicks directly within their Integrated Development Environment (IDE). Furthermore, TestSpark also allows users to easily modify and run each generated test and integrate them into the project workflow. TestSpark leverages the advances of search-based test generation tools, and it introduces a technique to generate unit tests using Large Language Models (LLMs) by creating a feedback cycle between the IDE and the LLM. Since TestSpark is an open-source (https://github.com/JetBrains-Research/TestSpark), extendable, and well-documented tool, it is possible to add new test generation methods into the plugin with the minimum effort. This paper also explains our future studies related to TestSpark and our preliminary results. Demo video: https://youtu.be/0F4PrxWfiXo;2024;30.04.2025 13:51;05.05.2025 12:14;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\Y3THIT2N\Sapozhnikov et al. - 2024 - TestSpark IntelliJ idea's ultimate test generation companion.pdf;;"3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.4. Test Repair; 3.1.5. Test Prioritization; 3.4. Integration Testing; 3.5. Robustness / Non-functional Testing; 3.1.9. Automated testing tools";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
6458TZGY;journalArticle;2025;"Gao, Cuiyun; Hu, Xing; Gao, Shan; Xia, Xin; Jin, Zhi";The current challenges of software engineering in the era of large language models;;ACM Transactions on Software Engineering and Methodology;;;10.1145/3712005;;With the advent of large language models (LLMs) in the artificial intelligence (AI) area, the field of software engineering (SE) has also witnessed a paradigm shift. These models, by leveraging the power of deep learning and massive amounts of data, have demonstrated an unprecedented capacity to understand, generate, and operate programming languages. They can assist developers in completing a broad spectrum of software development activities, encompassing software design, automated programming, and maintenance, which potentially reduces huge human efforts. Integrating LLMs within the SE landscape (LLM4SE) has become a burgeoning trend, necessitating exploring this emergent landscape’s challenges and opportunities. The paper aims at revisiting the software development life cycle (SDLC) under LLMs, and highlighting challenges and opportunities of the new paradigm. The paper first summarizes the overall process of LLM4SE, and then elaborates on the current challenges based on a through discussion. The discussion was held among more than 20 participants from academia and industry, specializing in fields such as software engineering and artificial intelligence. Specifically, we achieve 26 key challenges from seven aspects, including software requirement & design, coding assistance, testing code generation, code review, code maintenance, software vulnerability management, and data, training, and evaluation. We hope the achieved challenges would benefit future research in the LLM4SE field.;2025;30.04.2025 13:51;05.05.2025 12:15;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\IHBMJHNA\Gao et al. - 2025 - The current challenges of software engineering in the era of large language models.pdf;;"1. LLM; 1.3. Fine-tuned / Instruction-tuned; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 4.1.5. Requirement Classification; 4.1.6. Requirement Generation; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
TLXNSWC9;journalArticle;2023;"Fontes, Afonso; Gay, Gregory";The integration of machine learning into automated test generation: A systematic mapping study;;Software testing, verification & reliability;;;10.1002/stvr.1845;;Abstract Machine learning (ML) may enable effective automated test generation. We characterize emerging research, examining testing practices, researcher goals, ML techniques applied, evaluation, and challenges in this intersection by performing. We perform a systematic mapping study on a sample of 124 publications. ML generates input for system, GUI, unit, performance, and combinatorial testing or improves the performance of existing generation methods. ML is also used to generate test verdicts, property?based, and expected output oracles. Supervised learning—often based on neural networks—and reinforcement learning—often based on Q?learning—are common, and some publications also employ unsupervised or semi?supervised learning. (Semi?/Un?)Supervised approaches are evaluated using both traditional testing metrics and ML?related metrics (e.g., accuracy), while reinforcement learning is often evaluated using testing metrics tied to the reward function. The work?to?date shows great promise, but there are open challenges regarding training data, retraining, scalability, evaluation complexity, ML algorithms employed—and how they are applied—benchmarks, and replicability. Our findings can serve as a roadmap and inspiration for researchers in this field.;2023;30.04.2025 13:51;05.05.2025 12:15;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: 4367669694 tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\4WUL6U6U\Fontes and Gay - 2023 - The integration of machine learning into automated test generation A systematic mapping study.pdf;;"2. ML; 2.3. Supervised / Unsupervised Learning Strategies; 2.5. Reinforcement Learning Strategies; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.2. Test Data Generation; 3.2.3. Test Datasets (HTML, Screenshots, UML); 3.3. Functional Testing; 3.5. Robustness / Non-functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
XWNUDS99;conferencePaper;2025;"Ranapana, R. M. S.; Wijayanayake, WMJI";The Role of AI in Software Test Automation;0;2025 5th International Conference on Advanced Research in Computing (ICARC);;;;https://ieeexplore.ieee.org/abstract/document/10962814/;;2025;30.04.2025 12:00;05.05.2025 12:15;30.04.2025 12:00;1–6;;;;;;;;;;;IEEE;;;;;;;Google Scholar;;;;;;"2. ML; 2.2. Deep Learning; 2.2.3. Transfer / Ensemble Learning; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
BJAPSIQV;journalArticle;2022;"Dinella, Elizabeth; Dinella, Elizabeth; Ryan, Gabriel; Ryan, Gabriel; Mytkowicz, Todd; Mytkowicz, Todd; Lahiri, Shuvendu K.; Lahiri, Shuvendu K.";TOGA: A Neural Method for Test Oracle Generation;;International Conference on Software Engineering;;;10.1145/3510003.3510141;;Testing is widely recognized as an important stage of the software development lifecycle. Effective software testing can provide benefits such as bug finding, preventing regressions, and documentation. In terms of documentation, unit tests express a unit's intended functionality, as conceived by the developer. A test oracle, typically expressed as an condition, documents the intended behavior of a unit under a given test prefix. Synthesizing a functional test oracle is a challenging problem, as it must capture the intended functionality rather than the implemented functionality. In this paper, we propose TOGA (a neural method for Test Oracle GenerAtion), a unified transformer-based neural approach to infer both exceptional and assertion test oracles based on the context of the focal method. Our approach can handle units with ambiguous or missing documentation, and even units with a missing implementation. We evaluate our approach on both oracle inference accuracy and functional bug-finding. Our technique improves accuracy by 33% over existing oracle inference approaches, achieving 96% overall accuracy on a held out test dataset. Furthermore, we show that when integrated with a automated test generation tool (EvoSuite), our approach finds 57 real world bugs in large-scale Java programs, including 30 bugs that are not found by any other automated testing method in our evaluation.;2022;30.04.2025 13:51;05.05.2025 12:15;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: 4284690374 tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\2KHID29A\Dinella et al. - 2022 - TOGA A Neural Method for Test Oracle Generation.pdf;;"2. ML; 2.2. Deep Learning; 3. Software Testing; 3.2. Test Data Generation; 3.2.1. Test Oracle; 3.4. Integration Testing; ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
GFVBRYSI;journalArticle;2023;"Zheng, Zibin; Ning, Kaiwen; Chen, Jiachi; Wang, Yanlin; Chen, Wenqing; Guo, Liang-Hong; Wang, Weicheng";Towards an understanding of large language models in software engineering tasks;;Empirical Software Engineering;;;10.48550/arxiv.2308.11396;;Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in tasks such as text generation and reasoning. Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus. However, there is still a lack of systematic research on the application and evaluation of LLMs in the field of software engineering. Therefore, this paper is the first to comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks? To find the answers, we have collected related literature as extensively as possible from seven mainstream databases, and selected 123 papers for analysis. We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs. Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, providing guidance for researchers and developers to optimize.;2023;30.04.2025 13:51;05.05.2025 12:15;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: 4386114236 tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\AVJTL5XI\Zheng et al. - 2023 - Towards an understanding of large language models in software engineering tasks.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.5. Robustness / Non-functional Testing; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
ZP7F969B;journalArticle;2023;"Feldt, Robert; Kang, Sungmin; Yoon, Jaeseung; Yoo, Shin";Towards autonomous testing agents via conversational large language models;;International Conference on Automated Software Engineering;;;10.1109/ase56229.2023.00148;;Software testing is an important part of the development cycle, yet it requires specialized expertise and substantial developer effort to adequately test software. Recent discoveries of the capabilities of large language models (LLMs) suggest that they can be used as automated testing assistants, and thus provide helpful information and even drive the testing process. To highlight the potential of this technology, we present a taxonomy of LLM-based testing agents based on their level of autonomy, and describe how a greater level of autonomy can benefit developers in practice. An example use of LLMs as a testing assistant is provided to demonstrate how a conversational framework for testing can help developers. This also highlights how the often criticized “hallucination” of LLMs can be beneficial for testing. We identify other tangible benefits that LLM-driven testing agents can bestow, and also discuss potential limitations.;2023;30.04.2025 13:51;05.05.2025 12:15;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: 4388483618 tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\JAM3GHUS\Feldt et al. - 2023 - Towards autonomous testing agents via conversational large language models.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.5. Test Prioritization";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
92YH3UW5;conferencePaper;2024;"Jiang, Zongze; Wen, Ming; Cao, Jialun; Shi, Xuanhua; Jin, Hai";Towards Understanding the Effectiveness of Large Language Models on Directed Test Input Generation;3;Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering;;;;https://dl.acm.org/doi/abs/10.1145/3691620.3695513?casa_token=or-MFqKxfxoAAAAA:0iMOOdhQwtTzgQR50XR75wClv7KKslqFKVBLMIZsXBsvmyv3hKYQN1GVpl4cVKG8N83JHxR9bJKHkA;;2024;30.04.2025 12:02;05.05.2025 12:15;30.04.2025 12:02;1408–1420;;;;;;;;;;;;;;;;;;Google Scholar;;;;C:\Users\DK00747\Zotero\storage\D85Y54JV\Jiang et al. - 2024 - Towards Understanding the Effectiveness of Large L.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.2. Test Data Generation; 3.2.3. Test Datasets (HTML, Screenshots, UML); 3.5. Robustness / Non-functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
235S6Y7M;journalArticle;2025;"Chen, Xiaoquan; Liu, Jian; Zhang, Yingkai; Hu, Qinsong; Han, Yupeng; Zhang, Ruqi; Ran, Jingqi; Yan, Lei; Huang, Baiqi; Ma, Shengting";TraceAwareness and dual-strategy fuzz testing: Enhancing path coverage and crash localization with stochastic science and large language models;;Computers & electrical engineering;;;10.1016/j.compeleceng.2025.110266;;;2025;30.04.2025 13:51;05.05.2025 12:15;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\X3D2DW3K\Chen et al. - 2025 - TraceAwareness and dual-strategy fuzz testing Enhancing path coverage and crash localization with s.pdf;;"1. LLM; 1.3. Fine-tuned / Instruction-tuned; 2. ML; 2.2. Deep Learning; 2.2.1. CNN / RNN / LSTM; 3. Software Testing; 3.1. Test Automation; 3.1.2. Test Optimization; 3.1.8. Test Quality Analysis; 3.5. Robustness / Non-functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
PRD9ZIY9;journalArticle;2024;Deljouyi, Amirhossein;Understandable test generation through capture/replay and llms;;2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion);;;10.1145/3639478.3639789;;Automatic unit test generators, particularly search-based software testing (SBST) tools such as EvoSuite, efficiently generate unit test suites with acceptable coverage. Although this removes the burden of writing unit tests from developers, these generated tests often pose challenges in terms of comprehension for developers. In my doctoral research, I aim to investigate strategies to address the issue of comprehensibility in generated test cases and improve the test suite in terms of effectiveness. To achieve this, I introduce four projects leveraging Capture/Replay and Large Language Model (LLM) techniques. Capture/Replay carves information from End-to-End (E2E) tests, enabling the generation of unit tests containing meaningful test scenarios and actual test data. Moreover, the growing capabilities of large language models (LLMs) in language analysis and transformation play a significant role in improving readability in general. Our proposed approach involves leveraging E2E test scenario extraction alongside an LLM-guided approach to enhance test case understandability, augment coverage, and establish comprehensive mock and test oracles. In this research, we endeavor to conduct both a quantitative analysis and a user evaluation of the quality of the generated tests in terms of executability, coverage, and understandability.;2024;30.04.2025 13:51;05.05.2025 12:15;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\IBYJZYXF\Deljouyi - 2024 - Understandable test generation through capturereplay and llms.pdf;;"1. LLM; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.3. Functional Testing; 4. Software Engineering; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
Y5V7W2HN;conferencePaper;2024;"Bhatia, Shreya; Gandhi, Tarushi; Kumar, Dhruv; Jalote, Pankaj";Unit test generation using generative AI: A comparative performance analysis of autogeneration tools;44;Proceedings of the 1st International Workshop on Large Language Models for Code;;;;https://dl.acm.org/doi/abs/10.1145/3643795.3648396;;2024;30.04.2025 11:54;05.05.2025 12:15;30.04.2025 11:54;54–61;;;;;;Unit test generation using generative AI;;;;;;;;;;;;Google Scholar;;;;C:\Users\DK00747\Zotero\storage\3KGH6XLU\Bhatia et al. - 2024 - Unit test generation using generative AI A compar.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.4. Integration Testing; 5.1. Prompting Strategies; 5.5. Self-improving Prompts";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
YY7B83SD;journalArticle;2024;"Paduraru, C.; Stefanescu, Alin; Jianu, Augustin";Unit test generation using large language models for unity game development;;FaSE4Games@SIGSOFT FSE;;;10.1145/3663532.3664466;;;2024;30.04.2025 13:51;05.05.2025 12:15;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\8QEF29IL\Paduraru et al. - 2024 - Unit test generation using large language models for unity game development.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.3. Fine-tuned / Instruction-tuned; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.4. Integration Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
IMSH74WG;journalArticle;2024;"Zapkus, Dovydas Marius; Slotkienë, Asta";Unit test generation using large language models: A systematic literature review;3;Lietuvos magistrantø informatikos ir IT tyrimai: konferencijos darbai, 2024 m. geguþës 10 d.;;;;https://epublications.vu.lt/object/elaba:198450771/;;2024;30.04.2025 11:57;05.05.2025 12:15;30.04.2025 11:57;136–144;;;;;;Unit test generation using large language models;;;;;;;;;;;;Google Scholar;;Publisher: Vilniaus universiteto leidykla/Vilnius University Press;;C:\Users\DK00747\Zotero\storage\UYH7TP9K\Zapkus and Slotkienë - 2024 - Unit test generation using large language models A systematic literature review.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.4. Integration Testing; 3.3. Functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
ZQJFXZL3;journalArticle;2024;Hussein, Dania;Usability of LLMs for Assisting Software Engineering: a Literature Review;0;;;;;https://elib.dlr.de/210605/;;2024;30.04.2025 12:03;05.05.2025 12:16;30.04.2025 12:03;;;;;;;Usability of LLMs for Assisting Software Engineering;;;;;;;;;;;;Google Scholar;;Publisher: Universität Bonn;;C:\Users\DK00747\Zotero\storage\5MIULPSP\Hussein - 2024 - Usability of LLMs for Assisting Software Engineeri.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 4.1. Development Task Automation; 4.1.1. Code Generation; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
L8DY23K2;bookSection;2021;"Meinke, Karl; Khosrowjerdi, Hojat";Use Case Testing: A Constrained Active Machine Learning Approach;6;Tests and Proofs;978-3-030-79378-4 978-3-030-79379-1;;;https://link.springer.com/10.1007/978-3-030-79379-1_1;;2021;02.05.2025 11:55;05.05.2025 12:16;02.05.2025 11:55;3-21;;;12740.0;;;Use Case Testing;;;;;Springer International Publishing;Cham;en;;;;;DOI.org (Crossref);;Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-79379-1_1;;;;"2. ML; 2.1. Traditional Machine Learning; 2.4. Active Learning Strategies; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.2. Test Data Generation; 3.2.3. Test Datasets (HTML, Screenshots, UML); 3.3. Functional Testing";;"Loulergue, Frédéric; Wotawa, Franz";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
9BW8AVA9;journalArticle;2024;"Garc?a, Boni; Leotta, Maurizio; Ricca, Filippo; Whitehead, Jim";Use of ChatGPT as an assistant in the end-to-end test script generation for android apps;;A-TEST@ISSTA;;;10.1145/3678719.3685691;;;2024;30.04.2025 13:51;05.05.2025 12:16;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\335QXC2Q\Garc?a et al. - 2024 - Use of ChatGPT as an assistant in the end-to-end test script generation for android apps.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.3. Functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
9AYNPQ6G;journalArticle;2025;"Pophale, S.; Elwasif, W.; Bernholdt, D.";Using a large language model as a building block to generate UsableValidation and verification suite for OpenMP;;Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region;;;10.1145/3712031.3712331;;;2025;30.04.2025 13:51;05.05.2025 12:16;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\4PRY4YVV\Pophale et al. - 2025 - Using a large language model as a building block to generate UsableValidation and verification suite.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.3. Functional Testing; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
V9ESDZF7;journalArticle;2024;"Marques, Nuno; Silva, Rodrigo Rocha; Bernardino, Jorge";Using chatgpt in software requirements engineering: A comprehensive review;42;Future Internet;;;;https://www.mdpi.com/1999-5903/16/6/180;;2024;30.04.2025 12:01;05.05.2025 12:16;30.04.2025 12:01;180;;6.0;16.0;;;Using chatgpt in software requirements engineering;;;;;;;;;;;;Google Scholar;;Publisher: MDPI;;C:\Users\DK00747\Zotero\storage\DV865PWW\Marques et al. - 2024 - Using chatgpt in software requirements engineering.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 4. Software Engineering; 4.1. Development Task Automation; 4.1.6. Requirement Generation; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
5MSXKJ7W;journalArticle;2023;"Khaliq, Zubair; Khaliq, Zubair; Khan, Dawood A.; Khan, Dawood Ashraf; Farooq, Sheikh Umar; Farooq, Sheikh Umar";Using deep learning for selenium web UI functional tests: A case-study with e-commerce applications;;Engineering Applications of Artificial Intelligence;;;10.1016/j.engappai.2022.105446;;;2023;30.04.2025 13:51;05.05.2025 12:16;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: 4307059230 tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\AK3HJNG7\Khaliq et al. - 2023 - Using deep learning for selenium web UI functional tests A case-study with e-commerce applications.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.3. Fine-tuned / Instruction-tuned; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.3. Functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
PSP8NFYQ;journalArticle;2024;"Haji, Khalid El; Brandt, C.; Zaidman, A.";Using GitHub copilot for test generation in python: An empirical study;;International Conference/Workshop on Automation of Software Test;;;10.1145/3644032.3644443;;"Writing unit tests is a crucial task in software development, but it is also recognized as a time-consuming and tedious task. As such, numerous test generation approaches have been proposed and investigated. However, most of these test generation tools produce tests that are typically difficult to understand. Recently, Large Language Models (LLMs) have shown promising results in generating source code and supporting software engineering tasks. As such, we investigate the usability of tests generated by GitHub Copilot, a proprietary closed-source code generation tool that uses an LLM. We evaluate GitHub Copilot’s test generation abilities both within and without an existing test suite, and we study the impact of different code commenting strategies on test generations. Our investigation evaluates the usability of 290 tests generated by GitHub Copilot for 53 sampled tests from open source projects. Our findings highlight that within an existing test suite, approximately 45.28% of the tests generated by Copilot are passing tests; 54.72% of generated tests are failing, broken, or empty tests. Furthermore, if we generate tests using Copilot without an existing test suite in place, we observe that 92.45% of the tests are failing, broken, or empty tests. Additionally, we study how test method comments influence the usability of test generations.";2024;30.04.2025 13:51;05.05.2025 12:16;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\TRKZQYI3\Haji et al. - 2024 - Using GitHub copilot for test generation in python An empirical study.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.3. Functional Testing; 3.4. Integration Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
GL4LS4BH;journalArticle;2025;"Chen, Feng-Kai; Liu, Chien-Hung; You, S. D.";Using large language model to fill in web forms to support automated web application testing;;Information-an International Interdisciplinary Journal;;;10.3390/info16020102;;Web applications, widely used by enterprises for business services, require extensive testing to ensure functionality. Performing form testing with random input data often takes a long time to complete. Previously, we introduced a model for automated testing of web applications using reinforcement learning. The model was trained to fill form fields with fixed input values and click buttons. However, the performance of this model was limited by a fixed set of input data and the imprecise detection of successful form submission. This paper proposes a model to address these limitations. First, we use a large language model with data fakers to generate a wide variety of input data. Additionally, whether form submission is successful is partially determined by GPT-4o. Experiments show that our method increases average statement coverage by 2.3;2025;30.04.2025 13:51;05.05.2025 12:16;;;;;;;Information;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\E8NLEYFP\Chen et al. - 2025 - Using large language model to fill in web forms to support automated web application testing.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.3. Functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
CSRNNZAX;journalArticle;2023;"Siddiq, Mohammed Latif; Santos, Joanna C. S.; Tanvir, Ridwanul Hasan; Ulfat, Noshin; Rifat, Fahmid Al; Lopes, Vinicius Carvalho";Using large language models to generate junit tests: An empirical study;;International Conference on Evaluation & Assessment in Software Engineering;;;10.1145/3661167.3661216;;A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning for a strongly typed language like Java. To fill this gap, we investigated how well three models (Codex, GPT-3.5-Turbo, and StarCoder) can generate unit tests. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the effect of context generation on the unit test generation process. We evaluated the models based on compilation rates, test correctness, test coverage, and test smells. We found that the Codex model achieved above 80;2023;30.04.2025 13:51;05.05.2025 12:17;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\LP9QXLS9\Siddiq et al. - 2023 - Using large language models to generate junit tests An empirical study.pdf;;"1. LLM; 1.1. Transformer-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.4. Test Repair; 3.1.8. Test Quality Analysis; 3.3. Functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
PF2NSFWL;conferencePaper;2024;"Della Porta, Antonio; De Martino, Vincenzo; Recupito, Gilberto; Iemmino, Carmine; Catolino, Gemma; Di Nucci, Dario; Palomba, Fabio";Using large language models to support software engineering documentation in waterfall life cycles: Are we there yet?;2;CEUR WORKSHOP PROCEEDINGS;;;;https://ceur-ws.org/Vol-3762/523.pdf;;2024;30.04.2025 12:01;05.05.2025 12:17;30.04.2025 12:01;452–457;;;3762.0;;;Using large language models to support software engineering documentation in waterfall life cycles;;;;;CEUR-WS;;;;;;;Google Scholar;;;;C:\Users\DK00747\Zotero\storage\YT62IW2A\Della Porta et al. - 2024 - Using large language models to support software en.pdf;;"1. LLM; 1.1. Transformer-based Models; 1.2. Multimodal Models; 4.1. Development Task Automation; 4.1.6. Requirement Generation; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
LBGW84AX;journalArticle;2025;"Sakai, Yuki; Tahara, Yasuyuki; Ohsuga, Akihiko; Sei, Y.";Using LLM-based deep reinforcement learning agents to detect bugs in web applications;;International Conference on Agents and Artificial Intelligence;;;10.5220/0013248800003890;;: This paper presents an approach to automate black-box GUI testing for web applications by integrating deep reinforcement learning (DRL) with large language models (LLMs). Traditional GUI testing is often inefficient and costly due to the difficulty in generating comprehensive test scenarios. While DRL has shown potential in automating exploratory testing by leveraging GUI interaction data, such data is browser-dependent and not always accessible in web applications. To address this challenge, we propose using LLMs to infer interaction information directly from HTML code, incorporating these inferences into the DRL’s state representation. We hypothesize that combining the inferential capabilities of LLMs with the robustness of DRL can match the accuracy of methods relying on direct data collection. Through experiments, we demonstrate that LLM-inferred interaction information effectively substitutes for direct data, enhancing both the efficiency and accuracy of automated GUI testing. Our results indicate that this approach not only streamlines GUI testing for web applications but also has broader implications for domains where direct state information is hard to obtain. The study suggests that integrating LLMs with DRL offers a promising path toward more efficient and scalable automation in GUI testing.;2025;30.04.2025 13:51;05.05.2025 12:17;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\9XQZ5SCU\Sakai et al. - 2025 - Using LLM-based deep reinforcement learning agents to detect bugs in web applications.pdf;;"1. LLM; 1.3. Fine-tuned / Instruction-tuned; 2. ML; 2.2. Deep Learning; 2.2.1. CNN / RNN / LSTM; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.2. Test Data Generation; 3.2.3. Test Datasets (HTML, Screenshots, UML); 3.3. Functional Testing";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
EQ8SF3YU;journalArticle;2025;"Rahman, Shanto; Kuhar, Sachit; ?irisci, Berk; Garg, P.; Wang, Shiqi; Ma, Xiaofei; Deoras, Anoop; Ray, Baishakhi";UTFix: Change aware unit test repairing using LLM;;Proceedings of the ACM on Programming Languages;;;10.1145/3720419;;Software updates, including bug repair and feature additions, are frequent in modern applications but they often leave test suites outdated, resulting in undetected bugs and increased chances of system failures. A recent study by Meta revealed that 14 In this paper, we present UTFix, a novel approach for repairing unit tests when their corresponding focal methods undergo changes. UTFix addresses two critical issues: assertion failure and reduced code coverage caused by changes in the focal method. Our approach leverages language models to repair unit tests by providing contextual information such as static code slices, dynamic code slices, and failure messages. We evaluate UTFix on our generated synthetic benchmarks (Tool-Bench), and real-world benchmarks. Tool- Bench includes diverse changes from popular open-source Python GitHub projects, where UTFix successfully repaired 89.2;2025;30.04.2025 13:51;05.05.2025 12:17;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\G5TL7KKF\Rahman et al. - 2025 - UTFix Change aware unit test repairing using LLM.pdf;;"1. LLM; 3. Software Testing; 3.1. Test Automation; 3.1.4. Test Repair; 3.1.6. Failure Detection; 3.1.8. Test Quality Analysis; 3.5. Robustness / Non-functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.5. Self-improving Prompts";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
PIISMLNU;journalArticle;2024;"Ma, Ruiyang; Yang, Yuxin; Liu, Ziqian; Zhang, Jiaxi; Li, Min; Huang, Junhua; Luo, Guojie";VerilogReader: LLM-aided hardware test generation;;2024 IEEE LLM Aided Design Workshop (LAD);;;10.1109/lad62341.2024.10691801;;Test generation has been a critical and labor-intensive process in hardware design verification. Recently, the emergence of Large Language Model (LLM) with their advanced understanding and inference capabilities, has introduced a novel approach. In this work, we investigate the integration of LLM into the Coverage Directed Test Generation (CDG) process, where the LLM functions as a Verilog Reader. It accurately grasps the code logic, thereby generating stimuli that can reach unexplored code branches. We compare our framework with random testing, using our self-designed Verilog benchmark suite. Experiments demonstrate that our framework outperforms random testing on designs within the LLM’s comprehension scope. Our work also proposes prompt engineering optimizations to augment LLM’s understanding scope and accuracy.;2024;30.04.2025 13:51;05.05.2025 12:17;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\KNNYSH2H\Ma et al. - 2024 - VerilogReader LLM-aided hardware test generation.pdf;;"2. ML; 2.1. Traditional Machine Learning; 2.1.2. Feature-based Models; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.8. Test Quality Analysis; 3.5. Robustness / Non-functional Testing; 5. Prompt Engineering; 5.1. Prompting Strategies";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
RUENRU7W;journalArticle;2023;"Yu, Shuling; Fang, Chunrong; Zhang, Quanjun; Chen, Chunyang; Chen, Zhenyu; Su, Zhendong";Vision-based mobile app GUI testing: a survey;;arXiv.org;;;10.48550/arxiv.2310.13518;;Graphical User Interface (GUI) has become one of the most significant parts of mobile applications (apps). It is a direct bridge between mobile apps and end users, which directly affects the end user's experience. Neglecting GUI quality can undermine the value and effectiveness of the entire mobile app solution. Significant research efforts have been devoted to GUI testing, one effective method to ensure mobile app quality. By conducting rigorous GUI testing, developers can ensure that the visual and interactive elements of the mobile apps not only meet functional requirements but also provide a seamless and user-friendly experience. However, traditional solutions, relying on the source code or layout files, have met challenges in both effectiveness and efficiency due to the gap between what is obtained and what app GUI actually presents. Vision-based mobile app GUI testing approaches emerged with the development of computer vision technologies and have achieved promising progress. In this survey paper, we provide a comprehensive investigation of the state-of-the-art techniques on 226 papers, among which 78 are vision-based studies. This survey covers different topics of GUI testing, like GUI test generation, GUI test record & replay, GUI testing framework, etc. Specifically, the research emphasis of this survey is placed mostly on how vision-based techniques outperform traditional solutions and have gradually taken a vital place in the GUI testing field. Based on the investigation of existing studies, we outline the challenges and opportunities of (vision-based) mobile app GUI testing and propose promising research directions with the combination of emerging techniques.;2023;30.04.2025 13:51;05.05.2025 12:17;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: 4387892121 tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\7LMGSWP5\Yu et al. - 2023 - Vision-based mobile app GUI testing a survey.pdf;;"2. ML; 2.1. Traditional Machine Learning; 3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.3. Functional Testing; 3.4. Integration Testing; 5. Prompt Engineering; 5.1. Prompting Strategies; 5.4. Visual / GUI-based Prompting";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
73MLFU8L;journalArticle;2025;"Tihanyi, Norbert; Bisztray, Tam?s; Ferrag, M.; Cherif, Bilel; Dubniczky, Richard A.; Jain, Ridhi; Cordeiro, Lucas C.";Vulnerability detection: From formal verification to large language models and hybrid approaches: a comprehensive overview;0;arXiv.org;;;10.48550/arxiv.2503.10784;;Software testing and verification are critical for ensuring the reliability and security of modern software systems. Traditionally, formal verification techniques, such as model checking and theorem proving, have provided rigorous frameworks for detecting bugs and vulnerabilities. However, these methods often face scalability challenges when applied to complex, real-world programs. Recently, the advent of Large Language Models (LLMs) has introduced a new paradigm for software analysis, leveraging their ability to understand insecure coding practices. Although LLMs demonstrate promising capabilities in tasks such as bug prediction and invariant generation, they lack the formal guarantees of classical methods. This paper presents a comprehensive study of state-of-the-art software testing and verification, focusing on three key approaches: classical formal methods, LLM-based analysis, and emerging hybrid techniques, which combine their strengths. We explore each approach's strengths, limitations, and practical applications, highlighting the potential of hybrid systems to address the weaknesses of standalone methods. We analyze whether integrating formal rigor with LLM-driven insights can enhance the effectiveness and scalability of software verification, exploring their viability as a pathway toward more robust and adaptive testing frameworks.;2025;30.04.2025 13:51;05.05.2025 12:17;;;;;;;;;;;;;;;;;;;;;;PMID: null tex.mag_id: null tex.pmcid: null;;C:\Users\DK00747\Zotero\storage\WJ2SFHQN\Tihanyi et al. - 2025 - Vulnerability detection From formal verification to large language models and hybrid approaches a.pdf;;"1. LLM; 2. ML; 4. Software Engineering; 4.1. Development Task Automation; 4.1.1. Code Generation; 4.1.6. Requirement Generation";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
N58XEWNE;journalArticle;2021;"Nass, Michel; Alégroth, Emil; Feldt, Robert";Why many challenges with GUI test automation (will) remain;71;Information and Software Technology;;09505849;10.1016/j.infsof.2021.106625;https://linkinghub.elsevier.com/retrieve/pii/S0950584921000963;;2021-10;04.05.2025 10:31;05.05.2025 12:17;04.05.2025 10:31;106625;;;138.0;;Information and Software Technology;;;;;;;;en;;;;;DOI.org (Crossref);;;;C:\Users\DK00747\Zotero\storage\XSMK4AYR\Nass et al. - 2021 - Why many challenges with GUI test automation (will) remain.pdf;;"3. Software Testing; 3.1. Test Automation; 3.1.1. Test Generation; 3.1.4. Test Repair; 3.1.7. Self-healing Testing; 3.1.8. Test Quality Analysis";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
