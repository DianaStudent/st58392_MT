TY  - JOUR
TI  - A critical review of large language model on software engineering: An example from ChatGPT and automated program repair
AU  - Zhang, Quanjun
AU  - Zhang, Tongke
AU  - Zhai, Jun
AU  - Fang, Chunrong
AU  - Yu, Bo
AU  - Sun, Weisong
AU  - Chen, Zhenyu
T2  - arXiv.org
AB  - Large Language Models (LLMs) have been gaining increasing attention and demonstrated promising performance across a variety of Software Engineering (SE) tasks, such as Automated Program Repair (APR), code summarization, and code completion. For example, ChatGPT, the latest black-box LLM, has been investigated by numerous recent research studies and has shown impressive performance in various tasks. However, there exists a potential risk of data leakage since these LLMs are usually close-sourced with unknown specific training details, e.g., pre-training datasets. In this paper, we seek to review the bug-fixing capabilities of ChatGPT on a clean APR benchmark with different research objectives. We first introduce , a new benchmark with buggy and the corresponding fixed programs from competitive programming problems starting from 2023, after the training cutoff point of ChatGPT. The results on  show that ChatGPT is able to fix 109 out of 151 buggy programs using the basic prompt within 35 independent rounds, outperforming state-of-the-art LLMs CodeT5 and PLBART by 27.5% and 62.4% prediction accuracy. We also investigate the impact of three types of prompts, i.e., problem description, error feedback, and bug localization, leading to additional 34 fixed bugs. Besides, we provide additional discussion from the interactive nature of ChatGPT to illustrate the capacity of a dialog-based repair workflow with 9 additional fixed bugs. Inspired by the findings, we further pinpoint various challenges and opportunities for advanced SE study equipped with such LLMs (e.g., ChatGPT) in the near future. More importantly, our work calls for more research on the reevaluation of the achievements obtained by existing black-box LLMs across various SE tasks, not limited to ChatGPT on APR.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arxiv.2310.08879
N1  - Citation Count: 19
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.2. Code Repair / Refactoring
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - A Deep Learning-based automated framework for functional User Interface testing
AU  - Zubair, Khaliq
AU  - Zubair, Khaliq
AU  - Umar, Sheikh, Farooq
AU  - Umar, Sheikh, Farooq
AU  - Ashraf, Dawood, Khan
AU  - Ashraf, Dawood, Khan
T2  - Information & Software Technology
DA  - 2022///
PY  - 2022
DO  - 10.1016/j.infsof.2022.106969
N1  - Citation Count: 11
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.2.1. CNN / RNN / LSTM
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.4. Test Repair
KW  - 3.2. Test Data Generation
KW  - 3.2.1. Test Oracle
KW  - 3.2.3. Test Datasets (HTML
KW  - 3.3. Functional Testing
KW  - Screenshots
KW  - UML)
ER  - 

TY  - CONF
TI  - A literature survey of assertions in software testing
AU  - Masoumeh, Taromirad
AU  - Per, Runeson
T3  - International Conference on Engineering of Computer-Based Systems
DA  - 2024///
PY  - 2024
SP  - 75
EP  - 96
PB  - Springer
UR  - https://link.springer.com/chapter/10.1007/978-3-031-49252-5_8
Y2  - 2025/04/30/
N1  - Citation Count: 6
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.6. Failure Detection
KW  - 3.2. Test Data Generation
KW  - 3.2.1. Test Oracle
KW  - 3.3. Functional Testing
KW  - 3.5. Robustness / Non-functional Testing
ER  - 

TY  - JOUR
TI  - A machine learning approach for automated filling of categorical fields in data entry forms
AU  - Hichem, Belgacem
AU  - Xiaochen, Li
AU  - Domenico, Bianculli
AU  - Lionel, Briand
T2  - ACM Transactions on Software Engineering and Methodology
AB  - Users frequently interact with software systems through data entry forms. However, form filling is time-consuming and error-prone. Although several techniques have been proposed to auto-complete or pre-fill fields in the forms, they provide limited support to help users fill categorical fields, i.e., fields that require users to choose the right value among a large set of options. In this article, we propose LAFF, a learning-based automated approach for filling categorical fields in data entry forms. LAFF first builds Bayesian Network models by learning field dependencies from a set of historical input instances, representing the values of the fields that have been filled in the past. To improve its learning ability, LAFF uses local modeling to effectively mine the local dependencies of fields in a cluster of input instances. During the form filling phase, LAFF uses such models to predict possible values of a target field, based on the values in the already-filled fields of the form and their dependencies; the predicted values (endorsed based on field dependencies and prediction confidence) are then provided to the end-user as a list of suggestions. We evaluated LAFF by assessing its effectiveness and efficiency in form filling on two datasets, one of them proprietary from the banking domain. Experimental results show that LAFF is able to provide accurate suggestions with a Mean Reciprocal Rank value above 0.73. Furthermore, LAFF is efficient, requiring at most 317 ms per suggestion.
DA  - 2023///
PY  - 2023
DO  - 10.1145/3533021
VL  - 32
IS  - 2
SP  - 1
EP  - 40
LA  - en
SN  - 1049-331X, 1557-7392
UR  - https://dl.acm.org/doi/10.1145/3533021
Y2  - 2025/04/05/
N1  - Citation Count: 14
KW  - 2. ML
KW  - 2.1. Traditional Machine Learning
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
ER  - 

TY  - JOUR
TI  - A multi-year grey literature review on AI-assisted test automation
AU  - Filippo, Ricca
AU  - Alessandro, Marchetto
AU  - A., Marchetto
AU  - Andrea, Stocco
T2  - arXiv.org
AB  - Context: Test Automation (TA) techniques are crucial for quality assurance in software engineering but face limitations such as high test suite maintenance costs and the need for extensive programming skills. Artificial Intelligence (AI) offers new opportunities to address these issues through automation and improved practices. Objectives: Given the prevalent usage of AI in industry, sources of truth are held in grey literature as well as the minds of professionals, stakeholders, developers, and end-users. This study surveys grey literature to explore how AI is adopted in TA, focusing on the problems it solves, its solutions, and the available tools. Additionally, the study gathers expert insights to understand AI's current and future role in TA. Methods: We reviewed over 3,600 grey literature sources over five years, including blogs, white papers, and user manuals, and finally filtered 342 documents to develop taxonomies of TA problems and AI solutions. We also cataloged 100 AI-driven TA tools and interviewed five expert software testers to gain insights into AI's current and future role in TA. Results: The study found that manual test code development and maintenance are the main challenges in TA. In contrast, automated test generation and self-healing test scripts are the most common AI solutions. We identified 100 AI-based TA tools, with Applitools, Testim, Functionize, AccelQ, and Mabl being the most adopted in practice. Conclusion: This paper offers a detailed overview of AI's impact on TA through grey literature analysis and expert interviews. It presents new taxonomies of TA problems and AI solutions, provides a catalog of AI-driven tools, and relates solutions to problems and tools to solutions. Interview insights further revealed the state and future potential of AI in TA. Our findings support practitioners in selecting TA tools and guide future research directions.
DA  - 2024///
PY  - 2024
DO  - 10.48550/arxiv.2408.06224
N1  - Citation Count: 1
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.2.1. CNN / RNN / LSTM
KW  - 2.5. Reinforcement Learning Strategies
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.2. Test Optimization
KW  - 3.1.4. Test Repair
KW  - 3.2. Test Data Generation
KW  - 3.2.1. Test Oracle
ER  - 

TY  - JOUR
TI  - A new approach for automatic test case generation from use case diagram using LLMs and prompt engineering
AU  - Lahbib, Naimi
AU  - Mahi, El, Bouziane
AU  - Mohamed, Manaouch
AU  - Abdeslam, Jakim
T2  - IEEE International Conference on Circuits and Systems for Communications
AB  - The automation of test case generation from UML diagrams is a growing field that aims to make the software development process smoother. This paper suggests a new framework that uses generative artificial intelligence (AI) to turn use case diagrams into test cases that can be executed. By getting information from the XML representation of use case diagrams, we can create detailed instructions that guide a generative AI model to make test cases for each use case scenario. This method not only makes test case creation easier but also ensures we cover everything well and accurately, which could make software products get to market faster. this approach shows how traditional software engineering methods and new AI techniques can work well together, giving us an idea of what automated software testing might look like in the future.
DA  - 2024///
PY  - 2024
DO  - 10.1109/iccsc62074.2024.10616548
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.2. Test Optimization
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.4. Visual / GUI-based Prompting
ER  - 

TY  - CONF
TI  - A noval approach to automated test script generation using large language models for domain-specific language
AU  - J., Sun
AU  - J., Wang
AU  - Y., Zhu
AU  - X., Li
AU  - Y., Xie
AU  - J., Chen
AB  - This paper presents a novel method for generating automated test scripts for Domain-Specific Languages (DSLs) in software testing, particularly for the automotive industry. It emphasizes the growing importance of software testing in ensuring product quality amid IT advancements. The paper reviews software testing's evolution, modern processes, and the role of Large Language Models (LLMs). It highlights DSLs' significance and uses the automotive sector to show how LLMs can automate test script generation. Tests indicate that in cases with a small sample size, the effectiveness of prompt engineering is superior to model fine-tuning. The proposed method thus relies on well-designed prompts to direct LLMs to produce accurate scripts. The generation system's overview is discussed, along with an evaluation of the scripts' quality using metrics like Levenshtein Distance. Results indicate that LLMs boost test automation, defect detection, and software reliability. Future work will optimize these tools for higher testing automation levels. © 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
DA  - 2024///
PY  - 2024
VL  - 3864
SP  - 52
EP  - 57
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.2. Test Optimization
KW  - 3.1.6. Failure Detection
KW  - 3.1.8. Test Quality Analysis
KW  - 5.1. Prompting Strategies
KW  - 5.2. Few-shot / Zero-shot
ER  - 

TY  - CONF
TI  - A reinforcement learning approach to generating test cases for web applications
AU  - Xiaoning, Chang
AU  - Zheheng, Liang
AU  - Yifei, Zhang
AU  - Lei, Cui
AU  - Zhenyue, Long
AU  - Guoquan, Wu
AU  - Yu, Gao
AU  - Wei, Chen
AU  - Jun, Wei
AU  - Tao, Huang
T3  - 2023 IEEE/ACM International Conference on Automation of Software Test (AST)
DA  - 2023///
PY  - 2023
SP  - 13
EP  - 23
PB  - IEEE
UR  - https://ieeexplore.ieee.org/abstract/document/10173983/
Y2  - 2025/04/05/
N1  - Citation Count: 8
KW  - 2. ML
KW  - 2.5. Reinforcement Learning Strategies
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.3. Functional Testing
ER  - 

TY  - JOUR
TI  - A review of AI-augmented end-to-end test automation tools
AU  - Phuoc, Pham
AU  - Vu, Nguyen
AU  - N., Tien, Nguyen
T2  - International Conference on Automated Software Engineering
AB  - Software testing is a process of evaluating and verifying whether a software product still works as expected, and it is repetitive, laborious, and time-consuming. To address this problem, automation tools have been developed to automate testing activities and enhance quality and delivery time. However, automation tools become less effective with continuous integration and continuous delivery (CI/CD) pipelines when the system under test is constantly changing. Recent advances in artificial intelligence and machine learning (AI/ML) present the potential for addressing important challenges in test automation. AI/ML can be applied to automate various testing activities such as detecting bugs and errors, maintaining existing test cases, or generating new test cases much faster than humans.
DA  - 2022///
PY  - 2022
DO  - 10.1145/3551349.3563240
N1  - Citation Count: 8
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.9. Automated testing tools
KW  - 4. Software Engineering
KW  - 4.2. Development Process Automation
KW  - 4.2.1. CI/CD
KW  - 4.2.4. Low-code / No-code Development
ER  - 

TY  - JOUR
TI  - A roadmap for software testing in open-collaborative and AI-powered era
AU  - Qing, Wang
AU  - Junjie, Wang
AU  - Mingyang, Li
AU  - Yawen, Wang
AU  - Zhe, Liu
T2  - ACM Transactions on Software Engineering and Methodology
AB  - Internet technology has given rise to an open-collaborative software development paradigm, necessitating the open-collaborative schema to software testing. It enables diverse and globally distributed contributions, but also presents significant challenges to efficient testing processes, coordination among personnel, and management of testing artifacts. At the same time, advancements in artificial intelligence (AI) have enhanced testing capabilities and enabling automation, while also introducing new testing needs and unique challenges for AI-based systems. In this context, this paper explores software testing in the open-collaborative and AI-powered era, focusing on the interrelated dimensions of process, personnel, and technology. Among them, process involves managing testing workflows and artifacts to improve efficiency, personnel emphasizes the role of individuals in ensuring testing quality through collaboration and contributions, while technology refers to AI methods that enhance testing capabilities and address challenges in AI-based systems. Furthermore, we delve into the challenges and opportunities arising from emerging technologies such as large language models (LLMs) and the AI model-centric development paradigm.
DA  - 2024///
PY  - 2024
DO  - 10.1145/3709355
N1  - Citation Count: 0
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.2. Test Optimization
KW  - 3.1.8. Test Quality Analysis
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 4.1.5. Requirement Classification
KW  - 4.1.6. Requirement Generation
ER  - 

TY  - JOUR
TI  - A study of using multimodal llms for non-crash functional bug detection in android apps
AU  - Bangyan, Ju
AU  - Jin, Yang
AU  - Tingting, Yu
AU  - Tamerlan, Abdullayev
AU  - Yuanyuan, Wu
AU  - Dingbang, Wang
AU  - Yu, Zhao
T2  - Asia-Pacific Software Engineering Conference
AB  - Numerous approaches employing various strategies have been developed to test the graphical user interfaces (GUIs) of mobile apps. However, traditional GUI testing techniques, such as random and model-based testing, primarily focus on generating test sequences that excel in achieving high code coverage but often fail to act as effective test oracles for noncrash functional (NCF) bug detection. To tackle these limitations, this study empirically investigates the capability of leveraging large language models (LLMs) to be test oracles to detect NCF bugs in Android apps. Our intuition is that the training corpora of LLMs, encompassing extensive mobile app usage and bug report descriptions, enable them with the domain knowledge relevant to NCF bug detection. We conducted a comprehensive empirical study to explore the effectiveness of LLMs as test oracles for detecting NCF bugs in Android apps on 71 welldocumented NCF bugs. The results demonstrated that LLMs achieve a 49
DA  - 2024///
PY  - 2024
DO  - 10.1109/apsec65559.2024.00017
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.6. Failure Detection
KW  - 3.2. Test Data Generation
KW  - 3.2.1. Test Oracle
KW  - 3.2.3. Test Datasets (HTML
KW  - 3.3. Functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - Screenshots
KW  - UML)
ER  - 

TY  - JOUR
TI  - A study on C code defect detection with fine-tuned large language models
AU  - Yue, Wang
AU  - Xu, Wang
AU  - Hongwei, Yu
AU  - Fei, Gao
AU  - Xueshi, Liu
AU  - Xiaoling, Wang
T2  - Asia-Pacific Software Engineering Conference
AB  - Large Language Models(LLMs) have demonstrated excellent capabilities in many areas of software engineering(SE), including code completion, code generation, code understanding, code repair, etc., and the most prominent performer in this regard is ChatGPT. However, its cost of use makes the integration of ChatGPT into code defect detection techniques costly. In this paper, we focus on low-cost-of-use, fine-tunable, open-source large language models with less than 10B parameters, and study their capabilities of C code defect detection when fine-tuned with real-world data and improved with prompt engineering. We studied LLaMa3-8B, DeepSeek-Coder-7b and Qwen2-7B, as they are the typical models with prompt capabilities, whose performance in SE is close to ChatGPT, and they are open-source models. Experimental results show that our method can significantly improve the performance of LLMs within 10B parameters on code defect detection, and the output of the models can be applied to several downstream tasks, such as improving the report quality of static analysis tools.
DA  - 2024///
PY  - 2024
DO  - 10.1109/apsec65559.2024.00055
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 4.1.2. Code Repair / Refactoring
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - A survey of testing techniques based on large language models
AU  - Fei, Qi
AU  - Ying, Hou
AU  - Ning, Lin
AU  - Shanshan, Bao
AU  - Nuo, Xu
T2  - Proceedings of the 2024 International Conference on Computer and Multimedia Technology
DA  - 2024///
PY  - 2024
DO  - 10.1145/3675249.3675298
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.2.3. Transfer / Ensemble Learning
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.2. Test Data Generation
KW  - 3.2.3. Test Datasets (HTML
KW  - 3.4. Integration Testing
KW  - 3.5. Robustness / Non-functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - Screenshots
KW  - UML)
ER  - 

TY  - JOUR
TI  - A survey on evaluating large language models in code generation tasks
AU  - Liguo, Chen
AU  - Qi, Guo
AU  - Hongrui, Jia
AU  - Zhengran, Zeng
AU  - Xin, Wang
AU  - Yijiang, Xu
AU  - Jian, Wu
AU  - Yidong, Wang
AU  - Qing, Gao
AU  - Jindong, Wang
AU  - Wei, Ye
AU  - Shikun, Zhang
T2  - arXiv.org
AB  - This paper provides a comprehensive review of the current methods and metrics used to evaluate the performance of Large Language Models (LLMs) in code generation tasks. With the rapid growth in demand for automated software development, LLMs have demonstrated significant potential in the field of code generation. The paper begins by reviewing the historical development of LLMs and their applications in code generation. Next, it details various methods and metrics for assessing the code generation capabilities of LLMs, including code correctness, efficiency, readability, and evaluation methods based on expert review and user experience. The paper also evaluates the widely used benchmark datasets, identifying their limitations and proposing directions for future improvements. Specifically, the paper analyzes the performance of code generation models across different tasks by combining multiple evaluation metrics, such as code compilation/interpretation success rates, unit test pass rates, and performance and efficiency metrics, to comprehensively assess the practical application of LLMs in code generation. Finally, the paper discusses the challenges faced in evaluating LLMs in code generation, particularly how to ensure the comprehensiveness and accuracy of evaluation methods and how to adapt to the evolving practices of software development. These analyses and discussions provide valuable insights for further optimizing and improving the application of LLMs in code generation tasks.
DA  - 2024///
PY  - 2024
DO  - 10.48550/arxiv.2408.16498
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
ER  - 

TY  - JOUR
TI  - A system for automated unit test generation using large language models and assessment of generated test suites
AU  - Andrea, Lops
AU  - F., Narducci
AU  - Azzurra, Ragone
AU  - Michelantonio, Trizio
AU  - Claudio, Bartolini
T2  - International Conference on Software Testing, Verification and Validation Workshops
AB  - Unit tests are fundamental for ensuring software correctness but are costly and time-intensive to design and create. Recent advances in Large Language Models (LLMs) have shown potential for automating test generation, though existing evaluations often focus on simple scenarios and lack scalability for real-world applications. To address these limitations, we present AgoneTest, an automated system for generating and assessing complex, class-level test suites for Java projects. Leveraging the Methods2Test dataset, we developed Classes2Test, a new dataset enabling the evaluation of LLM-generated tests against human-written tests. Our key contributions include a scalable automated software system, a new dataset, and a detailed methodology for evaluating test quality.
DA  - 2024///
PY  - 2024
DO  - 10.1109/icstw64639.2025.10962454
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.4. Test Repair
KW  - 3.1.8. Test Quality Analysis
KW  - 3.5. Robustness / Non-functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - A3Test: Assertion-augmented automated test case generation
AU  - Saranya, Alagarsamy
AU  - Chakkrit, Tantithamthavorn
AU  - Aldeida, Aleti
T2  - Information and Software Technology
AB  - Test case generation is an important activity, yet a time-consuming and laborious task. Recently, AthenaTest – a deep learning approach for generating unit test cases – is proposed. However, AthenaTest can generate less than one-fifth of the test cases correctly, due to a lack of assertion knowledge and test signature verification. In this paper, we propose A3Test, a DL-based test case generation approach that is augmented by assertion knowledge with a mechanism to verify naming consistency and test signatures. A3Test leverages the domain adaptation principles where the goal is to adapt the existing knowledge from an assertion generation task to the test case generation task. We also introduce a verification approach to verify naming consistency and test signatures. Through an evaluation of 5,278 focal methods from the Defects4j dataset, we find that our A3Test (1) achieves 147
DA  - 2023///
PY  - 2023
DO  - 10.48550/arxiv.2302.10352
N1  - Citation Count: 3
KW  - 1. LLM
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.2.1. CNN / RNN / LSTM
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.4. Test Repair
KW  - 3.1.8. Test Quality Analysis
ER  - 

TY  - CONF
TI  - Accuracy improvement by training data selection in automatic test cases generation method
AU  - Kiyoshi, Ueda
AU  - Hikaru, Tsukada
T3  - 2021 9th International Conference on Information and Education Technology (ICIET)
DA  - 2021///
PY  - 2021
DO  - 10.1109/ICIET51873.2021.9419636
SP  - 438
EP  - 442
PB  - IEEE
SN  - 978-1-6654-1933-8
UR  - https://ieeexplore.ieee.org/document/9419636/
Y2  - 2025/02/05/
N1  - Citation Count: 3
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.2. Test Optimization
KW  - 3.1.8. Test Quality Analysis
KW  - 3.2. Test Data Generation
KW  - 3.2.3. Test Datasets (HTML
KW  - 3.3. Functional Testing
KW  - Screenshots
KW  - UML)
ER  - 

TY  - JOUR
TI  - Adaptive test generation using a large language model
AU  - Max, Schдfer
AU  - Sarah, Nadi
AU  - Aryaz, Eghbali
AU  - Frank, Tip
T2  - arXiv preprint arXiv:2302.06527
DA  - 2023///
PY  - 2023
SP  - 1
EP  - 21
N1  - Citation Count: 91
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.5. Self-improving Prompts
ER  - 

TY  - JOUR
TI  - Adaptive test healing using LLM/GPT and reinforcement learning
AU  - Nariman, Mani
AU  - Salma, Attaranasl
T2  - International Conference on Software Testing, Verification and Validation Workshops
AB  - Flaky tests disrupt software development pipelines by producing inconsistent results, undermining reliability and efficiency. This paper introduces a hybrid framework for adaptive test healing, combining Large Language Models (LLMs) like GPT with Reinforcement Learning (RL) to address test flakiness dynamically. LLMs analyze test logs to classify failures and extract contextual insights, while the RL agent learns optimal strategies for test retries, parameter tuning, and environment resets. Experimental results demonstrate the framework's effectiveness in reducing flakiness and improving CI/CD pipeline stability, outperforming traditional approaches. This work paves the way for scalable, intelligent test automation in dynamic development environments.
DA  - 2025///
PY  - 2025
DO  - 10.1109/icstw64639.2025.10962516
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 2. ML
KW  - 2.5. Reinforcement Learning Strategies
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.2. Test Optimization
KW  - 3.1.4. Test Repair
KW  - 3.1.7. Self-healing Testing
ER  - 

TY  - CONF
TI  - Adaptive testing for LLM-based applications: a diversity-based approach
AU  - Juyeon, Yoon
AU  - Robert, Feldt
AU  - Shin, Yoo
T3  - 2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
DA  - 2025///
PY  - 2025
SP  - 375
EP  - 382
PB  - IEEE
ST  - Adaptive testing for LLM-based applications
UR  - https://ieeexplore.ieee.org/abstract/document/10962467/
Y2  - 2025/04/30/
N1  - Citation Count: 0
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.3. Test Classification
KW  - 3.1.4. Test Repair
KW  - 3.1.5. Test Prioritization
KW  - 3.1.6. Failure Detection
KW  - 3.1.8. Test Quality Analysis
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Advancing software testing: Integrating AI, machine learning, and emerging technologies
AU  - Ragavula, Madhumita
AU  - Daravath, Chandana
T2  - environments
DA  - 2025///
PY  - 2025
VL  - 7
SP  - 9
ST  - Advancing software testing
UR  - https://www.academia.edu/download/121021281/IJRESM_V8_I1_14.pdf
Y2  - 2025/04/30/
N1  - Citation Count: 0
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.2.2. Autoencoders / GANs
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.3. Test Classification
KW  - 3.1.5. Test Prioritization
KW  - 5. Prompt Engineering
KW  - 5.4. Visual / GUI-based Prompting
ER  - 

TY  - CONF
TI  - AgoneTest: Automated creation and assessment of unit tests leveraging large language models
AU  - Andrea, Lops
AU  - Fedelucio, Narducci
AU  - Azzurra, Ragone
AU  - Michelantonio, Trizio
T3  - Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering
DA  - 2024///
PY  - 2024
SP  - 2440
EP  - 2441
ST  - AgoneTest
UR  - https://dl.acm.org/doi/abs/10.1145/3691620.3695318
Y2  - 2025/04/30/
N1  - Citation Count: 2
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.4. Test Repair
KW  - 3.1.8. Test Quality Analysis
KW  - 3.1.9. Automated testing tools
KW  - 3.3. Functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - AI-driven innovations in software engineering: a review of current practices and future directions
AU  - Mamdouh, Alenezi
AU  - Mohammed, Akour
T2  - Applied Sciences
DA  - 2025///
PY  - 2025
VL  - 15
IS  - 3
SP  - 1344
ST  - AI-driven innovations in software engineering
UR  - https://www.mdpi.com/2076-3417/15/3/1344
Y2  - 2025/04/30/
N1  - Citation Count: 10
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 2. ML
KW  - 2.1. Traditional Machine Learning
KW  - 2.2. Deep Learning
KW  - 2.3. Supervised / Unsupervised Learning Strategies
KW  - 2.4. Active Learning Strategies
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 4.2. Development Process Automation
KW  - 4.2.1. CI/CD
KW  - 4.2.4. Low-code / No-code Development
ER  - 

TY  - JOUR
TI  - AI-generated test scripts for web E2E testing with ChatGPT and copilot: a preliminary study
AU  - Maurizio, Leotta
AU  - Z., H., Yousaf
AU  - Filippo, Ricca
AU  - Boni, Garc?a
T2  - International Conference on Evaluation & Assessment in Software Engineering
AB  - Automated testing is vital for ensuring the reliability of web applications. This paper presents a preliminary study on leveraging artificial intelligence (AI) models, specifically ChatGPT and Github Copilot, to generate test scripts for web end-to-end testing. Through experimentation, we evaluated the feasibility and effectiveness of AI language models in generating test scripts based on natural language descriptions of user interactions with web applications. Our preliminary results show that AI-based generation generally provides an advantage over fully manual test scripts development. Starting from test cases clearly defined in Gherkin, a reduction in development time is always observable. In some cases, this reduction is statistically significant (e.g., Manual vs. a particular use of ChatGPT). These results are valid provided that the tester has some skills in manual test script development and is therefore able to modify the code produced by the AI-generation tools. This study contributes to the exploration of AI-driven solutions in web test scripts generation and lays the foundation for future research in this domain.
DA  - 2024///
PY  - 2024
DO  - 10.1145/3661167.3661192
N1  - Citation Count: 1
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.4. Test Repair
KW  - 3.1.8. Test Quality Analysis
KW  - 3.3. Functional Testing
ER  - 

TY  - JOUR
TI  - AI-powered multi-agent framework for automated unit test case generation: Enhancing software quality through llm’s
AU  - Anusha, Garlapati
AU  - Satya, Sai
AU  - Muni, Dr, Parmesh
AU  - S, Jaisri, Savitha
T2  - 2024 5th IEEE Global Conference for Advancement in Technology (GCAT)
AB  - Recent years have witnessed an enormous rise in the design, repair and the enhancement of software automation tests. The reliability of program’s unit testing has major impact on its overall performance. The anticipated influence of Artificial Intelligence advancements on test automation methodologies are significant. Many studies on automated testing implicitly assume that the test results are deterministic, means that similar tests faults remain same. The precision of software is largely ensured by unit testing. But writing unit tests manually is a time-consuming process, which leads us to drive into "Automation Analysis". Recent years comprised the application of Large Language Models (LLM’s) in numerous fields related to software development, especially the automated creation of unit testing.However, these frameworks require more instructions, or few shot learnings on sample tests that already exist. This research provides a comprehensive empirical assessment of the efficiency of LLM’s for automating unit testing production, with no need for further manual analysis. The method we employ is put into practice for test cases, an adaptable Agents and LLM-based testing framework that evaluates test cases generated, by reviewing and re-writing them in different phases. Evaluation of this test cases was done by using mistral-large LLM Model. The analysis results that developed acquired an overall coverage of 100
DA  - 2024///
PY  - 2024
DO  - 10.1109/gcat62922.2024.10923987
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.4. Test Repair
KW  - 3.1.8. Test Quality Analysis
KW  - 5.1. Prompting Strategies
KW  - 5.5. Self-improving Prompts
ER  - 

TY  - JOUR
TI  - AI-powered software testing: The impact of large language models on testing methodologies
AU  - Vahit, Bayr?
AU  - Ece, Demirel
T2  - 2023 4th International Informatics and Software Engineering Conference (IISEC)
AB  - Software testing is a crucial aspect of the software development lifecycle, ensuring the delivery of high-quality, reliable, and secure software systems. With the advancements in Artificial Intelligence (AI) and Natural Language Processing (NLP), Large Language Models (LLMs) have emerged as powerful tools capable of understanding and processing natural language texts easly. This article investigates the application of AI-based software testing, with a specific focus on the impact of LLMs in traditional testing methodologies. Through a comprehensive review of relevant literature and SeturDigital’s 25 year testing experience, this article explores the potential benefits, challenges, and prospects of integrating LLMs into software testing.
DA  - 2023///
PY  - 2023
DO  - 10.1109/iisec59749.2023.10391027
N1  - Citation Count: 3
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.3. Functional Testing
KW  - 3.4. Integration Testing
KW  - 3.5. Robustness / Non-functional Testing
ER  - 

TY  - JOUR
TI  - An approach to generating API test scripts using GPT
AU  - C., Nguyen
AU  - Huy, Bui
AU  - Vu-Loc, Nguyen
AU  - Tien, Nguyen
T2  - Symposium on Information and Communication Technology
AB  - As more software systems publish and use web services or APIs today, automated API testing is an important activity to help effectively ensure the quality of software services before they are released for their usage. Generating test scripts and data is a crucial step to perform API test automation successfully. In this paper, we propose an approach leveraging GPT, a large language model, and API’s Swagger specification to automatically generate test scripts and test data for API testing. Our approach also applies GPT’s self-refining with the feedback by executing tests on Katalon. We evaluate our proposed approach using a data set of seven APIs consisting of 157 endpoints and 179 operations. The result shows that while our approach generates fewer test scripts and data inputs, it can cover more successful status codes of 2xx than a state-of-the-art tool. This result suggests that leveraging the ability of GPT as a large language model to interpret API’s Swagger specification has the potential for improving the efficacy of generating test scripts and data for API testing.
DA  - 2023///
PY  - 2023
DO  - 10.1145/3628797.3628947
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.4. Integration Testing
ER  - 

TY  - JOUR
TI  - An initial investigation of ChatGPT unit test generation capability
AU  - Vitor, Guilherme
AU  - Rizzo, Auri Marcelo, Vincenzi
T2  - Brazilian Symposium on Systematic and Automated Software Testing
AB  - Context: Software testing ensures software quality, but developers often disregard it. The use of automated testing generation is pursued to reduce the consequences of overlooked test cases in a software project. Problem: In the context of Java programs, several tools can completely automate generating unit test sets. Additionally, studies are conducted to offer evidence regarding the quality of the generated test sets. However, it is worth noting that these tools rely on machine learning and other AI algorithms rather than incorporating the latest advancements in Large Language Models (LLMs). Solution: This work aims to evaluate the quality of Java unit tests generated by an OpenAI LLM algorithm, using metrics like code coverage and mutation test score. Method: For this study, 33 programs used by other researchers in the field of automated test generation were selected. This approach was employed to establish a baseline for comparison purposes. For each program, 33 unit test sets were generated automatically, without human interference, by changing Open AI API parameters. After executing each test set, metrics such as code line coverage, mutation score, and success rate of test execution were collected to evaluate the efficiency and effectiveness of each set. Summary of Results: Our findings revealed that the OpenAI LLM test set demonstrated similar performance across all evaluated aspects compared to traditional automated Java test generation tools used in the previous research. These results are particularly remarkable considering the simplicity of the experiment and the fact that the generated test code did not undergo human analysis.
DA  - 2023///
PY  - 2023
DO  - 10.1145/3624032.3624035
N1  - Citation Count: 17
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.3. Functional Testing
KW  - 3.4. Integration Testing
ER  - 

TY  - JOUR
TI  - An intelligent test management system for optimizing decision making during software testing
AU  - Albin, Lцnnfдlt
AU  - Viktor, Tu
AU  - Gregory, Gay
AU  - Animesh, Singh
AU  - Sahar, Tahvili
T2  - Journal of Systems and Software
DA  - 2025///
PY  - 2025
DO  - 10.1016/j.jss.2024.112202
N1  - Citation Count: 1
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 2. ML
KW  - 2.1. Traditional Machine Learning
KW  - 2.1.1. SVM / Decision Trees / Random Forest
KW  - 2.3. Supervised / Unsupervised Learning Strategies
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.3. Test Classification
KW  - 3.1.5. Test Prioritization
ER  - 

TY  - JOUR
TI  - Are we testing or being tested? Exploring the practical applications of large language models in software testing
AU  - Robson, Santos
AU  - ?talo, Santos
AU  - C., Magalh?es
AU  - S., Ronnie E., Santos
T2  - International Conference on Information Control Systems & Technologies
AB  - A Large Language Model (LLM) represents a cutting-edge artificial intelligence model that generates content, including grammatical sentences, human-like paragraphs, and syntactically code snippets. LLMs can play a pivotal role in soft-ware development, including software testing. LLMs go beyond traditional roles such as requirement analysis and documentation and can support test case generation, making them valuable tools that significantly enhance testing practices within the field. Hence, we explore the practical application of LLMs in software testing within an industrial setting, focusing on their current use by professional testers. In this context, rather than relying on existing data, we conducted a cross-sectional survey and collected data within real working contexts-specifically, engaging with practitioners in industrial settings. We applied quantitative and qualitative techniques to analyze and synthesize our collected data. Our findings demonstrate that LLMs effectively enhance testing documents and significantly assist testing professionals in programming tasks like debugging and test case automation. LLMs can support individuals engaged in manual testing who need to code. However, it is crucial to emphasize that, at this early stage, software testing professionals should use LLMs with caution while well-defined methods and guidelines are being built for the secure adoption of these tools.
DA  - 2023///
PY  - 2023
DO  - 10.1109/icst60714.2024.00039
N1  - Citation Count: 7
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 4. Software Engineering
KW  - 4.2. Development Process Automation
KW  - 4.2.2. Debugging Support
KW  - 4.2.4. Low-code / No-code Development
ER  - 

TY  - JOUR
TI  - ARTE: Automated generation of realistic test inputs for web APIs
AU  - C., Juan, Alonso
AU  - Alberto, Martin-Lopez
AU  - Sergio, Segura
AU  - Maria, Jose, Garcia
AU  - Antonio, Ruiz-Cortes
T2  - IEEE Transactions on Software Engineering
DA  - 2022///
PY  - 2022
VL  - 49
IS  - 1
SP  - 348
EP  - 363
ST  - ARTE
UR  - https://ieeexplore.ieee.org/abstract/document/9712422/
Y2  - 2025/04/05/
N1  - Citation Count: 43
KW  - 2. ML
KW  - 2.1. Traditional Machine Learning
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.2. Test Data Generation
KW  - 3.2.3. Test Datasets (HTML
KW  - Screenshots
KW  - UML)
ER  - 

TY  - JOUR
TI  - Artificial intelligence for software engineering: The journey so far and the road ahead
AU  - Iftekhar, Ahmed
AU  - A., Aleti
AU  - Haipeng, Cai
AU  - A., Chatzigeorgiou
AU  - Pinjia, He
AU  - Xing, Hu
AU  - Mauro, Pezz?
AU  - Denys, Poshyvanyk
AU  - Xin, Xia
T2  - ACM Transactions on Software Engineering and Methodology
AB  - Artificial intelligence and recent advances in deep learning architectures, including transformer networks and large language models, change the way people think and act to solve problems. Software engineering, as an increasingly complex process to design, develop, test, deploy, and maintain large-scale software systems for solving real-world challenges, is profoundly affected by many revolutionary artificial intelligence tools in general, and machine learning in particular. In this roadmap for artificial intelligence in software engineering, we highlight the recent deep impact of artificial intelligence on software engineering by discussing successful stories of applications of artificial intelligence to classic and new software development challenges. We identify the new challenges that the software engineering community has to address in the coming years to successfully apply artificial intelligence in software engineering, and we share our research roadmap towards the effective use of artificial intelligence in the software engineering profession, while still protecting fundamental human values. We spotlight three main areas that challenge the research in software engineering: the use of generative artificial intelligence and large language models for engineering large software systems, the need of large and unbiased datasets and benchmarks for training and evaluating deep learning and large language models for software engineering, and the need of a new code of digital ethics to apply artificial intelligence in software engineering.
DA  - 2025///
PY  - 2025
DO  - 10.1145/3719006
N1  - Citation Count: 1
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.2.2. Autoencoders / GANs
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.2. Development Process Automation
ER  - 

TY  - JOUR
TI  - Artificial intelligence in software development: a review of code generation, testing, maintenance and security
AU  - Sudhir, Ms Prajakta, Khade
AU  - U., Rajeshkumar, Sambhe
DA  - 2025///
PY  - 2025
ST  - Artificial intelligence in software development
UR  - https://ijcsrr.org/wp-content/uploads/2025/04/08-0804-2025.pdf
Y2  - 2025/04/30/
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 4. Software Engineering
KW  - 4.2. Development Process Automation
KW  - 4.2.1. CI/CD
KW  - 4.2.4. Low-code / No-code Development
ER  - 

TY  - CONF
TI  - Assured offline LLM-based software engineering
AU  - Nadia, Alshahwan
AU  - Mark, Harman
AU  - Inna, Harper
AU  - Alexandru, Marginean
AU  - Shubho, Sengupta
AU  - Eddy, Wang
T3  - Proceedings of the ACM/IEEE 2nd International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering
DA  - 2024///
PY  - 2024
SP  - 7
EP  - 12
UR  - https://dl.acm.org/doi/abs/10.1145/3643661.3643953
Y2  - 2025/04/30/
N1  - Citation Count: 17
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 4.2. Development Process Automation
KW  - 4.2.2. Debugging Support
KW  - 4.2.4. Low-code / No-code Development
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Augmenting automated game testing with deep reinforcement learning
AU  - Joakim, Bergdahl
AU  - Camilo, Gordillo
AU  - Konrad, Tollmar
AU  - Linus, Gisslйn
AB  - General game testing relies on the use of human play testers, play test scripting, and prior knowledge of areas of interest to produce relevant test data. Using deep reinforcement learning (DRL), we introduce a self-learning mechanism to the game testing framework. With DRL, the framework is capable of exploring and/or exploiting the game mechanics based on a user-defined, reinforcing reward signal. As a result, test coverage is increased and unintended game play mechanics, exploits and bugs are discovered in a multitude of game types. In this paper, we show that DRL can be used to increase test coverage, find exploits, test map difficulty, and to detect common problems that arise in the testing of first-person shooter (FPS) games.
DA  - 2021///
PY  - 2021
DO  - 10.48550/ARXIV.2103.15819
UR  - https://arxiv.org/abs/2103.15819
Y2  - 2025/04/05/
N1  - Citation Count: 102
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.5. Reinforcement Learning Strategies
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.6. Failure Detection
KW  - 3.5. Robustness / Non-functional Testing
ER  - 

TY  - CONF
TI  - Automated performance testing based on active deep learning
AU  - Ali, Sedaghatbaf
AU  - Helali, Mahshid, Moghadam
AU  - Mehrdad, Saadatmand
T3  - 2021 IEEE/ACM International Conference on Automation of Software Test (AST)
DA  - 2021///
PY  - 2021
DO  - 10.1109/AST52587.2021.00010
SP  - 11
EP  - 19
PB  - IEEE
SN  - 978-1-6654-3567-3
UR  - https://ieeexplore.ieee.org/document/9463020/
Y2  - 2025/02/05/
N1  - Citation Count: 8
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.2.1. CNN / RNN / LSTM
KW  - 2.4. Active Learning Strategies
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.5. Robustness / Non-functional Testing
ER  - 

TY  - CONF
TI  - Automated software testing using machine learning: a systematic mapping study
AU  - Abdellatif, Ahammad
AU  - Manal, El Bajta
AU  - Maryam, Radgui
T3  - 2024 10th International Conference on Optimization and Applications (ICOA)
DA  - 2024///
PY  - 2024
SP  - 1
EP  - 6
PB  - IEEE
ST  - Automated software testing using machine learning
UR  - https://ieeexplore.ieee.org/abstract/document/10754031/
Y2  - 2025/04/30/
N1  - Citation Count: 0
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.2.1. CNN / RNN / LSTM
KW  - 2.5. Reinforcement Learning Strategies
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
ER  - 

TY  - JOUR
TI  - Automated structural test case generation for human-computer interaction software based on large language model
AU  - Long, Kang
AU  - Jun, Ai
AU  - Minyan, Lu
T2  - International Conferences on Dependable Systems and Their Applications
AB  - As software systems expand in complexity, managing the vast and varied collection of test cases becomes increasingly difficult with traditional manual testing methods. This paper presents a new approach for automating the generation of structured test cases, named Test Element Extraction and Restructuring (TEER), which leverages the advanced natural language processing capabilities of large language models (LLMs). Specifically targeting human-computer interaction (HCI) software, TEER employs prompt tuning techniques to extract critical elements from natural language test cases and systematically reassemble them into structured formats. The study evaluates the effectiveness of TEER by applying it to common test cases from desktop HCI applications. The experimental results demonstrate that this method successfully produces structured test cases that meet predefined requirements.
DA  - 2024///
PY  - 2024
DO  - 10.1109/dsa63982.2024.00027
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.3. Test Classification
KW  - 3.1.5. Test Prioritization
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.3. Instructional / Role prompting
KW  - 5.4. Visual / GUI-based Prompting
ER  - 

TY  - JOUR
TI  - Automated test case output generation using Seq2Seq models
AU  - Edipcan, Ozer
AU  - A., M., Akcayol
T2  - Proceedings of the 2024 13th International Conference on Software and Information Engineering
DA  - 2024///
PY  - 2024
DO  - 10.1145/3708635.3708644
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.2.1. CNN / RNN / LSTM
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 4. Software Engineering
ER  - 

TY  - JOUR
TI  - Automated test creation using large language models: a practical application
AU  - S., Hadzhikoleva
AU  - Todor, Rachovski
AU  - Ivan, Ivanov
AU  - E., Hadzhikolev
AU  - G., Dimitrov
T2  - Applied Sciences
AB  - The article presents work on developing a software application for test creation using artificial intelligence and large language models. Its main goal is to optimize the educators’ work by automating the process of test generation and evaluation, with the tests being stored for subsequent analysis and use. The application can generate test questions based on specified criteria such as difficulty level, Bloom’s taxonomy level, question type, style and format, feedback inclusion, and more, thereby providing opportunities to enhance the adaptability and efficiency of the learning process. It is developed on the Google Firebase platform, utilizing the ChatGPT API, and also incorporates cloud computing to ensure scalability and data reliability.
DA  - 2024///
PY  - 2024
DO  - 10.3390/app14199125
N1  - Citation Count: 2
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.4. Visual / GUI-based Prompting
ER  - 

TY  - JOUR
TI  - Automated test data generation based on a genetic algorithm with maximum code coverage and population diversity
AU  - Tatiana, Avdeenko
AU  - Konstantin, Serdyukov
T2  - Applied Sciences
AB  - In the present paper, we investigate an approach to intelligent support of the software white-box testing process based on an evolutionary paradigm. As a part of this approach, we solve the urgent problem of automated generation of the optimal set of test data that provides maximum statement coverage of the code when it is used in the testing process. We propose the formulation of a fitness function containing two terms, and, accordingly, two versions for implementing genetic algorithms (GA). The first term of the fitness function is responsible for the complexity of the code statements executed on the path generated by the current individual test case (current set of statements). The second term formulates the maximum possible difference between the current set of statements and the set of statements covered by the remaining test cases in the population. Using only the first term does not make it possible to obtain 100 percent statement coverage by generated test cases in one population, and therefore implies repeated launch of the GA with changed weights of the code statements which requires recompiling the code under the test. By using both terms of the proposed fitness function, we obtain maximum statement coverage and population diversity in one launch of the GA. Optimal relation between the two terms of fitness function was obtained for two very different programs under testing.
DA  - 2021///
PY  - 2021
DO  - 10.3390/app11104673
VL  - 11
IS  - 10
SP  - 4673
LA  - en
SN  - 2076-3417
UR  - https://www.mdpi.com/2076-3417/11/10/4673
Y2  - 2025/02/05/
N1  - Citation Count: 7
KW  - 2. ML
KW  - 2.1. Traditional Machine Learning
KW  - 2.5. Reinforcement Learning Strategies
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.2. Test Data Generation
ER  - 

TY  - JOUR
TI  - Automated testing of the GUI of a real-life engineering software using large language models
AU  - Tim, Rosenbach
AU  - David, Heidrich
AU  - Alexander, Weinert
T2  - International Conference on Software Testing, Verification and Validation Workshops
AB  - One important step in software development is testing the finished product with actual users. These tests aim, among other goals, at determining unintuitive behavior of the software as it is presented to the end-user. Moreover, they aim to determine inconsistencies in the user-facing interface. They provide valuable feedback for the development of the software, but are time-intensive to conduct.In this work, we present GERALLT, a system that uses Large Language Models (LLMs) to perform exploratory tests of the Graphical User Interface (GUI) of a real-life engineering software. GERALLT automatically generates a list of potential unintuitive and inconsistent parts of the interface. We present the architecture of GERALLT and evaluate it on a real-world use case of the engineering software, which has been extensively tested by developers and users. Our results show that GERALLT is able to determine issues with the interface that support the software development team in future development of the software.
DA  - 2025///
PY  - 2025
DO  - 10.1109/icstw64639.2025.10962502
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.3. Functional Testing
KW  - 3.5. Robustness / Non-functional Testing
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.4. Visual / GUI-based Prompting
ER  - 

TY  - JOUR
TI  - Automated unit test improvement using large language models at meta
AU  - N., Alshahwan
AU  - Jubin, Chheda
AU  - Anastasia, Finogenova
AU  - Beliz, Gokkaya
AU  - Mark, Harman
AU  - Inna, Harper
AU  - Alexandru, Marginean
AU  - Shubho, Sengupta
AU  - Eddy, Wang
T2  - SIGSOFT FSE Companion
AB  - This paper describes Meta's TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests. TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination. We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75
DA  - 2024///
PY  - 2024
DO  - 10.48550/arxiv.2402.09171
N1  - Citation Count: 2
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.4. Test Repair
KW  - 3.1.8. Test Quality Analysis
KW  - 3.3. Functional Testing
KW  - 3.5. Robustness / Non-functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Automatic unit test code generation using large language models
AU  - Kutay, Akdeniz, Ц?al
AU  - Mehmet, Keskinцz
T2  - Signal Processing and Communications Applications Conference
AB  - This study aimed to automate the production of unit tests, a critical component of the software development process. By using pre-trained Large Language Models, manual effort and training costs were reduced, and test production capacity was increased. Instead of directly feeding the test functions obtained from the Java projects to be tested into the model, the project was analyzed to extract additional information. The data obtained from this analysis were used to create an effective prompt template. Furthermore, the sources of the problematic tests produced were identified, and this information was fed back into the model, enabling it to autonomously correct the errors. The results of the study showed that the model was able to generate tests covering
DA  - 2024///
PY  - 2024
DO  - 10.1109/siu61531.2024.10600772
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.4. Test Repair
KW  - 3.1.8. Test Quality Analysis
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.5. Self-improving Prompts
ER  - 

TY  - CONF
TI  - Automatic web testing using curiosity-driven reinforcement learning
AU  - Yan, Zheng
AU  - Yi, Liu
AU  - Xiaofei, Xie
AU  - Yepang, Liu
AU  - Lei, Ma
AU  - Jianye, Hao
AU  - Yang, Liu
T3  - 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
DA  - 2021///
PY  - 2021
DO  - 10.1109/ICSE43902.2021.00048
SP  - 423
EP  - 435
PB  - IEEE
SN  - 978-1-6654-0296-5
UR  - https://ieeexplore.ieee.org/document/9402046/
Y2  - 2025/02/05/
N1  - Citation Count: 49
KW  - 1. LLM
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 2. ML
KW  - 2.5. Reinforcement Learning Strategies
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.6. Failure Detection
KW  - 3.3. Functional Testing
ER  - 

TY  - JOUR
TI  - Automatically inspecting thousands of static bug warnings with large language model: How far are we?
AU  - Cheng, Wen
AU  - Yuandao, Cai
AU  - Bin, Zhang
AU  - Jie, Su
AU  - Zhiwu, Xu
AU  - Dugang, Liu
AU  - Shengchao, Qin
AU  - Zhong, Ming
AU  - Tian, Cong
T2  - ACM Transactions on Knowledge Discovery from Data
AB  - Static analysis tools for capturing bugs and vulnerabilities in software programs are widely employed in practice, as they have the unique advantages of high coverage and independence from the execution environment. However, existing tools for analyzing large codebases often produce a great deal of false warnings over genuine bug reports. As a result, developers are required to manually inspect and confirm each warning, a challenging, time-consuming, and automation-essential task. This article advocates a fast, general, and easily extensible approach called Llm4sa that automatically inspects a sheer volume of static warnings by harnessing (some of) the powers of Large Language Models (LLMs). Our key insight is that LLMs have advanced program understanding capabilities, enabling them to effectively act as human experts in conducting manual inspections on bug warnings with their relevant code snippets. In this spirit, we propose a static analysis to effectively extract the relevant code snippets via program dependence traversal guided by the bug warning reports themselves. Then, by formulating customized questions that are enriched with domain knowledge and representative cases to query LLMs, Llm4sa can remove a great deal of false warnings and facilitate bug discovery significantly. Our experiments demonstrate that Llm4sa is practical in automatically inspecting thousands of static warnings from Juliet benchmark programs and 11 real-world C/C++ projects, showcasing a high precision (81.13
DA  - 2024///
PY  - 2024
DO  - 10.1145/3653718
N1  - Citation Count: 10
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.6. Failure Detection
KW  - 3.1.8. Test Quality Analysis
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Automation of software test data generation using genetic algorithm and reinforcement learning
AU  - Mehdi, Esnaashari
AU  - Hossein, Amir, Damia
T2  - Expert Systems with Applications
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.eswa.2021.115446
VL  - 183
SP  - 115446
LA  - en
SN  - 09574174
UR  - https://linkinghub.elsevier.com/retrieve/pii/S0957417421008605
Y2  - 2025/02/05/
N1  - Citation Count: 59
KW  - 2. ML
KW  - 2.1. Traditional Machine Learning
KW  - 2.5. Reinforcement Learning Strategies
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.2. Test Data Generation
ER  - 

TY  - JOUR
TI  - Can llms see what I see? A study on five prompt engineering techniques for evaluating UX on a shopping site
AU  - Subin, Shin
AU  - Jeesun, Oh
AU  - Sangwon, Lee
T2  - CHI Extended Abstracts
DA  - 2025///
PY  - 2025
DO  - 10.1145/3706599.3720079
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.2. Test Data Generation
KW  - 3.2.3. Test Datasets (HTML
KW  - 3.3. Functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.3. Instructional / Role prompting
KW  - 5.4. Visual / GUI-based Prompting
KW  - Screenshots
KW  - UML)
ER  - 

TY  - JOUR
TI  - ChatUniTest: a ChatGPT-based automated unit test generation tool
AU  - Zhuokui, Xie
AU  - Yinghao, Chen
AU  - Chen, Zhi
AU  - Shuiguang, Deng
AU  - Jianwei, Yin
T2  - arXiv e-prints
DA  - 2023///
PY  - 2023
SP  - arXiv
EP  - 2305
ST  - ChatUniTest
N1  - Citation Count: 105
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.9. Automated testing tools
KW  - 3.3. Functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.5. Self-improving Prompts
ER  - 

TY  - JOUR
TI  - ChatUniTest: a framework for LLM-based test generation
AU  - Yinghao, Chen
AU  - Zehao, Hu
AU  - Chen, Zhi
AU  - Junxiao, Han
AU  - Shuiguang, Deng
AU  - Jianwei, Yin
T2  - SIGSOFT FSE Companion
AB  - Unit testing is an essential yet frequently arduous task. Various automated unit test generation tools have been introduced to mitigate this challenge. Notably, methods based on large language models (LLMs) have garnered considerable attention and exhibited promising results in recent years. Nevertheless, LLM-based tools encounter limitations in generating accurate unit tests. This paper presents ChatUniTest, an LLM-based automated unit test generation framework. ChatUniTest incorporates an adaptive focal context mechanism to encompass valuable context in prompts and adheres to a generation-validation-repair mechanism to rectify errors in generated unit tests. Subsequently, we have developed ChatUniTest Core, a common library that implements core workflow, complemented by the ChatUniTest Toolchain, a suite of seamlessly integrated tools enhancing the capabilities of ChatUniTest. Our effectiveness evaluation reveals that ChatUniTest outperforms TestSpark and EvoSuite in half of the evaluated projects, achieving the highest overall line coverage. Furthermore, insights from our user study affirm that ChatUniTest delivers substantial value to various stakeholders in the software testing domain. ChatUniTest is available at https://github.com/ZJU-ACES-ISE/ChatUniTest, and the demo video is available at https://www.youtube.com/watch?v=GmfxQUqm2ZQ.
DA  - 2023///
PY  - 2023
DO  - 10.1145/3663529.3663801
N1  - Citation Count: 9
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.4. Integration Testing
ER  - 

TY  - JOUR
TI  - ClarifyGPT: a framework for enhancing LLM-based code generation via requirements clarification
AU  - Fangwen, Mu
AU  - Lin, Shi
AU  - Song, Wang
AU  - Zhuohao, Yu
AU  - Binquan, Zhang
AU  - ChenXue, Wang
AU  - Shichao, Liu
AU  - Qing, Wang
T2  - Proc. ACM Softw. Eng.
AB  - Large Language Models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in automatically generating code from provided natural language requirements. However, in real-world practice, it is inevitable that the requirements written by users might be ambiguous or insufficient. Current LLMs will directly generate programs according to those unclear requirements, regardless of interactive clarification, which will likely deviate from the original user intents. To bridge that gap, we introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. Specifically, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we invite ten participants to use ClarifyGPT for code generation on two benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96
DA  - 2024///
PY  - 2024
DO  - 10.1145/3660810
N1  - Citation Count: 3
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 4.1.5. Requirement Classification
KW  - 4.1.6. Requirement Generation
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - CodaMosa: Escaping coverage plateaus in test generation with pre-trained large language models
AU  - Carole, Lemieux
AU  - Priya, Jeevana, Inala
AU  - K., Shuvendu, Lahiri
AU  - Siddhartha, Sen
T2  - International Conference on Software Engineering
AB  - Search-based software testing (SBST) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST's performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, SBST can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models (LLMs) of code, such as OpenAI's Codex, can be used to help SBST's exploration. Our proposed algorithm, CodaMosa, conducts SBST until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help SBST redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, CodaMosa achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to SBST and LLM-only baselines.
DA  - 2023///
PY  - 2023
DO  - 10.1109/icse48619.2023.00085
N1  - Citation Count: 112
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.2. Test Data Generation
KW  - 3.2.1. Test Oracle
ER  - 

TY  - JOUR
TI  - Code-aware prompting: a study of coverage guided test generation in regression setting using LLM
AU  - Gabriel, Ryan
AU  - Siddhartha, Jain
AU  - Mingyue, Shang
AU  - Shiqi, Wang
AU  - Xiaofei, Ma
AU  - M., Ramanathan
AU  - Baishakhi, Ray
T2  - Proc. ACM Softw. Eng.
AB  - Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent work using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated testsuites still suffer from low coverage. In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt’s approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a specific prompt aligned with the execution paths of the method under test, and exposing relevant type and dependency focal context to the model. Our approach enables pretrained LLMs to generate more complete test cases without any additional training. We implement SymPrompt using the TreeSitter parsing framework and evaluate on a benchmark challenging methods from open source Python projects. SymPrompt enhances correct test generations by a factor of 5 and bolsters relative coverage by 26
DA  - 2024///
PY  - 2024
DO  - 10.48550/arxiv.2402.00097
N1  - Citation Count: 2
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.5. Robustness / Non-functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation
AU  - Yue, Wang
AU  - Weishi, Wang
AU  - Shafiq, Joty
AU  - H., Steven C., Hoi
AB  - Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https: //github.com/salesforce/CodeT5 .
DA  - 2021///
PY  - 2021
DO  - 10.48550/arXiv.2109.00859
ST  - CodeT5
UR  - http://arxiv.org/abs/2109.00859
Y2  - 2025/02/05/
N1  - Citation Count: 15
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 4.1.4. Source Code Analysis
ER  - 

TY  - CONF
TI  - Combinatorial test design model creation using large language models
AU  - Ann, Deborah, Furman
AU  - Eitan, Farchi
AU  - Edward, Michael, Gildein
AU  - CM, Andrew, Hicks
AU  - Thomas, Ryan, Rawlins
T3  - 2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
DA  - 2025///
PY  - 2025
SP  - 314
EP  - 323
PB  - IEEE
UR  - https://ieeexplore.ieee.org/abstract/document/10962528/
Y2  - 2025/04/30/
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Combining low-code development with ChatGPT to novel no-code approaches: A focus-group study
AU  - Josй, Martins
AU  - Frederico, Branco
AU  - Henrique, Mamede
T2  - Intelligent Systems with Applications
DA  - 2023///
PY  - 2023
DO  - 10.1016/j.iswa.2023.200289
VL  - 20
SP  - 200289
LA  - en
SN  - 26673053
ST  - Combining low-code development with ChatGPT to novel no-code approaches
UR  - https://linkinghub.elsevier.com/retrieve/pii/S266730532300114X
N1  - Citation Count: 7
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 4.2. Development Process Automation
KW  - 4.2.2. Debugging Support
KW  - 4.2.4. Low-code / No-code Development
ER  - 

TY  - CONF
TI  - Comparing the adaptability of a genetic algorithm and an LLM-based framework for automated software test data generation: In the context of web applications
AU  - Sashini, Wanigasekara
AU  - Dinesh, Asanka
AU  - Chathura, Rajapakse
AU  - Dilani, Wickramaarachchi
AU  - Abhiru, Wijesinghe
T3  - 2024 IEEE 3rd International Conference on Data, Decision and Systems (ICDDS)
AB  - In the fast-paced world of software development, ensuring software quality is paramount. Software Quality Assurance (SQA) plays a vital role, primarily through testing, which can be carried out manually or automatically. Yet, creating comprehensive test data (TD) for web applications can be a formidable task. Manual test data generation (TDG) is time-consuming and error prone. Automation of TDG has become increasingly important in the realm of software quality assurance as it enables efficient and effective testing of software systems. The need for an appropriate framework for automated TDG is critical to achieve comprehensive and reliable test coverage. Automated TDG offers significant advantages, including time and resource savings, improved test coverage, and seamless integration into the software development process. The core aim of this research is to bridge the gap between manual and existing automated methods, resulting in time and cost savings, heightened testing efficiency, and elevated software quality. Research objectives encompass comparing the adaptability of an AGA based automated TDG model and a LLM based automated TDG model to a web application. The results from the LLM model for triangle classification program was found to be potentially acceptable and accurate than the AGA model's results. This research discusses the challenges encountered when implementing and using the AGA-based framework in the web application context and how an LLM model could overcome the challenges. The study highlights the benefits of using the LLM approach, demonstrating its relevance and accuracy in generating test data compared to the Genetic Algorithm-based model. The practical implications for software quality assurance practices are discussed, emphasizing the enhanced efficiency and effectiveness of the LLM model in improving software quality.
DA  - 2024///
PY  - 2024
DO  - 10.1109/icdds62937.2024.10910287
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 2. ML
KW  - 2.1. Traditional Machine Learning
KW  - 2.1.2. Feature-based Models
KW  - 2.2. Deep Learning
KW  - 2.2.3. Transfer / Ensemble Learning
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.2. Test Data Generation
KW  - 3.2.3. Test Datasets (HTML
KW  - 3.5. Robustness / Non-functional Testing
KW  - Screenshots
KW  - UML)
ER  - 

TY  - JOUR
TI  - Comprehensive evaluation and insights into the use of large language models in the automation of behavior-driven development acceptance test formulation
AU  - Shanthi, Karpurapu
AU  - Sravanthy, Myneni
AU  - Unnati, Nettur
AU  - Sagar, Likhit, Gajja
AU  - Dave, Burke
AU  - Tom, Stiehm
AU  - Jeffery, Payne
T2  - IEEE access : practical innovations, open solutions
AB  - Behavior-driven development (BDD) is an Agile testing methodology fostering collaboration among developers, QA analysts, and stakeholders. In this manuscript, we propose a novel approach to enhance BDD practices using large language models (LLMs) to automate acceptance test generation. Our study uses zero and few-shot prompts to evaluate LLMs such as GPT-3.5, GPT-4, Llama-2-13B, and PaLM-2. The paper presents a detailed methodology that includes the dataset, prompt techniques, LLMs, and the evaluation process. The results demonstrate that GPT-3.5 and GPT-4 generate error-free BDD acceptance tests with better performance. The few-shot prompt technique highlights its ability to provide higher accuracy by incorporating examples for in-context learning. Furthermore, the study examines syntax errors, validation accuracy, and comparative analysis of LLMs, revealing their effectiveness in enhancing BDD practices. However, our study acknowledges that there are limitations to the proposed approach. We emphasize that this approach can support collaborative BDD processes and create opportunities for future research into automated BDD acceptance test generation using LLMs.
DA  - 2024///
PY  - 2024
DO  - 10.1109/access.2024.3391815
N1  - Citation Count: 3
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.3. Functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - CONF
TI  - Context-tailored workload model generation for continuous representative load testing
AU  - Henning, Schulz
AU  - Duрan, Okanoviг
AU  - Andrй, Van Hoorn
AU  - Petr, T?ma
T3  - Proceedings of the ACM/SPEC International Conference on Performance Engineering
DA  - 2021///
PY  - 2021
DO  - 10.1145/3427921.3450240
SP  - 21
EP  - 32
LA  - en
PB  - ACM
SN  - 978-1-4503-8194-9
UR  - https://dl.acm.org/doi/10.1145/3427921.3450240
Y2  - 2025/02/05/
N1  - Citation Count: 7
KW  - 2. ML
KW  - 2.1. Traditional Machine Learning
KW  - 2.1.1. SVM / Decision Trees / Random Forest
KW  - 2.2. Deep Learning
KW  - 2.2.1. CNN / RNN / LSTM
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.5. Robustness / Non-functional Testing
ER  - 

TY  - JOUR
TI  - Cypress copilot: Development of an AI assistant for boosting productivity and transforming web application testing
AU  - Babu, Suresh, Nettur
AU  - Shanthi, Karpurapu
AU  - Unnati, Nettur
AU  - Sagar, Likhit, Gajja
T2  - IEEE access : practical innovations, open solutions
AB  - In today’s fast-paced software development environment, Agile methodologies demand rapid delivery and continuous improvement, making automated testing essential for maintaining quality and accelerating feedback loops. Our study addresses the challenges of developing and maintaining automation code for web-based application testing. In this paper, we propose a novel approach that leverages large language models (LLMs) and a novel prompt technique, few-shot chain, to automate code generation for web application testing. We chose the Behavior-Driven Development (BDD) methodology owing to its advantages and selected the Cypress tool for automating web application testing, as it is one of the most popular and rapidly growing frameworks in this domain. We comprehensively evaluated various OpenAI models, including GPT-4-Turbo, GPT-4o, and GitHub Copilot, using zero-shot and few-shot chain prompt techniques. Furthermore, we extensively validated with a vast set of test cases to identify the optimal approach. Our results indicate that the Cypress automation code generated by GPT-4o using a few-shot chained prompt approach excels in generating complete code for each test case, with fewer empty methods and improved syntactical accuracy and maintainability. Based on these findings, we developed a novel open-source Visual Studio Code (IDE) extension, “Cypress Copilot” utilizing GPT-4o and a few-shot chain prompt technique, which has shown promising results. Finally, we validate the Cypress Copilot tool by generating automation code for end-to-end web tests, demonstrating its effectiveness in testing various web applications and its ability to streamline development processes. More importantly, we are releasing this tool to the open-source community, as it has the potential to be a promising partner in enhancing productivity in web application automation testing.
DA  - 2025///
PY  - 2025
DO  - 10.1109/access.2024.3521407
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.3. Functional Testing
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.4. Visual / GUI-based Prompting
ER  - 

TY  - CONF
TI  - Deep learning-based prediction of test input validity for restful apis
AU  - Giuliano, A., Mirabella
AU  - Alberto, Martin-Lopez
AU  - Sergio, Segura
AU  - Luis, Valencia-Cabrera
AU  - Antonio, Ruiz-Cortйs
T3  - 2021 IEEE/ACM third international workshop on deep learning for testing and testing for deep learning (deepTest)
DA  - 2021///
PY  - 2021
SP  - 9
EP  - 16
PB  - IEEE
UR  - https://ieeexplore.ieee.org/abstract/document/9476896/
Y2  - 2025/05/05/
N1  - Citation Count: 36
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.2.1. CNN / RNN / LSTM
KW  - 2.5. Reinforcement Learning Strategies
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.5. Test Prioritization
KW  - 3.1.6. Failure Detection
KW  - 3.1.8. Test Quality Analysis
KW  - 3.2. Test Data Generation
KW  - 3.2.3. Test Datasets (HTML
KW  - 3.5. Robustness / Non-functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - Screenshots
KW  - UML)
ER  - 

TY  - JOUR
TI  - DeepREST: Automated test case generation for REST apis exploiting deep reinforcement learning
AU  - Davide, Corradini
AU  - Zeno, Montolli
AU  - Michele, Pasqua
AU  - Mariano, Ceccato
T2  - International Conference on Automated Software Engineering
AB  - Automatically crafting test scenarios for REST APIs helps deliver more reliable and trustworthy web-oriented systems. However, current black-box testing approaches rely heavily on the information available in the API’s formal documentation, i.e., the Open API Specification (OAS for short). While useful, the OAS mostly covers syntactic aspects of the API (e.g., producer-consumer relations between operations, input value properties, and additional constraints in natural language), and it lacks a deeper understanding of the API business logic. Missing semantics include implicit ordering (logic dependency) between operations and implicit input-value constraints. These limitations hinder the ability of black-box testing tools to generate truly effective test cases automatically.This paper introduces DeepREST, a novel black-box approach for automatically testing REST APIs. It leverages deep reinforcement learning to uncover implicit API constraints, that is, constraints hidden from API documentation. Curiosity-driven learning guides an agent in the exploration of the API and learns an effective order to test its operations. This helps identify which operations to test first to take the API in a testable state and avoid failing API interactions later. At the same time, experience gained on successful API interactions is leveraged to drive accurate input data generation (i.e., what parameters to use and how to pick their values). Additionally, DeepREST alternates exploration with exploitation by mutating successful API interactions to improve test coverage and collect further experience.Our empirical validation suggests that the proposed approach is very effective in achieving high test coverage and fault detection and superior to a state-of-the-art baseline.
DA  - 2024///
PY  - 2024
DO  - 10.48550/arxiv.2408.08594
N1  - Citation Count: 0
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.2.2. Autoencoders / GANs
KW  - 2.5. Reinforcement Learning Strategies
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.5. Robustness / Non-functional Testing
ER  - 

TY  - JOUR
TI  - Domain adaptation for code model-based unit test case generation
AU  - Jiho, Shin
AU  - Sepehr, Hashtroudi
AU  - H., Hemmati
AU  - Song, Wang
T2  - International Symposium on Software Testing and Analysis
AB  - Recently, deep learning-based test case generation approaches have been proposed to automate the generation of unit test cases. In this study, we leverage Transformer-based code models to generate unit tests with the help of Domain Adaptation (DA) at a project level. Specifically, we use CodeT5, a relatively small language model trained on source code data, and fine-tune it on the test generation task. Then, we apply domain adaptation to each target project data to learn project-specific knowledge (project-level DA). We use the Methods2test dataset to fine-tune CodeT5 for the test generation task and the Defects4j dataset for project-level domain adaptation and evaluation. We compare our approach with (a) CodeT5 fine-tuned on the test generation without DA, (b) the A3Test tool, and (c) GPT-4 on five projects from the Defects4j dataset. The results show that tests generated using DA can increase the line coverage by 18.62
DA  - 2023///
PY  - 2023
DO  - 10.1145/3650212.3680354
N1  - Citation Count: 1
KW  - 1. LLM
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.3. Functional Testing
ER  - 

TY  - JOUR
TI  - DroidbotX: Test case generation tool for android applications using q-learning
AU  - N., Husam, Yasin
AU  - Ab, Siti Hafizah, Hamid
AU  - Jamilah, Raja, Raja Yusof
T2  - Symmetry
AB  - Android applications provide benefits to mobile phone users by offering operative functionalities and interactive user interfaces. However, application crashes give users an unsatisfactory experience, and negatively impact the application’s overall rating. Android application crashes can be avoided through intensive and extensive testing. In the related literature, the graphical user interface (GUI) test generation tools focus on generating tests and exploring application functions using different approaches. Such tools must choose not only which user interface element to interact with, but also which type of action to be performed, in order to increase the percentage of code coverage and to detect faults with a limited time budget. However, a common limitation in the tools is the low code coverage because of their inability to find the right combination of actions that can drive the application into new and important states. A Q-Learning-based test coverage approach developed in DroidbotX was proposed to generate GUI test cases for Android applications to maximize instruction coverage, method coverage, and activity coverage. The overall performance of the proposed solution was compared to five state-of-the-art test generation tools on 30 Android applications. The DroidbotX test coverage approach achieved 51.5
DA  - 2021///
PY  - 2021
DO  - 10.3390/sym13020310
VL  - 13
IS  - 2
SP  - 310
LA  - en
SN  - 2073-8994
ST  - DroidbotX
UR  - https://www.mdpi.com/2073-8994/13/2/310
Y2  - 2025/02/05/
N1  - Citation Count: 20
KW  - 1. LLM
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 2. ML
KW  - 2.5. Reinforcement Learning Strategies
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.3. Functional Testing
ER  - 

TY  - JOUR
TI  - Effective test generation using pre-trained large language models and mutation testing
AU  - Moradi, Arghavan, Dakhel
AU  - Amin, Nikanjam
AU  - Vahid, Majdinasab
AU  - Foutse, Khomh
AU  - C., Michel, Desmarais
T2  - Information and Software Technology
AB  - One of the critical phases in software development is software testing. Testing helps with identifying potential bugs and reducing maintenance costs. The goal of automated test generation tools is to ease the development of tests by suggesting efficient bug-revealing tests. Recently, researchers have leveraged Large Language Models (LLMs) of code to generate unit tests. While the code coverage of generated tests was usually assessed, the literature has acknowledged that the coverage is weakly correlated with the efficiency of tests in bug detection. To improve over this limitation, in this paper, we introduce MuTAP for improving the effectiveness of test cases generated by LLMs in terms of revealing bugs by leveraging mutation testing. Our goal is achieved by augmenting prompts with surviving mutants, as those mutants highlight the limitations of test cases in detecting bugs. MuTAP is capable of generating effective test cases in the absence of natural language descriptions of the Program Under Test (PUTs). We employ different LLMs within MuTAP and evaluate their performance on different benchmarks. Our results show that our proposed method is able to detect up to 28
DA  - 2023///
PY  - 2023
DO  - 10.48550/arxiv.2308.16557
N1  - Citation Count: 1
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.6. Failure Detection
KW  - 3.5. Robustness / Non-functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Enabling cost-effective UI automation testing with retrieval-based llms: a case study in WeChat
AU  - Sidong, Feng
AU  - Haochuan, Lu
AU  - Jianqin, Jiang
AU  - Ting, Xiong
AU  - Likun, Huang
AU  - Yinglin, Liang
AU  - Xiaoqin, Li
AU  - Yuetang, Deng
AU  - A., Aleti
T2  - International Conference on Automated Software Engineering
DA  - 2024///
PY  - 2024
DO  - 10.48550/arxiv.2409.07829
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.6. Failure Detection
KW  - 3.3. Functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.4. Visual / GUI-based Prompting
ER  - 

TY  - JOUR
TI  - Enhancing exploratory testing by large language model and knowledge graph
AU  - Yanqi, Su
AU  - Dianshu, Liao
AU  - Zhenchang, Xing
AU  - Qing, Huang
AU  - Mulong, Xie
AU  - Qinghua, Lu
AU  - Xiwei, Xu
T2  - International Conference on Software Engineering
AB  - Exploratory testing leverages the tester's knowledge and creativity to design test cases for effectively uncovering system-level bugs from the end user's perspective. Researchers have worked on test scenario generation to support exploratory testing based on a system knowledge graph, enriched with scenario and oracle knowledge from bug reports. Nevertheless, the adoption of this approach is hindered by difficulties in handling bug reports of inconsistent quality and varied expression styles, along with the infeasibility of the generated test scenarios. To overcome these limitations, we utilize the superior natural language understanding (NLU) capabilities of Large Language Models (LLMs) to construct a System KG of User Tasks and Failures (SysKG-UTF). Leveraging the system and bug knowledge from the KG, along with the logical reasoning capabilities of LLMs, we generate test scenarios with high feasibility and coherence. Particularly, we design chain-of-thought (CoT) reasoning to extract human-like knowledge and logical reasoning from LLMs, simulating a developer's process of validating test scenario feasibility. Our evaluation shows that our approach significantly enhances the KG construction, particularly for bug reports with low quality. Furthermore, our approach generates test scenarios with high feasibility and coherence. The user study further proves the effectiveness of our generated test scenarios in supporting exploratory testing. Specifically, 8 participants find 36 bugs from 8 seed bugs in two hours using our test scenarios, a significant improvement over the 21 bugs found by the state-of-the-art baseline.
DA  - 2024///
PY  - 2024
DO  - 10.1145/3597503.3639157
N1  - Citation Count: 2
KW  - 1. LLM
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 2. ML
KW  - 2.1. Traditional Machine Learning
KW  - 2.1.2. Feature-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.2. Test Data Generation
KW  - 3.2.2. Test Documentation
KW  - 3.3. Functional Testing
KW  - 3.5. Robustness / Non-functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.2. Few-shot / Zero-shot
KW  - 5.4. Visual / GUI-based Prompting
ER  - 

TY  - JOUR
TI  - Erratum: Leveraging Flexible Tree Matching to repair broken locators in web automation scripts
AU  - Sacha, Brisset
AU  - Romain, Rouvoy
AU  - Lionel, Seinturier
AU  - Renaud, Pawlak
T2  - Information and Software Technology
DA  - 2022///
PY  - 2022
DO  - 10.1016/j.infsof.2021.106754
VL  - 144
SP  - 106754
LA  - en
SN  - 9505849.0
ST  - Erratum
UR  - https://linkinghub.elsevier.com/retrieve/pii/S0950584921002020
Y2  - 2025/04/05/
N1  - Citation Count: 12
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.4. Test Repair
KW  - 3.1.7. Self-healing Testing
ER  - 

TY  - JOUR
TI  - Evaluating and improving ChatGPT for unit test generation
AU  - Zhiqiang, Yuan
AU  - Mingwei, Liu
AU  - Shiji, Ding
AU  - Kaixin, Wang
AU  - Yixuan, Chen
AU  - Xin, Peng
AU  - Yiling, Lou
T2  - Proc. ACM Softw. Eng.
AB  - Unit testing plays an essential role in detecting bugs in functionally-discrete program units (e.g., methods). Manually writing high-quality unit tests is time-consuming and laborious. Although the traditional techniques are able to generate tests with reasonable coverage, they are shown to exhibit low readability and still cannot be directly adopted by developers in practice. Recent work has shown the large potential of large language models (LLMs) in unit test generation. By being pre-trained on a massive developer-written code corpus, the models are capable of generating more human-like and meaningful test code. In this work, we perform the first empirical study to evaluate the capability of ChatGPT (i.e., one of the most representative LLMs with outstanding performance in code generation and comprehension) in unit test generation. In particular, we conduct both a quantitative analysis and a user study to systematically investigate the quality of its generated tests in terms of correctness, sufficiency, readability, and usability. We find that the tests generated by ChatGPT still suffer from correctness issues, including diverse compilation errors and execution failures (mostly caused by incorrect assertions); but the passing tests generated by ChatGPT almost resemble manually-written tests by achieving comparable coverage, readability, and even sometimes developers' preference. Our findings indicate that generating unit tests with ChatGPT could be very promising if the correctness of its generated tests could be further improved. Inspired by our findings above, we further propose ChatTester, a novel ChatGPT-based unit test generation approach, which leverages ChatGPT itself to improve the quality of its generated tests. ChatTester incorporates an initial test generator and an iterative test refiner. Our evaluation demonstrates the effectiveness of ChatTester by generating 34.3
DA  - 2024///
PY  - 2024
DO  - 10.1145/3660783
N1  - Citation Count: 12
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.4. Test Repair
KW  - 3.1.8. Test Quality Analysis
ER  - 

TY  - JOUR
TI  - Evaluating large language models for software testing
AU  - Yihao, Li
AU  - Pan, Liu
AU  - Haiyang, Wang
AU  - Jie, Chu
AU  - E., W., Wong
DA  - 2024///
PY  - 2024
DO  - 10.1016/j.csi.2024.103942
N1  - Citation Count: 1
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.6. Failure Detection
KW  - 3.1.8. Test Quality Analysis
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.3. Instructional / Role prompting
ER  - 

TY  - JOUR
TI  - Evaluating large language models using arabic prompts to generate python codes
AU  - N., Al-khafaji
AU  - Khalaf, Basit, Majeed
T2  - 2024 4th International Conference on Emerging Smart Technologies and Applications (eSmarTA)
AB  - Currently, the popularity of large language models (LLMs) for instance, ChatGPT from OpenAI and Gemini from Google is increasing greatly in our lives, due to their unparalleled performance in various applications. These models play a vital role in both academic and industrial fields. With this popularity, evaluating these models has become extremely important, especially when using the Arabic language. Despite the increasing popularity and performance of AI, there have been no empirical studies evaluating the use of LLMs for Arabic prompts in the field of code generation. However, the code generation in LLM can be strongly influenced by the choice of prompt. Evaluating the LLMs by Arabic prompts helps us better understand the strengths and weaknesses of these models. Therefore, we highlighted the evaluation of the most popular LLM programs (Chatgpt-3.5, ChatGPT-4 and Gemini) when generating Python codes based on Arabic prompts. In this study we employed CodeBLUE score and Flake8 as a metric to evaluate the LLMs capabilities for code generation via Arabic prompts. Our results indicate significant differences in performance across different LLMs and prompts levels. This study lays the foundation for further research into LLM capabilities based on Arabic prompts and suggests practical implications for using LLM in automated code generation and test-driven development tasks.
DA  - 2024///
PY  - 2024
DO  - 10.1109/esmarta62850.2024.10638877
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 4.2. Development Process Automation
KW  - 4.2.3. Test-Driven Development
KW  - 4.2.4. Low-code / No-code Development
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Evaluating the effectiveness of large language models in automated unit test generation
AU  - Tharindu, Godage
AU  - Sivaraj, Nimishan
AU  - S., Vasanthapriyan
AU  - Palanisamy, Vigneshwaran
AU  - Charles, Joseph
AU  - S., Thuseethan
T2  - 2025 5th International Conference on Advanced Research in Computing (ICARC)
AB  - The increasing use of Artificial Intelligence (AI) in software development underscores the need to select suitable Large Language Models (LLMs) for automating software unit test generation. No prior work has been conducted to evaluate the performance of LLM in this domain. To address this gap, this study evaluates the effectiveness of four prominent LLMs—GPT-4, Claude 3.5, Command-R-08-2024 and Llama 3.1—in generating unit test cases. This study particularly aims to evaluate the performance of these models in real-world testing scenarios. Hence, 106 test cases from 23 test suites based on interviews with software experts and QA engineers are used to ensure relevance and comprehensiveness. These test cases are analyzed using JavaScript Engines Specification Tester (JEST) for code coverage and Stryker for mutation testing while adopting both quantitative and qualitative analysis. The findings reveal that Claude 3.5 consistently outperforms the other models against test success rate, statement coverage, and mutation score with the achieved accuracy of 93.33
DA  - 2025///
PY  - 2025
DO  - 10.1109/icarc64760.2025.10962997
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.5. Robustness / Non-functional Testing
ER  - 

TY  - JOUR
TI  - Evaluation of large language models for unit test generation
AU  - Metin, Konuk
AU  - Cem, Ba?lum
AU  - U?ur, Yayan
T2  - 2024 Innovations in Intelligent Systems and Applications Conference (ASYU)
AB  - In recent years, Artificial Intelligence (AI) has significantly transformed various industries, especially software development, through automation and enhanced decision-making processes. Traditional software testing, often manual and error-prone, cannot keep up with rapid development cycles and complex systems, leading to extended development times, higher costs, and undetected bugs. This study develops an AI-based platform using OpenAI models to generate and execute unit tests across multiple programming languages. By leveraging Large Language Models (LLMs) like GPT, we automate unit test creation, demonstrating proficiency in understanding and generating natural language to interpret code. Our web-based system architecture ensures efficient test generation and execution, significantly reducing manual effort and mitigating human error, thus revolutionizing software testing. Furthermore, we introduce unique evaluation metrics such as “Is Executable” and “Assertion Count” to assess the performance and effectiveness of the generated unit tests, providing a comprehensive measure of the models' capabilities.
DA  - 2024///
PY  - 2024
DO  - 10.1109/asyu62119.2024.10756954
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.3. Functional Testing
KW  - 3.4. Integration Testing
ER  - 

TY  - JOUR
TI  - From code generation to software testing: AI copilot with context-based RAG
AU  - Yuchen, Wang
AU  - Shangxin, Guo
AU  - Wei, Chee, Tan
T2  - IEEE Software
AB  - The rapid pace of large-scale software development places increasing demands on traditional testing methodologies, often leading to bottlenecks in efficiency, accuracy, and coverage. We propose a novel perspective on software testing by positing bug detection and coding with fewer bugs as two interconnected problems that share a common goal, which is reducing bugs with limited resources. We extend our previous work on AI-assisted programming, which supports code auto-completion and chatbot-powered Q&A, to the realm of software testing. We introduce Copilot for Testing, an automated testing system that synchronizes bug detection with codebase updates, leveraging context-based Retrieval Augmented Generation (RAG) to enhance the capabilities of large language models (LLMs). Our evaluation demonstrates a 31.2
DA  - 2025///
PY  - 2025
DO  - 10.1109/ms.2025.3549628
N1  - Citation Count: 0
KW  - 2. ML
KW  - 2.1. Traditional Machine Learning
KW  - 2.1.2. Feature-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.6. Failure Detection
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 4.1.4. Source Code Analysis
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.4. Visual / GUI-based Prompting
ER  - 

TY  - CONF
TI  - Generating software tests for mobile applications using fine-tuned large language models
AU  - Jacob, Hoffmann
AU  - Demian, Frister
T3  - Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)
DA  - 2024///
PY  - 2024
SP  - 76
EP  - 77
UR  - https://dl.acm.org/doi/abs/10.1145/3644032.3644454
Y2  - 2025/04/30/
N1  - Citation Count: 6
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.3. Functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Generation of regression tests from logs with clustering guided by usage patterns
AU  - Frйdйric, Tamagnan
AU  - Alexandre, Vernotte
AU  - F., Bouquet
AU  - B., Legeard
T2  - Software testing, verification & reliability
AB  - Clustering is increasingly being used to select the appropriate test suites. In this paper, we apply this approach to regression testing. Regression testing is the practice of verifying the robustness and reliability of software by retesting after changes have been made. Creating and maintaining functional regression tests is a laborious and costly activity. To be effective, these tests must represent the actual user journeys of the application. In addition, an optimal number of test cases is critical for the rapid execution of the regression test suite to stay within the time and computational resource budget as it is re?run at each major iteration of the software development. Therefore, the selection and maintenance of functional regression tests based on the analysis of application logs has gained popularity in recent years. This paper presents a novel approach to improve regression testing by automating the creation of test suites using user traces fed into clustering pipelines. Our methodology introduces a new metric based on pattern mining to quantify the statistical coverage of prevalent user paths. This metric helps to determine the optimal number of clusters within a clustering pipeline, thus addressing the challenge of suboptimal test suite sizes. Additionally, we introduce two criteria, to systematically evaluate and rank clustering pipelines. Experimentation involving 33 variations of clustering pipelines across four datasets demonstrates the potential effectiveness of our automated approach compared with manually crafted test suites. (All the experiments and data on Scanner, Spree and Booked Scheduler are available at https://github.com/frederictamagnan/STVR2024.) Then, we analyse the semantics of the clusters based on their principal composing patterns.
DA  - 2024///
PY  - 2024
DO  - 10.1002/stvr.1900
N1  - Citation Count: 0
KW  - 2. ML
KW  - 2.1. Traditional Machine Learning
KW  - 2.1.1. SVM / Decision Trees / Random Forest
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.2. Test Optimization
KW  - 3.1.3. Test Classification
KW  - 3.1.5. Test Prioritization
KW  - 3.1.8. Test Quality Analysis
KW  - 3.5. Robustness / Non-functional Testing
ER  - 

TY  - JOUR
TI  - GUI-based software testing: An automated approach using GPT-4 and selenium WebDriver
AU  - E., Daniel, Zimmermann
AU  - Anne, Koziolek
T2  - 2023 38th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW)
AB  - This paper presents a novel method for GUI testing in web applications that largely automates the process by integrating the advanced language model GPT-4 with Selenium, a popular web application testing framework. Unlike traditional deep learning approaches, which require extensive training data, GPT-4 is pre-trained on a large corpus, giving it significant generalisation and inference capabilities. These capabilities allow testing without the need for recorded data from human testers, significantly reducing the time and effort required for the testing process. We also compare the efficiency of our integrated GPT-4 approach with monkey testing, a widely used technique for automated GUI testing where user input is randomly generated. To evaluate our approach, we implemented a web calculator with an integrated code coverage system. The results show that our integrated GPT-4 approach provides significantly better branch coverage compared to monkey testing. These results highlight the significant potential of integrating specific AI models such as GPT-4 and automated testing tools to improve the accuracy and efficiency of GUI testing in web applications.
DA  - 2023///
PY  - 2023
DO  - 10.1109/asew60602.2023.00028
N1  - Citation Count: 7
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.3. Functional Testing
KW  - 4. Software Engineering
ER  - 

TY  - JOUR
TI  - Harnessing large language models for automated software testing: a leap towards scalable test case generation
AU  - Shaheer, Rehan
AU  - Baidaa, Al-Bander
AU  - Al-Said, Amro, Ahmad
T2  - Electronics
AB  - Software testing is critical for ensuring software reliability, with test case generation often being resource-intensive and time-consuming. This study leverages the Llama-2 large language model (LLM) to automate unit test generation for Java focal methods, demonstrating the potential of AI-driven approaches to optimize software testing workflows. Our work leverages focal methods to prioritize critical components of the code to produce more context-sensitive and scalable test cases. The dataset, comprising 25,000 curated records, underwent tokenization and QLoRA quantization to facilitate training. The model was fine-tuned, achieving a training loss of 0.046. These results show the promise of AI-driven test case generation and underscore the feasibility of using fine-tuned LLMs for test case generation, highlighting opportunities for improvement through larger datasets, advanced hyperparameter optimization, and enhanced computational resources. We conducted a human-in-the-loop validation on a subset of unit tests generated by our fined-tuned LLM. This confirms that these tests effectively leverage focal methods, demonstrating the model’s capability to generate more contextually accurate unit tests. The work suggests the need to develop novel validation objective metrics specifically tailored for the automation of test cases generated by utilizing large language models. This work establishes a foundation for scalable and efficient software testing solutions driven by artificial intelligence. The data and code are publicly available on GitHub
DA  - 2025///
PY  - 2025
DO  - 10.3390/electronics14071463
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.3. Functional Testing
ER  - 

TY  - JOUR
TI  - HITS: High-coverage LLM-based unit test generation via method slicing
AU  - Zejun, Wang
AU  - Kaibo, Liu
AU  - Ge, Li
AU  - Zhi, Jin
T2  - International Conference on Automated Software Engineering
AB  - Large language models (LLMs) have behaved well in generating unit tests for Java projects. However, the performance for covering the complex focal methods within the projects is poor. Complex methods comprise many conditions and loops, requiring the test cases to be various enough to cover all lines and branches. However, existing test generation methods with LLMs provide the whole method-to-test to the LLM without assistance on input analysis. The LLM has difficulty inferring the test inputs to cover all conditions, resulting in missing lines and branches. To tackle the problem, we propose decomposing the focal methods into slices and asking the LLM to generate test cases slice by slice. Our method simplifies the analysis scope, making it easier for the LLM to cover more lines and branches in each slice. We build a dataset comprising complex focal methods collected from the projects used by existing state-of-the-art approaches. Our experiment results show that our method significantly outperforms current test case generation methods with LLMs and the typical SBST method Evosuite regarding both line and branch coverage scores.CCS CONCEPTS• Software and its engineering ? Software testing and debugging; • Computing methodologies ? Natural language processing.
DA  - 2024///
PY  - 2024
DO  - 10.48550/arxiv.2408.11324
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.4. Integration Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Improving software development workflows using generative AI
AU  - Samu, Kдhkцnen
DA  - 2024///
PY  - 2024
UR  - https://aaltodoc.aalto.fi/items/26cf8b65-5f4e-4a24-938f-2597bd51ebc7
Y2  - 2025/04/30/
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 4.1.2. Code Repair / Refactoring
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.5. Self-improving Prompts
ER  - 

TY  - JOUR
TI  - Improving the readability of generated tests using GPT-4 and ChatGPT code interpreter
AU  - Gregory, Gay
T2  - International Symposium on Search Based Software Engineering
DA  - 2023///
PY  - 2023
DO  - 10.1007/978-3-031-48796-5_11
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.8. Test Quality Analysis
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Increasing test coverage by automating BDD tests in proofs of concepts (pocs) using LLM
AU  - Shexmo, Santos
AU  - S., Raiane Eunice, Fernandes
AU  - Barbosa, Marcos Cesar, dos Santos
AU  - S., Michel, Soares
AU  - Gomes, F?bio, Rocha
AU  - Sabrina, Marczak
T2  - Brazilian Symposium on Software Quality
DA  - 2024///
PY  - 2024
DO  - 10.1145/3701625.3701637
N1  - Citation Count: 1
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.2. Test Optimization
KW  - 3.1.8. Test Quality Analysis
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Intent-driven mobile GUI testing with autonomous large language model agents
AU  - Juyeon, Yoon
AU  - R., Feldt
AU  - Shin, Yoo
T2  - International Conference on Information Control Systems & Technologies
AB  - GUI testing checks if a software system behaves as expected when users interact with its graphical interface, e.g., testing specific functionality or validating relevant use case scenarios. Currently, deciding what to test at this high level is a manual task since automated GUI testing tools target lower level adequacy metrics such as structural code coverage or activity coverage. We propose DroidAgent, an autonomous GUI testing agent for Android, for semantic, intent-driven automation of GUI testing. It is based on Large Language Models and support mechanisms such as long- and short-term memory. Given an Android app, DroidAgent sets relevant task goals and subsequently tries to achieve them by interacting with the app. Our empirical evaluation of DroidAgent using 15 apps from the Themis benchmark shows that it can set up and perform realistic tasks, with a higher level of autonomy. For example, when testing a messaging app, DroidAgent created a second account and added a first account as a friend, testing a realistic use case, without human intervention. On average, DroidAgent achieved 61
DA  - 2024///
PY  - 2024
DO  - 10.1109/icst60714.2024.00020
N1  - Citation Count: 2
KW  - 1. LLM
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.3. Test Classification
KW  - 3.1.5. Test Prioritization
KW  - 3.1.8. Test Quality Analysis
KW  - 3.3. Functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.4. Visual / GUI-based Prompting
ER  - 

TY  - JOUR
TI  - Investigating execution trace embedding for test case prioritization
AU  - Emad, Jabbar
AU  - Hadi, Hemmati
AU  - R., Feldt
T2  - International Conference on Software Quality, Reliability and Security
AB  - Most automated software testing tasks, such as test case generation, selection, and prioritization, can benefit from an abstract representation of test cases. Although test case representation usually is not explicitly discussed in software literature, but traditionally test cases are mostly represented based on what they cover in source code (e.g., which statements or branches), which is in fact an abstract representation. In this paper, we hypothesize that execution traces of test cases, as representations of their behaviour, can be leveraged to better encode test cases compared to code-based coverage information, for automated testing tasks. To validate this hypothesis, we propose an embedding approach, Test2Vec, based on an state-of-the-art neural program embedding (CodeBert), where the encoder maps test execution traces, i.e. sequences of method calls with their inputs and return values, to fixed-length, numerical vectors. We evaluate this representation in automated test case prioritization (TP) task. Our TP method is a classifier trained on the passing and failing vectors of historical test cases, in regression testing. We compare our embedding with multiple baselines and related work including CodeBert itself. The empirical study is based on 250 real faults and 703,353 seeded faults (mutants) over 250 revisions of 10 open-source Java projects from Defects4J, with a total of over 1,407,206 execution traces. Results show that our approach improves all alternatives, significantly, with respect to studied metrics. We also show that both inputs and outputs of a method are important elements of the execution-based embedding.
DA  - 2023///
PY  - 2023
DO  - 10.1109/qrs60937.2023.00036
N1  - Citation Count: 0
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.2.1. CNN / RNN / LSTM
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.5. Test Prioritization
ER  - 

TY  - JOUR
TI  - KAT: Dependency-aware automated API testing with large language models
AU  - Tri, Le
AU  - Thien, Tran
AU  - Duy, Cao
AU  - Vy, Le
AU  - Tien, Nguyen
AU  - Vu, Nguyen
T2  - International Conference on Information Control Systems & Technologies
AB  - API testing has increasing demands for software companies. Prior API testing tools were aware of certain types of dependencies that needed to be concise between operations and parameters. However, their approaches, which are mostly done manually or using heuristic-based algorithms, have limitations due to the complexity of these dependencies. In this paper, we present KAT (Katalon API Testing), a novel AI -driven approach that leverages the large language model GPT in conjunction with advanced prompting techniques to autonomously generate test cases to validate RESTful APIs. Our comprehensive strategy encompasses various processes to construct an operation depen-dency graph from an OpenAPI specification and to generate test scripts, constraint validation scripts, test cases, and test data. Our evaluation of KAT using 12 real-world RESTful services shows that it can improve test coverage, detect more undocumented status codes, and reduce false positives in these services in comparison with a state-of-the-art automated test generation tool. These results indicate the effectiveness of using the large language model for generating test scripts and data for API testing.
DA  - 2024///
PY  - 2024
DO  - 10.1109/icst60714.2024.00017
N1  - Citation Count: 3
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.2. Test Data Generation
KW  - 3.2.3. Test Datasets (HTML
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - Screenshots
KW  - UML)
ER  - 

TY  - JOUR
TI  - Large language models for software engineering: a systematic literature review
AU  - Xinyi, Hou
AU  - Yanjie, Zhao
AU  - Yue, Liu
AU  - Yang, Zhou
AU  - Kailong, Wang
AU  - Li, Li
AU  - Xiapu, Luo
AU  - David, Lo
AU  - John, Grundy
AU  - Haoyu, Wang
T2  - ACM Transactions on Software Engineering and Methodology
AB  - Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We collect and analyze 229 research papers from 2017 to 2023 to answer four key research questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, preprocessing, and application highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and flagging promising areas for future study.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arxiv.2308.10620
N1  - Citation Count: 62
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.2. Test Data Generation
KW  - 3.2.2. Test Documentation
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 4.1.4. Source Code Analysis
KW  - 4.2. Development Process Automation
KW  - 4.2.2. Debugging Support
KW  - 4.2.4. Low-code / No-code Development
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - CONF
TI  - Large language models for software engineering: a systematic mapping study
AU  - Kürşat, Muhammet, Görmez
AU  - Murat, Yılmaz
AU  - M., Paul, Clarke
T3  - European Conference on Software Process Improvement
DA  - 2024///
PY  - 2024
SP  - 64
EP  - 79
PB  - Springer
ST  - Large language models for software engineering
UR  - https://link.springer.com/chapter/10.1007/978-3-031-71139-8_5
Y2  - 2025/04/30/
N1  - Citation Count: 3
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Large language models for software vulnerability detection: a guide for researchers on models, methods, techniques, datasets, and metrics
AU  - Taghavi, Seyed Mohammad, Far
AU  - Farid, Feyzi
DA  - 2025///
PY  - 2025
DO  - 10.1007/s10207-025-00992-7
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
ER  - 

TY  - JOUR
TI  - Large language models in software engineering: a critical review of evaluation strategies
AU  - Ali, Bektas
DA  - 2025///
PY  - 2025
ST  - Large language models in software engineering
UR  - https://www.inf.fu-berlin.de/inst/ag-se/theses/Bektas25-LLM-evaluation-strategies.pdf
Y2  - 2025/04/30/
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
ER  - 

TY  - JOUR
TI  - Large language model-supported software testing with the CS matrix taxonomy
AU  - L., Johannah, Crandall
AU  - S., Aaron, Crandall
T2  - Journal of Computing Sciences in Colleges (JCSC; Formerly: Journal of Computing in Small Colleges)
DA  - 2024///
PY  - 2024
DO  - 10.5555/3715602.3715612
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - CONF
TI  - Learning deep semantics for test completion
AU  - Pengyu, Nie
AU  - Rahul, Banerjee
AU  - Jessy, Junyi, Li
AU  - J., Raymond, Mooney
AU  - Milos, Gligoric
T3  - 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)
DA  - 2023///
PY  - 2023
DO  - 10.1109/ICSE48619.2023.00178
SP  - 2111
EP  - 2123
PB  - IEEE
SN  - 978-1-6654-5701-9
UR  - https://ieeexplore.ieee.org/document/10172620/
Y2  - 2025/02/05/
N1  - Citation Count: 17
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.2. Test Data Generation
KW  - 3.2.1. Test Oracle
ER  - 

TY  - JOUR
TI  - Leveraging large language models to improve REST API testing
AU  - Myeongsoo, Kim
AU  - Tyler, Stennett
AU  - Dhruv, Shah
AU  - Saurabh, Sinha
AU  - A., Orso
T2  - 2024 IEEE/ACM 46th International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER)
AB  - The widespread adoption of REST APIs, coupled with their growing complexity and size, has led to the need for automated REST API testing tools. Current tools focus on the structured data in REST API specifications but often neglect valuable insights available in unstructured natural-language descriptions in the specifications, which leads to suboptimal test coverage. Recently, to address this gap, researchers have developed techniques that extract rules from these human-readable descriptions and query knowledge bases to derive meaningful input values. However, these techniques are limited in the types of rules they can extract and prone to produce inaccurate results. This paper presents RESTGPT, an innovative approach that leverages the power and intrinsic context-awareness of Large Language Models (LLMs) to improve REST API testing. RESTGPT takes as input an API specification, extracts machine-interpretable rules, and generates example parameter values from natural-language descriptions in the specification. It then augments the original specification with these rules and values. Our evaluations indicate that RESTGPT outperforms existing techniques in both rule extraction and value generation. Given these promising results, we outline future research directions for advancing REST API testing through LLMs.
DA  - 2023///
PY  - 2023
DO  - 10.1145/3639476.3639769
N1  - Citation Count: 5
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.2. Test Data Generation
KW  - 3.2.3. Test Datasets (HTML
KW  - Screenshots
KW  - UML)
ER  - 

TY  - JOUR
TI  - Leveraging pre-trained large language models (llms) for on-premises comprehensive automated test case generation: An empirical study
AU  - Hang, Yin
AU  - Hamza, Mohammed
AU  - Sai, Boyapati
T2  - 2024 9th International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS)
AB  - The rapidly evolving field of Artificial Intelligence (AI)-assisted software testing has predominantly focused on automated test code generation, with limited research exploring the realm of automated test case generation from user stories requirements. This paper presents a comprehensive empirical study on harnessing pre-trained Large Language Models (LLMs) for generating concrete test cases from natural language requirements given in user stories. We investigate the efficacy of various prompting and alignment techniques, including prompt chaining, few-shot instructions, and agency-based approaches, to facilitate secure on-premises deployment. By integrating our learnings with an on-premises model setup, wherein we deploy a RoPE scaled 4-bit quantized LLaMA 3 70B Instruct model, optionally augmented with LoRA adapters trained on QA datasets, we demonstrate that this approach yields more accurate and consistent test cases despite video random-access memory (VRAM) constraints, thereby maintaining the security benefits of an on-premises deployment.
DA  - 2024///
PY  - 2024
DO  - 10.1109/iciibms62405.2024.10792720
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - LLM for test script generation and migration: Challenges, capabilities, and opportunities
AU  - Shengcheng, Yu
AU  - Chunrong, Fang
AU  - Yucheng, Ling
AU  - Chentian, Wu
AU  - Zhenyu, Chen
T2  - International Conference on Software Quality, Reliability and Security
AB  - This paper investigates the application of large language models (LLM) in the domain of mobile application test script generation. Test script generation is a vital component of software testing, enabling efficient and reliable automation of repetitive test tasks. However, existing generation approaches often encounter limitations, such as difficulties in accurately capturing and reproducing test scripts across diverse devices, platforms, and applications. These challenges arise due to differences in screen sizes, input modalities, platform behaviors, API inconsistencies, and application architectures. Overcoming these limitations is crucial for achieving robust and comprehensive test automation.By leveraging the capabilities of LLMs, we aim to address these challenges and explore its potential as a versatile tool for test automation. We investigate how well LLMs can adapt to diverse devices and systems while accurately capturing and generating test scripts. Additionally, we evaluate its cross-platform generation capabilities by assessing its ability to handle operating system variations and platform-specific behaviors. Furthermore, we explore the application of LLMs in cross-app migration, where it generates test scripts across different applications and software environments based on existing scripts.Throughout the investigation, we analyze its adaptability to various user interfaces, app architectures, and interaction patterns, ensuring accurate script generation and compatibility. The findings of this research contribute to the understanding of LLMs’ capabilities in test automation. Ultimately, this research aims to enhance software testing practices, empowering app developers to achieve higher levels of software quality and development efficiency.
DA  - 2023///
PY  - 2023
DO  - 10.1109/qrs60937.2023.00029
N1  - Citation Count: 14
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.4. Test Repair
KW  - 3.2. Test Data Generation
KW  - 3.2.3. Test Datasets (HTML
KW  - 3.3. Functional Testing
KW  - 3.5. Robustness / Non-functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.3. Instructional / Role prompting
KW  - Screenshots
KW  - UML)
ER  - 

TY  - JOUR
TI  - LLM prompt engineering for automated white-box integration test generation in REST apis
AU  - Mesquita, Andrй, Rincon
AU  - A., Vincenzi
AU  - Pascoal, Jo?o, Faria
T2  - International Conference on Software Testing, Verification and Validation Workshops
AB  - This study explores prompt engineering for automated white-box integration testing of RESTful APIs using Large Language Models (LLMs). Four versions of prompts were designed and tested across three OpenAI models (GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o) to assess their impact on code coverage, token consumption, execution time, and financial cost. The results indicate that different prompt versions, especially with more advanced models, achieved up to 90
DA  - 2025///
PY  - 2025
DO  - 10.1109/icstw64639.2025.10962507
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.4. Integration Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - LLM4Fin: Fully automating LLM-powered test case generation for FinTech software acceptance testing
AU  - Zhiyi, Xue
AU  - Liangguo, Li
AU  - Senyue, Tian
AU  - Xiaohong, Chen
AU  - Pingping, Li
AU  - Liangyu, Chen
AU  - Tingting, Jiang
AU  - Min, Zhang
T2  - International Symposium on Software Testing and Analysis
DA  - 2024///
PY  - 2024
DO  - 10.1145/3650212.3680388
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.3. Functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - LLM-based methods for the creation of unit tests in game development
AU  - C., Paduraru
AU  - A., Staicu
AU  - Alin, Stefanescu
T2  - International Conference on Knowledge-Based Intelligent Information & Engineering Systems
DA  - 2024///
PY  - 2024
DO  - 10.1016/j.procs.2024.09.473
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.3. Functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - LLM-based multi-agent systems for software engineering: Literature review, vision and the road ahead
AU  - Junda, He
AU  - Christoph, Treude
AU  - David, Lo
T2  - ACM Transactions on Software Engineering and Methodology
AB  - Integrating Large Language Models (LLMs) into autonomous agents marks a significant shift in the research landscape by offering cognitive abilities that are competitive with human planning and reasoning. This paper explores the transformative potential of integrating Large Language Models into Multi-Agent (LMA) systems for addressing complex challenges in software engineering (SE). By leveraging the collaborative and specialized abilities of multiple agents, LMA systems enable autonomous problem-solving, improve robustness, and provide scalable solutions for managing the complexity of real-world software projects. In this paper, we conduct a systematic review of recent primary studies to map the current landscape of LMA applications across various stages of the software development lifecycle (SDLC). To illustrate current capabilities and limitations, we perform two case studies to demonstrate the effectiveness of state-of-the-art LMA frameworks. Additionally, we identify critical research gaps and propose a comprehensive research agenda focused on enhancing individual agent capabilities and optimizing agent synergy. Our work outlines a forward-looking vision for developing fully autonomous, scalable, and trustworthy LMA systems, laying the foundation for the evolution of Software Engineering 2.0.
DA  - 2024///
PY  - 2024
DO  - 10.1145/3712003
N1  - Citation Count: 1
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 4.1.5. Requirement Classification
KW  - 4.1.6. Requirement Generation
KW  - 4.2. Development Process Automation
KW  - 4.2.2. Debugging Support
KW  - 4.2.4. Low-code / No-code Development
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - CONF
TI  - Llms and prompting for unit test generation: A large-scale evaluation
AU  - C., Wendkuuni, Ouedraogo
AU  - Kader, Kabore
AU  - Haoye, Tian
AU  - Yewei, Song
AU  - Anil, Koyuncu
AU  - Jacques, Klein
AU  - David, Lo
AU  - F., Tegawende, Bissyande
T3  - Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering
DA  - 2024///
PY  - 2024
SP  - 2464
EP  - 2465
ST  - Llms and prompting for unit test generation
UR  - https://dl.acm.org/doi/abs/10.1145/3691620.3695330
Y2  - 2025/04/30/
N1  - Citation Count: 4
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - LLMs for intelligent software testing: a comparative study
AU  - Mohamed, Boukhlif
AU  - Nassim, Kharmoum
AU  - N., Kharmoum
AU  - Mohamed, Hanine
T2  - Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security
DA  - 2024///
PY  - 2024
DO  - 10.1145/3659677.3659749
N1  - Citation Count: 3
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.3. Functional Testing
KW  - 3.5. Robustness / Non-functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - LLMSQLi: a black-box web sqli detection tool based on large language model
AU  - Tinghui, Yang
AU  - Zhiyuan, Jiang
AU  - Yongjun, Wang
T2  - 2024 5th International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)
AB  - Black box detection tools of SQL injection vulnerabilities simulate real-world web attack scenarios, making it essential for evaluating SQLi risks in actual web applications. However, current black-box approaches depend on predefined rules for SQLi vulnerability detection, which limits both their efficiency and accuracy. In this paper, we propose a black-box SQLi detection tool based on large language multi-agent, LLMSQLi, which uses the context understanding and reasoning capabilities of large language models and the cooperative division of labor mode of multi-agent to generate payloads customized for test targets and efficiently detect SQL injection vulnerabilities in Web programs. Drawing inspiration from real-world teams of security experts, LLMSQLi simulates the step-by-step process of human experts in testing tasks through the LLM Muti-Agent collaboration model. We ran experiments on SQLiMicroBenchmark to compare the performance of LLMSQLi and two of the most advanced black-box SQLi testing tools. Experiments show that LLMSQLi successfully detected all 15 targets and outperformed other tools.
DA  - 2024///
PY  - 2024
DO  - 10.1109/icbase63199.2024.10762654
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1.9. Automated testing tools
KW  - 3.5. Robustness / Non-functional Testing
ER  - 

TY  - JOUR
TI  - Make LLM a testing expert: Bringing human-like interaction to mobile GUI testing via functionality-aware decisions
AU  - Zhe, Liu
AU  - Chunyang, Chen
AU  - Junjie, Wang
AU  - Mengzhuo, Chen
AU  - Boyu, Wu
AU  - Xing, Che
AU  - Dandan, Wang
AU  - Qing, Wang
T2  - International Conference on Software Engineering
AB  - Automated Graphical User Interface (GUI) testing plays a crucial role in ensuring app quality, especially as mobile applications have become an integral part of our daily lives. Despite the growing popularity of learning-based techniques in automated GUI testing due to their ability to generate human-like interactions, they still suffer from several limitations, such as low testing coverage, inadequate generalization capabilities, and heavy reliance on training data. Inspired by the success of Large Language Models (LLMs) like ChatGPT in natural language understanding and question answering, we formulate the mobile GUI testing problem as a Q&A task. We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process. Within this framework, we have also introduced a functionality-aware memory prompting mechanism that equips the LLM with the ability to retain testing knowledge of the whole process and conduct long-term, functionality-based reasoning to guide exploration. We evaluate it on 93 apps from Google Play and demonstrate that it outperforms the best baseline by 32
DA  - 2023///
PY  - 2023
DO  - 10.1145/3597503.3639180
N1  - Citation Count: 20
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.6. Failure Detection
KW  - 3.1.8. Test Quality Analysis
KW  - 3.3. Functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.4. Visual / GUI-based Prompting
ER  - 

TY  - JOUR
TI  - Model-based test execution from high-level natural language instructions using GPT-4
AU  - Yusaf, Mohammad, Azimi
AU  - Cemal, Yilmaz
T2  - Software quality journal
DA  - 2025///
PY  - 2025
DO  - 10.1007/s11219-025-09712-9
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.4. Visual / GUI-based Prompting
ER  - 

TY  - JOUR
TI  - Mutation testing via iterative large language model-driven scientific debugging
AU  - Philipp, Straubinger
AU  - M., Kreis
AU  - Stephan, Lukasczyk
AU  - Gordon, Fraser
T2  - International Conference on Software Testing, Verification and Validation Workshops
AB  - Large Language Models (LLMs) can generate plausible test code. Intuitively they generate this by imitating tests seen in their training data, rather than reasoning about execution semantics. However, such reasoning is important when applying mutation testing, where individual tests need to demonstrate differences in program behavior between a program and specific artificial defects (mutants). In this paper, we evaluate whether Scientific Debugging, which has been shown to help LLMs when debugging, can also help them to generate tests for mutants. In the resulting approach, LLMs form hypotheses about how to kill specific mutants, and then iteratively generate and refine tests until they succeed, all with detailed explanations for each step. We compare this method to three baselines: (1) directly asking the LLM to generate tests, (2) repeatedly querying the LLM when tests fail, and (3) search-based test generation with Pynguin. Our experiments evaluate these methods based on several factors, including mutation score, code coverage, success rate, and the ability to identify equivalent mutants. The results demonstrate that LLMs, although requiring higher computational cost, consistently outperform Pynguin in generating tests with better fault detection and coverage. Importantly, we observe that the iterative refinement of test cases is important for achieving high-quality test suites.
DA  - 2025///
PY  - 2025
DO  - 10.1109/icstw64639.2025.10962485
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.6. Failure Detection
KW  - 3.5. Robustness / Non-functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.5. Self-improving Prompts
ER  - 

TY  - JOUR
TI  - Navigating confidentiality in test automation: a case study in LLM driven test data generation
AU  - Hrishikesh, Karmarkar
AU  - Supriya, Agrawal
AU  - Avriti, Chauhan
AU  - Pranav, Shete
T2  - IEEE International Conference on Software Analysis, Evolution, and Reengineering
AB  - In out sourced industrial projects for testing of web applications, often neither the application to be tested, nor its source code are provided to the testing team, due to confidentiality reasons, making systematic testing of these applications very challenging. However, textual descriptions of such systems are often available. So, one can consider leveraging a Large Language Model (LLM) to parse these descriptions and synthesize test generators (programs that produce test data). In our experience, LLM synthesized test generators suffer from two problems:- (1) unsound: the generators might produce invalid data and (2) incomplete: the generators typically fail to generate all expected valid inputs. To mitigate these problems, we introduce TestRefineGen a method for autonomously generating test data from textual descriptions. TestRe-fineGen begins by invoking an LLM to parse a given corpus of documents and produce multiple test gener-ators. It then uses a novel ranking approach to identify generators that can produce invalid test data, and then automatically repairs them using a counterexample-guided refinement process. Lastly, TestRefineGen per-forms a generalization procedure that offsets synthesis or refinements that leads to incompleteness, to obtain generators that produce more comprehensive valid in-puts. We evaluated the effectiveness of TestRefineGen on a manually curated set of 256 textual descriptions of test data. TestRefineGen synthesized generators that produce valid test data for 66.01
DA  - 2024///
PY  - 2024
DO  - 10.1109/saner60148.2024.00041
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.4. Test Repair
KW  - 3.3. Functional Testing
KW  - 3.5. Robustness / Non-functional Testing
ER  - 

TY  - JOUR
TI  - Observation-based unit test generation at Meta
AU  - N., Alshahwan
AU  - Mark, Harman
AU  - Alexandru, Marginean
AU  - Rotem, Tal
AU  - Eddy, Wang
T2  - SIGSOFT FSE Companion
AB  - TestGen automatically generates unit tests, carved from serialized observations of complex objects, observed during app execution. We describe the development and deployment of TestGen at Meta. In particular, we focus on the scalability challenges overcome during development in order to deploy observation-based test carving at scale in industry. So far, TestGen has landed 518 tests into production, which have been executed 9,617,349 times in continuous integration, finding 5,702 faults. Meta is currently in the process of more widespread deployment. Our evaluation reveals that, when carving its observations from 4,361 reliable end-to-end tests, TestGen was able to generate tests for at least 86
DA  - 2024///
PY  - 2024
DO  - 10.48550/arxiv.2402.06111
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.6. Failure Detection
KW  - 3.1.8. Test Quality Analysis
KW  - 3.3. Functional Testing
KW  - 3.5. Robustness / Non-functional Testing
ER  - 

TY  - CONF
TI  - On the evaluation of large language models in unit test generation
AU  - Lin, Yang
AU  - Chen, Yang
AU  - Shutao, Gao
AU  - Weijing, Wang
AU  - Bo, Wang
AU  - Qihao, Zhu
AU  - Xiao, Chu
AU  - Jianyi, Zhou
AU  - Guangtai, Liang
AU  - Qianxiang, Wang
T3  - Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering
DA  - 2024///
PY  - 2024
SP  - 1607
EP  - 1619
UR  - https://dl.acm.org/doi/abs/10.1145/3691620.3695529?casa_token=RN6C5CwP27kAAAAA:ttVBtvNRdcXcbiHfVfv1HZ5cnIK10tDw3Lc6MZGqc00O7X7djuTpdEsf0PWo_5XQcNQXR8F8EkQsFg
Y2  - 2025/04/30/
N1  - Citation Count: 23
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.3. Functional Testing
KW  - 3.4. Integration Testing
KW  - 3.5. Robustness / Non-functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.2. Few-shot / Zero-shot
ER  - 

TY  - JOUR
TI  - Optimizing search-based unit test generation with large language models: An empirical study
AU  - Danni, Xiao
AU  - Yimeng, Guo
AU  - Yanhui, Li
AU  - Lin, Chen
T2  - Asia-Pacific Symposium on Internetware
DA  - 2024///
PY  - 2024
DO  - 10.1145/3671016.3674813
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.5. Robustness / Non-functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Practitioners’ expectations on automated test generation
AU  - Xiao, Yu
AU  - Lei, Liu
AU  - Xing, Hu
AU  - J., Keung
AU  - Xin, Xia
AU  - David, Lo
T2  - Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis
DA  - 2024///
PY  - 2024
DO  - 10.1145/3650212.3680386
N1  - Citation Count: 1
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.6. Failure Detection
ER  - 

TY  - CONF
TI  - Prioritized test generation guided by software fault prediction
AU  - Eran, Hershkovich
AU  - Roni, Stern
AU  - Rui, Abreu
AU  - Amir, Elmishali
T3  - 2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
DA  - 2021///
PY  - 2021
DO  - 10.1109/ICSTW52544.2021.00045
SP  - 218
EP  - 225
PB  - IEEE
SN  - 978-1-6654-4456-9
UR  - https://ieeexplore.ieee.org/document/9440144/
Y2  - 2025/02/05/
N1  - Citation Count: 3
KW  - 2. ML
KW  - 2.3. Supervised / Unsupervised Learning Strategies
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.5. Test Prioritization
KW  - 3.1.8. Test Quality Analysis
ER  - 

TY  - CONF
TI  - Quality assurance for LLM-generated test cases: a systematic literature review
AU  - Hasali, Edirisinghe
AU  - Dilani, Wickramaarachchi
T3  - 2024 8th SLAAI International Conference on Artificial Intelligence (SLAAI-ICAI)
DA  - 2024///
PY  - 2024
SP  - 1
EP  - 6
PB  - IEEE
ST  - Quality assurance for LLM-generated test cases
UR  - https://ieeexplore.ieee.org/abstract/document/10844968/
Y2  - 2025/04/30/
N1  - Citation Count: 1
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
ER  - 

TY  - JOUR
TI  - Refining software defect prediction through attentive neural models for code understanding
AU  - Mona, Nashaat
AU  - James, Miller
T2  - Journal of Systems and Software
DA  - 2024///
PY  - 2024
DO  - 10.1016/j.jss.2024.112266
N1  - Citation Count: 0
KW  - 2. ML
KW  - 2.3. Supervised / Unsupervised Learning Strategies
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.4. Source Code Analysis
ER  - 

TY  - BOOK
TI  - Regenerating a graphical user interface using deep learning
AU  - Fatima, Cheddi
AU  - Ahmed, Tahiri
AU  - Mohammed, Serrhini
DA  - 2022///
PY  - 2022
VL  - 1399
LA  - en
PB  - Springer Singapore
SN  - 978-981-16-5558-6 978-981-16-5559-3
UR  - https://link.springer.com/10.1007/978-981-16-5559-3_28
Y2  - 2025/04/05/
N1  - Citation Count: 2
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.2.1. CNN / RNN / LSTM
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 5. Prompt Engineering
KW  - 5.4. Visual / GUI-based Prompting
ER  - 

TY  - JOUR
TI  - Requirements-driven automated software testing: a systematic review
AU  - Fanyu, Wang
AU  - Chetan, Arora
AU  - Chakkrit, Tantithamthavorn
AU  - Kaicheng, Huang
AU  - Aldeida, Aleti
DA  - 2025///
PY  - 2025
ST  - Requirements-driven automated software testing
UR  - https://www.preprints.org/frontend/manuscript/3ad71ccffb8f4bc5ef999185fdf2db9c/download_pub
Y2  - 2025/04/30/
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.5. Requirement Classification
KW  - 4.1.6. Requirement Generation
ER  - 

TY  - JOUR
TI  - SearchGEM5: Towards reliable gem5 with search based software testing and large language models
AU  - Aidan, Dakhama
AU  - Karine, Even-Mendoza
AU  - B., W., Langdon
AU  - D., Hйctor, Menйndez
AU  - Justyna, Petke
T2  - International Symposium on Search Based Software Engineering
DA  - 2023///
PY  - 2023
DO  - 10.1007/978-3-031-48796-5_14
N1  - Citation Count: 3
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.5. Robustness / Non-functional Testing
ER  - 

TY  - JOUR
TI  - Self-collaboration code generation via ChatGPT
AU  - Yihong, Dong
AU  - Xue, Jiang
AU  - Zhi, Jin
AU  - Ge, Li
T2  - ACM Transactions on Software Engineering and Methodology
AB  - Although Large Language Models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, 1) Multiple LLM agents act as distinct ‘experts’, each responsible for a specific subtask within a complex task; 2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other’s work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development’s analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9
DA  - 2023///
PY  - 2023
DO  - 10.1145/3672459
N1  - Citation Count: 39
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.3. Instructional / Role prompting
ER  - 

TY  - JOUR
TI  - Self-healing automation testing with Selenium and ChatGPT API integration to improve the efficiency of the maintenance
AU  - Yunjin, Kim
AU  - Rand, Kouatly
T2  - 2024 2nd International Conference on Foundation and Large Language Models (FLLM)
AB  - The rapid delivery in software development life cycle demands more adaptable automation testing frameworks. The current automation test frameworks struggle with maintaining the scripts due to frequent changes in web elements. To resolve this issue, this paper introduces a self-healing automation testing framework that updates broken locators without human intervention. ChatGPT API is integrated within the newly developed automation framework to reduce time and effort for maintenance. The results show the potential of the self-healing automation testing framework with ChatGPT API integration compared to traditional automation tests.
DA  - 2024///
PY  - 2024
DO  - 10.1109/fllm63129.2024.10852490
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.2.2. Autoencoders / GANs
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.4. Test Repair
KW  - 3.1.7. Self-healing Testing
KW  - 3.3. Functional Testing
ER  - 

TY  - JOUR
TI  - Semantic constraint inference for web form test generation
AU  - Parsa, Alian
AU  - Noor, Nashid
AU  - M., Shahbandeh
AU  - Ali, Mesbah
T2  - International Symposium on Software Testing and Analysis
AB  - Automated test generation for web forms has been a longstanding challenge, exacerbated by the intrinsic human-centric design of forms and their complex, device-agnostic structures. We introduce an innovative approach, called FormNexus, for automated web form test generation, which emphasizes deriving semantic insights from individual form elements and relations among them, utilizing textual content, DOM tree structures, and visual proximity. The insights gathered are transformed into a new conceptual graph, the Form Entity Relation Graph (FERG), which offers machine-friendly semantic information extraction. Leveraging LLMs, FormNexus adopts a feedback-driven mechanism for generating and refining input constraints based on real-time form submission responses. The culmination of this approach is a robust set of test cases, each produced by methodically invalidating constraints, ensuring comprehensive testing scenarios for web forms. This work bridges the existing gap in automated web form testing by intertwining the capabilities of LLMs with advanced semantic inference methods. Our evaluation demonstrates that FormNexus combined with GPT-4 achieves 89
DA  - 2024///
PY  - 2024
DO  - 10.1145/3650212.3680332
N1  - Citation Count: 0
KW  - 2. ML
KW  - 2.1. Traditional Machine Learning
KW  - 2.1.2. Feature-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.3. Functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.5. Self-improving Prompts
ER  - 

TY  - JOUR
TI  - Semantic test repair for web applications
AU  - Xiaofang, Qi
AU  - Xiang, Qian
AU  - Yanhui, Li
T2  - ESEC/SIGSOFT FSE
AB  - Automation testing is widely used in the functional testing of web applications. However, during the evolution of web applications, such web test scripts tend to break. It is essential to repair such broken test scripts to make regression testing run successfully. As manual repairing is time-consuming and expensive, researchers focus on automatic repairing techniques. Empirical study shows that the web element locator is the leading cause of web test breakages. Most existing repair techniques utilize Document Object Model attributes or visual appearances of elements to find their location but neglect their semantic information. This paper proposes a novel semantic repair technique called Semantic Test Repair (Semter) for web test repair. Our approach captures relevant semantic information from test executions on the application’s basic version and locates target elements by calculating semantic similarity between elements to repair tests. Our approach can also repair test workflow due to web page additions or deletions by a local exploration in the updated version. We evaluated the efficacy of our technique on six real-world web applications compared with three baselines. Experimental results show that Semter achieves an 84
DA  - 2023///
PY  - 2023
DO  - 10.1145/3611643.3616324
N1  - Citation Count: 1
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.4. Test Repair
KW  - 3.1.8. Test Quality Analysis
KW  - 5. Prompt Engineering
KW  - 5.4. Visual / GUI-based Prompting
ER  - 

TY  - JOUR
TI  - Software testing with large language models: Survey, landscape, and vision
AU  - Junjie, Wang
AU  - Yuchao, Huang
AU  - Chunyang, Chen
AU  - Zhe, Liu
AU  - Song, Wang
AU  - Qing, Wang
T2  - IEEE Transactions on Software Engineering
AB  - Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.
DA  - 2024///
PY  - 2024
DO  - 10.1109/tse.2024.3368208
N1  - Citation Count: 85
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.3. Functional Testing
KW  - 3.4. Integration Testing
KW  - 3.5. Robustness / Non-functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.2. Few-shot / Zero-shot
ER  - 

TY  - CONF
TI  - Studying the usage of text-to-text transfer transformer to support code-related tasks
AU  - Antonio, Mastropaolo
AU  - Simone, Scalabrino
AU  - Nathan, Cooper
AU  - David, Nader Palacio
AU  - Denys, Poshyvanyk
AU  - Rocco, Oliveto
AU  - Gabriele, Bavota
T3  - 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
DA  - 2021///
PY  - 2021
DO  - 10.1109/ICSE43902.2021.00041
SP  - 336
EP  - 347
PB  - IEEE
SN  - 978-1-6654-0296-5
UR  - https://ieeexplore.ieee.org/document/9401982/
Y2  - 2025/02/05/
N1  - Citation Count: 176
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 4.1.2. Code Repair / Refactoring
KW  - 4.1.4. Source Code Analysis
ER  - 

TY  - CONF
TI  - Supervised learning over test executions as a test oracle
AU  - Foivos, Tsimpourlas
AU  - Ajitha, Rajan
AU  - Miltiadis, Allamanis
T3  - Proceedings of the 36th Annual ACM Symposium on Applied Computing
DA  - 2021///
PY  - 2021
DO  - 10.1145/3412841.3442027
SP  - 1521
EP  - 1531
LA  - en
PB  - ACM
SN  - 978-1-4503-8104-8
UR  - https://dl.acm.org/doi/10.1145/3412841.3442027
Y2  - 2025/02/05/
N1  - Citation Count: 8
KW  - 1. LLM
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 2. ML
KW  - 2.1. Traditional Machine Learning
KW  - 2.1.1. SVM / Decision Trees / Random Forest
KW  - 2.3. Supervised / Unsupervised Learning Strategies
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.3. Test Classification
KW  - 3.1.8. Test Quality Analysis
KW  - 3.5. Robustness / Non-functional Testing
ER  - 

TY  - JOUR
TI  - Symbolic execution with test cases generated by large language models
AU  - Jiahe, Xu
AU  - Jingwei, Xu
AU  - Taolue, Chen
AU  - Xiaoxing, Ma
T2  - International Conference on Software Quality, Reliability and Security
AB  - Symbolic execution is a powerful program analysis technique. External environment construction and internal path explosion are two long-standing problems which may affect the effectiveness and performance of symbolic execution on complex programs. The intrinsic challenge is to achieve a sufficient understanding of the program context to construct a set of execution environments which can guide the selection of symbolic states. In this paper, we propose a novel program-context-guided symbolic execution framework LangSym based on program’s instruction/user manual. Leveraging the capabilities of natural language understanding and code generation in large language models (LLMs), LangSym can automatically extract the knowledge related to the functionality of the program, and generate adequate test cases and the corresponding environments as the prior knowledge for symbolic execution. We instantiate LangSym in KLEE, a widely adopted symbolic execution engine, to build a pipeline that could automatically leverage LLMs to boost the symbolic execution. We evaluate LangSym on almost all GNU Coreutils programs and considerable large-scale programs, showing that LangSym outperforms the existing strategies in KLEE with at least a 10
DA  - 2024///
PY  - 2024
DO  - 10.1109/qrs62785.2024.00031
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 2. ML
KW  - 2.1. Traditional Machine Learning
KW  - 2.1.2. Feature-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.2. Test Data Generation
KW  - 3.2.2. Test Documentation
KW  - 3.2.3. Test Datasets (HTML
KW  - Screenshots
KW  - UML)
ER  - 

TY  - JOUR
TI  - Test amplification for REST apis using "out-of-the-box" large language models
AU  - Tolgahan, Bardakci
AU  - Serge, Demeyer
AU  - Mutlu, Beyazit
T2  - IEEE Software
AB  - REST APIs (Representational State Transfer Application Programming Interfaces) are an indispensable building block in today's cloud-native applications, so testing them is critically important. However, writing automated tests for such REST APIs is challenging because one needs strong and readable tests that exercise the boundary values of the protocol embedded in the REST API. In this paper, we report our experience with using"out of the box"large language models (ChatGPT and GitHub's Copilot) to amplify REST API test suites. We compare the resulting tests based on coverage and understandability, and we derive a series of guidelines and lessons learned concerning the prompts that result in the strongest test suite.
DA  - 2025///
PY  - 2025
DO  - 10.48550/arxiv.2503.10306
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.4. Visual / GUI-based Prompting
ER  - 

TY  - JOUR
TI  - Test code generation for telecom software systems using two-stage generative model
AU  - Mohamad, Nabeel
AU  - Daniil, Doumitrou, Nimara
AU  - Tahar, Zanouda
T2  - 2024 IEEE International Conference on Communications Workshops (ICC Workshops)
AB  - In recent years, the evolution of Telecom towards achieving intelligent, autonomous, and open networks has led to an increasingly complex Telecom Software system, supporting various heterogeneous deployment scenarios, with multi-standard and multi-vendor support. As a result, it becomes a challenge for large-scale Telecom software companies to develop and test software for all deployment scenarios. To address these challenges, we propose a framework for Automated Test Generation for large-scale Telecom Software systems. We begin by generating Test Case Input data for test scenarios observed using a time-series Generative model trained on historical Telecom Network data during field trials. Additionally, the time-series Generative model helps in preserving the privacy of Telecom data. The generated time-series software performance data are then utilized with test descriptions written in natural language to generate Test Script using the Generative Large Language Model. Our comprehensive experiments on public datasets and Telecom datasets obtained from operational Telecom Networks demonstrate that the framework can effectively generate comprehensive test case data input and useful test code.
DA  - 2024///
PY  - 2024
DO  - 10.1109/iccworkshops59551.2024.10615269
N1  - Citation Count: 1
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.5. Test Prioritization
KW  - 3.1.8. Test Quality Analysis
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Test suite optimization using machine learning techniques: a comprehensive study
AU  - Abid, Mehmood
AU  - Mudassir, Qazi, Ilyas
AU  - Muneer, Ahmad
AU  - Zhongliang, Shi
T2  - IEEE access : practical innovations, open solutions
DA  - 2024///
PY  - 2024
J2  - IEEE Access
ST  - Test suite optimization using machine learning techniques
UR  - https://ieeexplore.ieee.org/abstract/document/10741285/
Y2  - 2025/04/30/
N1  - Citation Count: 3
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.2.1. CNN / RNN / LSTM
KW  - 2.2.2. Autoencoders / GANs
KW  - 2.5. Reinforcement Learning Strategies
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.2. Test Optimization
KW  - 3.1.5. Test Prioritization
ER  - 

TY  - JOUR
TI  - Test-agent: a multimodal app automation testing framework based on the large language model
AU  - Youwei, Li
AU  - Yangyang, Li
AU  - Yangzhao, Yang
T2  - 2024 IEEE 4th International Conference on Digital Twins and Parallel Intelligence (DTPI)
AB  - This paper introduces a multimodal agent-based app automation testing framework, named Test-Agent, built on the Large Language Model (LLM), designed to address the growing challenges in mobile application automation testing. As mobile applications become more prevalent and emerging systems like Harmony OS Next and mini-programs emerge, traditional automated testing methods, which depend on manually crafting test cases and scripts, are no longer sufficient for cross-platform compatibility and complex interaction logic. The Test-Agent framework employs artificial intelligence technologies to analyze application interface screenshots and user natural language instructions. Combined with deep learning models, it automatically generates and executes test actions on mobile devices. This innovative approach eliminates the need for pre-written test scripts or backend system access, relying solely on screenshots and UI structure information. It achieves cross-platform and cross-application universality, significantly reducing the workload of test case writing, enhancing test execution efficiency, and strengthening cross-platform adaptability. Test-Agent offers an innovative and efficient solution for automated testing of mobile applications.
DA  - 2024///
PY  - 2024
DO  - 10.1109/dtpi61353.2024.10778901
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.2.1. CNN / RNN / LSTM
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.2. Test Data Generation
KW  - 3.2.3. Test Datasets (HTML
KW  - 3.3. Functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.3. Instructional / Role prompting
KW  - 5.4. Visual / GUI-based Prompting
KW  - Screenshots
KW  - UML)
ER  - 

TY  - JOUR
TI  - Test-driven development and LLM-based code generation
AU  - N., Mathews
AU  - Mei, Nagappan
T2  - International Conference on Automated Software Engineering
AB  - Recent Large Language Models (LLMs) have demonstrated significant capabilities in generating code snippets directly from problem statements. This increasingly automated process mirrors traditional human-led software development, where code is often written in response to a requirement. Historically, Test-Driven Development (TDD) has proven its merit, requiring developers to write tests before the functional code, ensuring alignment with the initial problem statements. Applying TDD principles to LLM-based code generation offers one distinct benefit: it enables developers to verify the correctness of generated code against predefined tests. This paper investigates if and how TDD can be incorporated into AI-assisted code-generation processes. We experimentally evaluate our hypothesis that providing LLMs like GPT-4 and Llama 3 with tests in addition to the problem statements enhances code generation outcomes. We experimented with established function-level code generation benchmarks such as MBPP and HumanEval. Our results consistently demonstrate that including test cases leads to higher success in solving programming challenges. We assert that TDD is a promising paradigm for helping ensure that the code generated by LLMs effectively captures the requirements.CCS CONCEPTS• Software and its engineering ? Software development techniques; • Computing methodologies ? Artificial intelligence.
DA  - 2024///
PY  - 2024
DO  - 10.1145/3691620.3695527
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 4.1.2. Code Repair / Refactoring
KW  - 4.2. Development Process Automation
KW  - 4.2.3. Test-Driven Development
KW  - 4.2.4. Low-code / No-code Development
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - TestSpark: IntelliJ idea's ultimate test generation companion
AU  - Arkadii, Sapozhnikov
AU  - Mitchell, Olsthoorn
AU  - Annibale, Panichella
AU  - Vladimir, Kovalenko
AU  - P., Derakhshanfar
T2  - 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)
AB  - Writing software tests is laborious and time-consuming. To address this, prior studies introduced various automated test-generation techniques. A well-explored research direction in this field is unit test generation, wherein artificial intelligence (AI) techniques create tests for a method/class under test. While many of these techniques have primarily found applications in a research context, existing tools (e.g., EvoSuite, Randoop, and AthenaTest) are not user-friendly and are tailored to a single technique. This paper introduces Test-Spark, a plugin for IntelliJ IDEA that enables users to generate unit tests with only a few clicks directly within their Integrated Development Environment (IDE). Furthermore, TestSpark also allows users to easily modify and run each generated test and integrate them into the project workflow. TestSpark leverages the advances of search-based test generation tools, and it introduces a technique to generate unit tests using Large Language Models (LLMs) by creating a feedback cycle between the IDE and the LLM. Since TestSpark is an open-source (https://github.com/JetBrains-Research/TestSpark), extendable, and well-documented tool, it is possible to add new test generation methods into the plugin with the minimum effort. This paper also explains our future studies related to TestSpark and our preliminary results. Demo video: https://youtu.be/0F4PrxWfiXo
DA  - 2024///
PY  - 2024
DO  - 10.1145/3639478.3640024
N1  - Citation Count: 0
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.4. Test Repair
KW  - 3.1.5. Test Prioritization
KW  - 3.1.9. Automated testing tools
KW  - 3.4. Integration Testing
KW  - 3.5. Robustness / Non-functional Testing
ER  - 

TY  - JOUR
TI  - The current challenges of software engineering in the era of large language models
AU  - Cuiyun, Gao
AU  - Xing, Hu
AU  - Shan, Gao
AU  - Xin, Xia
AU  - Zhi, Jin
T2  - ACM Transactions on Software Engineering and Methodology
AB  - With the advent of large language models (LLMs) in the artificial intelligence (AI) area, the field of software engineering (SE) has also witnessed a paradigm shift. These models, by leveraging the power of deep learning and massive amounts of data, have demonstrated an unprecedented capacity to understand, generate, and operate programming languages. They can assist developers in completing a broad spectrum of software development activities, encompassing software design, automated programming, and maintenance, which potentially reduces huge human efforts. Integrating LLMs within the SE landscape (LLM4SE) has become a burgeoning trend, necessitating exploring this emergent landscape’s challenges and opportunities. The paper aims at revisiting the software development life cycle (SDLC) under LLMs, and highlighting challenges and opportunities of the new paradigm. The paper first summarizes the overall process of LLM4SE, and then elaborates on the current challenges based on a through discussion. The discussion was held among more than 20 participants from academia and industry, specializing in fields such as software engineering and artificial intelligence. Specifically, we achieve 26 key challenges from seven aspects, including software requirement & design, coding assistance, testing code generation, code review, code maintenance, software vulnerability management, and data, training, and evaluation. We hope the achieved challenges would benefit future research in the LLM4SE field.
DA  - 2025///
PY  - 2025
DO  - 10.1145/3712005
N1  - Citation Count: 1
KW  - 1. LLM
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 4.1.5. Requirement Classification
KW  - 4.1.6. Requirement Generation
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - The integration of machine learning into automated test generation: A systematic mapping study
AU  - Afonso, Fontes
AU  - Gregory, Gay
T2  - Software testing, verification & reliability
AB  - Abstract Machine learning (ML) may enable effective automated test generation. We characterize emerging research, examining testing practices, researcher goals, ML techniques applied, evaluation, and challenges in this intersection by performing. We perform a systematic mapping study on a sample of 124 publications. ML generates input for system, GUI, unit, performance, and combinatorial testing or improves the performance of existing generation methods. ML is also used to generate test verdicts, property?based, and expected output oracles. Supervised learning—often based on neural networks—and reinforcement learning—often based on Q?learning—are common, and some publications also employ unsupervised or semi?supervised learning. (Semi?/Un?)Supervised approaches are evaluated using both traditional testing metrics and ML?related metrics (e.g., accuracy), while reinforcement learning is often evaluated using testing metrics tied to the reward function. The work?to?date shows great promise, but there are open challenges regarding training data, retraining, scalability, evaluation complexity, ML algorithms employed—and how they are applied—benchmarks, and replicability. Our findings can serve as a roadmap and inspiration for researchers in this field.
DA  - 2023///
PY  - 2023
DO  - 10.1002/stvr.1845
N1  - Citation Count: 5
KW  - 2. ML
KW  - 2.3. Supervised / Unsupervised Learning Strategies
KW  - 2.5. Reinforcement Learning Strategies
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.2. Test Data Generation
KW  - 3.2.3. Test Datasets (HTML
KW  - 3.3. Functional Testing
KW  - 3.5. Robustness / Non-functional Testing
KW  - Screenshots
KW  - UML)
ER  - 

TY  - CONF
TI  - The role of AI in software test automation
AU  - S., R. M., Ranapana
AU  - WMJI, Wijayanayake
T3  - 2025 5th International Conference on Advanced Research in Computing (ICARC)
DA  - 2025///
PY  - 2025
SP  - 1
EP  - 6
PB  - IEEE
UR  - https://ieeexplore.ieee.org/abstract/document/10962814/
Y2  - 2025/04/30/
N1  - Citation Count: 0
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.2.3. Transfer / Ensemble Learning
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
ER  - 

TY  - JOUR
TI  - TOGA: a neural method for test oracle generation
AU  - Elizabeth, Dinella
AU  - Elizabeth, Dinella
AU  - Gabriel, Ryan
AU  - Gabriel, Ryan
AU  - Todd, Mytkowicz
AU  - Todd, Mytkowicz
AU  - K., Shuvendu, Lahiri
AU  - K., Shuvendu, Lahiri
T2  - International Conference on Software Engineering
AB  - Testing is widely recognized as an important stage of the software development lifecycle. Effective software testing can provide benefits such as bug finding, preventing regressions, and documentation. In terms of documentation, unit tests express a unit's intended functionality, as conceived by the developer. A test oracle, typically expressed as an condition, documents the intended behavior of a unit under a given test prefix. Synthesizing a functional test oracle is a challenging problem, as it must capture the intended functionality rather than the implemented functionality. In this paper, we propose TOGA (a neural method for Test Oracle GenerAtion), a unified transformer-based neural approach to infer both exceptional and assertion test oracles based on the context of the focal method. Our approach can handle units with ambiguous or missing documentation, and even units with a missing implementation. We evaluate our approach on both oracle inference accuracy and functional bug-finding. Our technique improves accuracy by 33
DA  - 2022///
PY  - 2022
DO  - 10.1145/3510003.3510141
N1  - Citation Count: 53
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 3. Software Testing
KW  - 3.2. Test Data Generation
KW  - 3.2.1. Test Oracle
KW  - 3.4. Integration Testing
ER  - 

TY  - JOUR
TI  - Towards an understanding of large language models in software engineering tasks
AU  - Zibin, Zheng
AU  - Kaiwen, Ning
AU  - Jiachi, Chen
AU  - Yanlin, Wang
AU  - Wenqing, Chen
AU  - Liang-Hong, Guo
AU  - Weicheng, Wang
T2  - Empirical Software Engineering
AB  - Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in tasks such as text generation and reasoning. Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus. However, there is still a lack of systematic research on the application and evaluation of LLMs in the field of software engineering. Therefore, this paper is the first to comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks? To find the answers, we have collected related literature as extensively as possible from seven mainstream databases, and selected 123 papers for analysis. We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs. Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, providing guidance for researchers and developers to optimize.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arxiv.2308.11396
N1  - Citation Count: 8
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.5. Robustness / Non-functional Testing
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Towards autonomous testing agents via conversational large language models
AU  - Robert, Feldt
AU  - Sungmin, Kang
AU  - Jaeseung, Yoon
AU  - Shin, Yoo
T2  - International Conference on Automated Software Engineering
AB  - Software testing is an important part of the development cycle, yet it requires specialized expertise and substantial developer effort to adequately test software. Recent discoveries of the capabilities of large language models (LLMs) suggest that they can be used as automated testing assistants, and thus provide helpful information and even drive the testing process. To highlight the potential of this technology, we present a taxonomy of LLM-based testing agents based on their level of autonomy, and describe how a greater level of autonomy can benefit developers in practice. An example use of LLMs as a testing assistant is provided to demonstrate how a conversational framework for testing can help developers. This also highlights how the often criticized “hallucination” of LLMs can be beneficial for testing. We identify other tangible benefits that LLM-driven testing agents can bestow, and also discuss potential limitations.
DA  - 2023///
PY  - 2023
DO  - 10.1109/ase56229.2023.00148
N1  - Citation Count: 18
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.5. Test Prioritization
ER  - 

TY  - CONF
TI  - Towards understanding the effectiveness of large language models on directed test input generation
AU  - Zongze, Jiang
AU  - Ming, Wen
AU  - Jialun, Cao
AU  - Xuanhua, Shi
AU  - Hai, Jin
T3  - Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering
DA  - 2024///
PY  - 2024
SP  - 1408
EP  - 1420
UR  - https://dl.acm.org/doi/abs/10.1145/3691620.3695513?casa_token=or-MFqKxfxoAAAAA:0iMOOdhQwtTzgQR50XR75wClv7KKslqFKVBLMIZsXBsvmyv3hKYQN1GVpl4cVKG8N83JHxR9bJKHkA
Y2  - 2025/04/30/
N1  - Citation Count: 3
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.2. Test Data Generation
KW  - 3.2.3. Test Datasets (HTML
KW  - 3.5. Robustness / Non-functional Testing
KW  - Screenshots
KW  - UML)
ER  - 

TY  - JOUR
TI  - TraceAwareness and dual-strategy fuzz testing: Enhancing path coverage and crash localization with stochastic science and large language models
AU  - Xiaoquan, Chen
AU  - Jian, Liu
AU  - Yingkai, Zhang
AU  - Qinsong, Hu
AU  - Yupeng, Han
AU  - Ruqi, Zhang
AU  - Jingqi, Ran
AU  - Lei, Yan
AU  - Baiqi, Huang
AU  - Shengting, Ma
T2  - Computers & electrical engineering
DA  - 2025///
PY  - 2025
DO  - 10.1016/j.compeleceng.2025.110266
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.2.1. CNN / RNN / LSTM
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.2. Test Optimization
KW  - 3.1.8. Test Quality Analysis
KW  - 3.5. Robustness / Non-functional Testing
ER  - 

TY  - JOUR
TI  - Understandable test generation through capture/replay and llms
AU  - Amirhossein, Deljouyi
T2  - 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)
AB  - Automatic unit test generators, particularly search-based software testing (SBST) tools such as EvoSuite, efficiently generate unit test suites with acceptable coverage. Although this removes the burden of writing unit tests from developers, these generated tests often pose challenges in terms of comprehension for developers. In my doctoral research, I aim to investigate strategies to address the issue of comprehensibility in generated test cases and improve the test suite in terms of effectiveness. To achieve this, I introduce four projects leveraging Capture/Replay and Large Language Model (LLM) techniques. Capture/Replay carves information from End-to-End (E2E) tests, enabling the generation of unit tests containing meaningful test scenarios and actual test data. Moreover, the growing capabilities of large language models (LLMs) in language analysis and transformation play a significant role in improving readability in general. Our proposed approach involves leveraging E2E test scenario extraction alongside an LLM-guided approach to enhance test case understandability, augment coverage, and establish comprehensive mock and test oracles. In this research, we endeavor to conduct both a quantitative analysis and a user evaluation of the quality of the generated tests in terms of executability, coverage, and understandability.
DA  - 2024///
PY  - 2024
DO  - 10.1145/3639478.3639789
N1  - Citation Count: 1
KW  - 1. LLM
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.3. Functional Testing
KW  - 4. Software Engineering
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - CONF
TI  - Unit test generation using generative AI: A comparative performance analysis of autogeneration tools
AU  - Shreya, Bhatia
AU  - Tarushi, Gandhi
AU  - Dhruv, Kumar
AU  - Pankaj, Jalote
T3  - Proceedings of the 1st International Workshop on Large Language Models for Code
DA  - 2024///
PY  - 2024
SP  - 54
EP  - 61
ST  - Unit test generation using generative AI
UR  - https://dl.acm.org/doi/abs/10.1145/3643795.3648396
Y2  - 2025/04/30/
N1  - Citation Count: 44
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.4. Integration Testing
KW  - 5.1. Prompting Strategies
KW  - 5.5. Self-improving Prompts
ER  - 

TY  - JOUR
TI  - Unit test generation using large language models for unity game development
AU  - C., Paduraru
AU  - Alin, Stefanescu
AU  - Augustin, Jianu
T2  - FaSE4Games@SIGSOFT FSE
DA  - 2024///
PY  - 2024
DO  - 10.1145/3663532.3664466
N1  - Citation Count: 1
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.4. Integration Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Unit test generation using large language models: A systematic literature review
AU  - Marius, Dovydas, Zapkus
AU  - Asta, Slotkienл
T2  - Lietuvos magistrantш informatikos ir IT tyrimai: konferencijos darbai, 2024 m. geguюлs 10 d.
DA  - 2024///
PY  - 2024
SP  - 136
EP  - 144
ST  - Unit test generation using large language models
UR  - https://epublications.vu.lt/object/elaba:198450771/
Y2  - 2025/04/30/
N1  - Citation Count: 3
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.3. Functional Testing
KW  - 3.4. Integration Testing
ER  - 

TY  - JOUR
TI  - Usability of llms for assisting software engineering: a literature review
AU  - Dania, Hussein
DA  - 2024///
PY  - 2024
ST  - Usability of llms for assisting software engineering
UR  - https://elib.dlr.de/210605/
Y2  - 2025/04/30/
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - BOOK
TI  - Use case testing: a constrained active machine learning approach
AU  - Karl, Meinke
AU  - Hojat, Khosrowjerdi
DA  - 2021///
PY  - 2021
VL  - 12740
LA  - en
PB  - Springer International Publishing
SN  - 978-3-030-79378-4 978-3-030-79379-1
ST  - Use case testing
UR  - https://link.springer.com/10.1007/978-3-030-79379-1_1
Y2  - 2025/02/05/
N1  - Citation Count: 6
KW  - 2. ML
KW  - 2.1. Traditional Machine Learning
KW  - 2.4. Active Learning Strategies
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.2. Test Data Generation
KW  - 3.2.3. Test Datasets (HTML
KW  - 3.3. Functional Testing
KW  - Screenshots
KW  - UML)
ER  - 

TY  - JOUR
TI  - Use of ChatGPT as an assistant in the end-to-end test script generation for android apps
AU  - Boni, Garc?a
AU  - Maurizio, Leotta
AU  - Filippo, Ricca
AU  - Jim, Whitehead
T2  - A-TEST@ISSTA
DA  - 2024///
PY  - 2024
DO  - 10.1145/3678719.3685691
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.3. Functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Using a large language model as a building block to generate UsableValidation and verification suite for OpenMP
AU  - S., Pophale
AU  - W., Elwasif
AU  - D., Bernholdt
T2  - Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region
DA  - 2025///
PY  - 2025
DO  - 10.1145/3712031.3712331
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.3. Functional Testing
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Using chatgpt in software requirements engineering: A comprehensive review
AU  - Nuno, Marques
AU  - Rocha, Rodrigo, Silva
AU  - Jorge, Bernardino
T2  - Future Internet
DA  - 2024///
PY  - 2024
VL  - 16
IS  - 6
SP  - 180
ST  - Using chatgpt in software requirements engineering
UR  - https://www.mdpi.com/1999-5903/16/6/180
Y2  - 2025/04/30/
N1  - Citation Count: 42
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.6. Requirement Generation
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Using deep learning for selenium web UI functional tests: A case-study with e-commerce applications
AU  - Zubair, Khaliq
AU  - Zubair, Khaliq
AU  - A., Dawood, Khan
AU  - Ashraf, Dawood, Khan
AU  - Umar, Sheikh, Farooq
AU  - Umar, Sheikh, Farooq
T2  - Engineering Applications of Artificial Intelligence
DA  - 2023///
PY  - 2023
DO  - 10.1016/j.engappai.2022.105446
N1  - Citation Count: 9
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.3. Functional Testing
ER  - 

TY  - JOUR
TI  - Using GitHub copilot for test generation in python: An empirical study
AU  - El, Khalid, Haji
AU  - C., Brandt
AU  - A., Zaidman
T2  - International Conference/Workshop on Automation of Software Test
AB  - Writing unit tests is a crucial task in software development, but it is also recognized as a time-consuming and tedious task. As such, numerous test generation approaches have been proposed and investigated. However, most of these test generation tools produce tests that are typically difficult to understand. Recently, Large Language Models (LLMs) have shown promising results in generating source code and supporting software engineering tasks. As such, we investigate the usability of tests generated by GitHub Copilot, a proprietary closed-source code generation tool that uses an LLM. We evaluate GitHub Copilot’s test generation abilities both within and without an existing test suite, and we study the impact of different code commenting strategies on test generations. Our investigation evaluates the usability of 290 tests generated by GitHub Copilot for 53 sampled tests from open source projects. Our findings highlight that within an existing test suite, approximately 45.28
DA  - 2024///
PY  - 2024
DO  - 10.1145/3644032.3644443
N1  - Citation Count: 2
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.3. Functional Testing
KW  - 3.4. Integration Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Using large language model to fill in web forms to support automated web application testing
AU  - Feng-Kai, Chen
AU  - Chien-Hung, Liu
AU  - D., S., You
T2  - Information-an International Interdisciplinary Journal
AB  - Web applications, widely used by enterprises for business services, require extensive testing to ensure functionality. Performing form testing with random input data often takes a long time to complete. Previously, we introduced a model for automated testing of web applications using reinforcement learning. The model was trained to fill form fields with fixed input values and click buttons. However, the performance of this model was limited by a fixed set of input data and the imprecise detection of successful form submission. This paper proposes a model to address these limitations. First, we use a large language model with data fakers to generate a wide variety of input data. Additionally, whether form submission is successful is partially determined by GPT-4o. Experiments show that our method increases average statement coverage by 2.3
DA  - 2025///
PY  - 2025
DO  - 10.3390/info16020102
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.3. Functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Using large language models to generate junit tests: An empirical study
AU  - Latif, Mohammed, Siddiq
AU  - S., Joanna C., Santos
AU  - Hasan, Ridwanul, Tanvir
AU  - Noshin, Ulfat
AU  - Al, Fahmid, Rifat
AU  - Carvalho, Vinicius, Lopes
T2  - International Conference on Evaluation & Assessment in Software Engineering
AB  - A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning for a strongly typed language like Java. To fill this gap, we investigated how well three models (Codex, GPT-3.5-Turbo, and StarCoder) can generate unit tests. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the effect of context generation on the unit test generation process. We evaluated the models based on compilation rates, test correctness, test coverage, and test smells. We found that the Codex model achieved above 80
DA  - 2023///
PY  - 2023
DO  - 10.1145/3661167.3661216
N1  - Citation Count: 16
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.4. Test Repair
KW  - 3.1.8. Test Quality Analysis
KW  - 3.3. Functional Testing
ER  - 

TY  - CONF
TI  - Using large language models to support software engineering documentation in waterfall life cycles: Are we there yet?
AU  - Antonio, Della Porta
AU  - Vincenzo, De Martino
AU  - Gilberto, Recupito
AU  - Carmine, Iemmino
AU  - Gemma, Catolino
AU  - Dario, Di Nucci
AU  - Fabio, Palomba
T3  - CEUR WORKSHOP PROCEEDINGS
DA  - 2024///
PY  - 2024
VL  - 3762
SP  - 452
EP  - 457
PB  - CEUR-WS
ST  - Using large language models to support software engineering documentation in waterfall life cycles
UR  - https://ceur-ws.org/Vol-3762/523.pdf
Y2  - 2025/04/30/
N1  - Citation Count: 2
KW  - 1. LLM
KW  - 1.1. Transformer-based Models
KW  - 1.2. Multimodal Models
KW  - 4.1. Development Task Automation
KW  - 4.1.6. Requirement Generation
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Using LLM-based deep reinforcement learning agents to detect bugs in web applications
AU  - Yuki, Sakai
AU  - Yasuyuki, Tahara
AU  - Akihiko, Ohsuga
AU  - Y., Sei
T2  - International Conference on Agents and Artificial Intelligence
AB  - : This paper presents an approach to automate black-box GUI testing for web applications by integrating deep reinforcement learning (DRL) with large language models (LLMs). Traditional GUI testing is often inefficient and costly due to the difficulty in generating comprehensive test scenarios. While DRL has shown potential in automating exploratory testing by leveraging GUI interaction data, such data is browser-dependent and not always accessible in web applications. To address this challenge, we propose using LLMs to infer interaction information directly from HTML code, incorporating these inferences into the DRL’s state representation. We hypothesize that combining the inferential capabilities of LLMs with the robustness of DRL can match the accuracy of methods relying on direct data collection. Through experiments, we demonstrate that LLM-inferred interaction information effectively substitutes for direct data, enhancing both the efficiency and accuracy of automated GUI testing. Our results indicate that this approach not only streamlines GUI testing for web applications but also has broader implications for domains where direct state information is hard to obtain. The study suggests that integrating LLMs with DRL offers a promising path toward more efficient and scalable automation in GUI testing.
DA  - 2025///
PY  - 2025
DO  - 10.5220/0013248800003890
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 1.3. Fine-tuned / Instruction-tuned
KW  - 2. ML
KW  - 2.2. Deep Learning
KW  - 2.2.1. CNN / RNN / LSTM
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.2. Test Data Generation
KW  - 3.2.3. Test Datasets (HTML
KW  - 3.3. Functional Testing
KW  - Screenshots
KW  - UML)
ER  - 

TY  - JOUR
TI  - UTFix: Change aware unit test repairing using LLM
AU  - Shanto, Rahman
AU  - Sachit, Kuhar
AU  - Berk, ?irisci
AU  - P., Garg
AU  - Shiqi, Wang
AU  - Xiaofei, Ma
AU  - Anoop, Deoras
AU  - Baishakhi, Ray
T2  - Proceedings of the ACM on Programming Languages
AB  - Software updates, including bug repair and feature additions, are frequent in modern applications but they often leave test suites outdated, resulting in undetected bugs and increased chances of system failures. A recent study by Meta revealed that 14 In this paper, we present UTFix, a novel approach for repairing unit tests when their corresponding focal methods undergo changes. UTFix addresses two critical issues: assertion failure and reduced code coverage caused by changes in the focal method. Our approach leverages language models to repair unit tests by providing contextual information such as static code slices, dynamic code slices, and failure messages. We evaluate UTFix on our generated synthetic benchmarks (Tool-Bench), and real-world benchmarks. Tool- Bench includes diverse changes from popular open-source Python GitHub projects, where UTFix successfully repaired 89.2
DA  - 2025///
PY  - 2025
DO  - 10.1145/3720419
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.4. Test Repair
KW  - 3.1.6. Failure Detection
KW  - 3.1.8. Test Quality Analysis
KW  - 3.5. Robustness / Non-functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.5. Self-improving Prompts
ER  - 

TY  - JOUR
TI  - VerilogReader: LLM-aided hardware test generation
AU  - Ruiyang, Ma
AU  - Yuxin, Yang
AU  - Ziqian, Liu
AU  - Jiaxi, Zhang
AU  - Min, Li
AU  - Junhua, Huang
AU  - Guojie, Luo
T2  - 2024 IEEE LLM Aided Design Workshop (LAD)
AB  - Test generation has been a critical and labor-intensive process in hardware design verification. Recently, the emergence of Large Language Model (LLM) with their advanced understanding and inference capabilities, has introduced a novel approach. In this work, we investigate the integration of LLM into the Coverage Directed Test Generation (CDG) process, where the LLM functions as a Verilog Reader. It accurately grasps the code logic, thereby generating stimuli that can reach unexplored code branches. We compare our framework with random testing, using our self-designed Verilog benchmark suite. Experiments demonstrate that our framework outperforms random testing on designs within the LLM’s comprehension scope. Our work also proposes prompt engineering optimizations to augment LLM’s understanding scope and accuracy.
DA  - 2024///
PY  - 2024
DO  - 10.1109/lad62341.2024.10691801
N1  - Citation Count: 0
KW  - 2. ML
KW  - 2.1. Traditional Machine Learning
KW  - 2.1.2. Feature-based Models
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.8. Test Quality Analysis
KW  - 3.5. Robustness / Non-functional Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
ER  - 

TY  - JOUR
TI  - Vision-based mobile app GUI testing: a survey
AU  - Shuling, Yu
AU  - Chunrong, Fang
AU  - Quanjun, Zhang
AU  - Chunyang, Chen
AU  - Zhenyu, Chen
AU  - Zhendong, Su
T2  - arXiv.org
AB  - Graphical User Interface (GUI) has become one of the most significant parts of mobile applications (apps). It is a direct bridge between mobile apps and end users, which directly affects the end user's experience. Neglecting GUI quality can undermine the value and effectiveness of the entire mobile app solution. Significant research efforts have been devoted to GUI testing, one effective method to ensure mobile app quality. By conducting rigorous GUI testing, developers can ensure that the visual and interactive elements of the mobile apps not only meet functional requirements but also provide a seamless and user-friendly experience. However, traditional solutions, relying on the source code or layout files, have met challenges in both effectiveness and efficiency due to the gap between what is obtained and what app GUI actually presents. Vision-based mobile app GUI testing approaches emerged with the development of computer vision technologies and have achieved promising progress. In this survey paper, we provide a comprehensive investigation of the state-of-the-art techniques on 226 papers, among which 78 are vision-based studies. This survey covers different topics of GUI testing, like GUI test generation, GUI test record & replay, GUI testing framework, etc. Specifically, the research emphasis of this survey is placed mostly on how vision-based techniques outperform traditional solutions and have gradually taken a vital place in the GUI testing field. Based on the investigation of existing studies, we outline the challenges and opportunities of (vision-based) mobile app GUI testing and propose promising research directions with the combination of emerging techniques.
DA  - 2023///
PY  - 2023
DO  - 10.48550/arxiv.2310.13518
N1  - Citation Count: 1
KW  - 2. ML
KW  - 2.1. Traditional Machine Learning
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.3. Functional Testing
KW  - 3.4. Integration Testing
KW  - 5. Prompt Engineering
KW  - 5.1. Prompting Strategies
KW  - 5.4. Visual / GUI-based Prompting
ER  - 

TY  - JOUR
TI  - Vulnerability detection: From formal verification to large language models and hybrid approaches: a comprehensive overview
AU  - Norbert, Tihanyi
AU  - Tam?s, Bisztray
AU  - M., Ferrag
AU  - Bilel, Cherif
AU  - A., Richard, Dubniczky
AU  - Ridhi, Jain
AU  - C., Lucas, Cordeiro
T2  - arXiv.org
AB  - Software testing and verification are critical for ensuring the reliability and security of modern software systems. Traditionally, formal verification techniques, such as model checking and theorem proving, have provided rigorous frameworks for detecting bugs and vulnerabilities. However, these methods often face scalability challenges when applied to complex, real-world programs. Recently, the advent of Large Language Models (LLMs) has introduced a new paradigm for software analysis, leveraging their ability to understand insecure coding practices. Although LLMs demonstrate promising capabilities in tasks such as bug prediction and invariant generation, they lack the formal guarantees of classical methods. This paper presents a comprehensive study of state-of-the-art software testing and verification, focusing on three key approaches: classical formal methods, LLM-based analysis, and emerging hybrid techniques, which combine their strengths. We explore each approach's strengths, limitations, and practical applications, highlighting the potential of hybrid systems to address the weaknesses of standalone methods. We analyze whether integrating formal rigor with LLM-driven insights can enhance the effectiveness and scalability of software verification, exploring their viability as a pathway toward more robust and adaptive testing frameworks.
DA  - 2025///
PY  - 2025
DO  - 10.48550/arxiv.2503.10784
N1  - Citation Count: 0
KW  - 1. LLM
KW  - 2. ML
KW  - 4. Software Engineering
KW  - 4.1. Development Task Automation
KW  - 4.1.1. Code Generation
KW  - 4.1.6. Requirement Generation
ER  - 

TY  - JOUR
TI  - Why many challenges with GUI test automation (will) remain
AU  - Michel, Nass
AU  - Emil, Alйgroth
AU  - Robert, Feldt
T2  - Information and Software Technology
DA  - 2021///
PY  - 2021
DO  - 10.1016/j.infsof.2021.106625
VL  - 138
SP  - 106625
LA  - en
SN  - 09505849
UR  - https://linkinghub.elsevier.com/retrieve/pii/S0950584921000963
Y2  - 2025/04/05/
N1  - Citation Count: 71
KW  - 3. Software Testing
KW  - 3.1. Test Automation
KW  - 3.1.1. Test Generation
KW  - 3.1.4. Test Repair
KW  - 3.1.7. Self-healing Testing
KW  - 3.1.8. Test Quality Analysis
ER  - 

